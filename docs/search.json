[
  {
    "objectID": "01-overview/index.html",
    "href": "01-overview/index.html",
    "title": "Overview",
    "section": "",
    "text": "Before jumping into network science details, we need to cover some fundamentals. I assume that most of the contents here are well known to you–we will be brief–but I want to ensure we are all on the same page."
  },
  {
    "objectID": "01-overview/index.html#getting-help",
    "href": "01-overview/index.html#getting-help",
    "title": "Overview",
    "section": "1.1 Getting help",
    "text": "1.1 Getting help\nUnlike other languages, R’s documentation is highly reliable. The Comprehensive R Archive Network [CRAN] is the official repository of R packages. All packages posted on CRAN must pass a series of tests to ensure the quality of the code, including the documentation.\nTo get help on a function, we can use the help() function. For example, if we wanted to get help on the mean() function, we would do:\n\nhelp(\"mean\")"
  },
  {
    "objectID": "01-overview/index.html#naming-conventions",
    "href": "01-overview/index.html#naming-conventions",
    "title": "Overview",
    "section": "1.2 Naming conventions",
    "text": "1.2 Naming conventions\nR has a set of naming conventions that we should follow to avoid confusion. The most important ones are:\n\nUse lowercase letters (optional)\nUse underscores to separate words (optional)\nDo not start with a number\nDo not use special characters\nDo not use reserved words\n\n\n\n\n\n\n\nQuestion\n\n\n\nOf the following list, which are valid names and which are valid but to be avoided?\n_my.var\nmy.var\nmy_var\nmyVar\nmyVar1\n1myVar\nmy var\nmy-var"
  },
  {
    "objectID": "01-overview/index.html#assignment",
    "href": "01-overview/index.html#assignment",
    "title": "Overview",
    "section": "1.3 Assignment",
    "text": "1.3 Assignment\nIn R, we have two (four) ways of assigning values to objects: the &lt;- and = binary operators2. Although both are equivalent, the former is the preferred way of assigning values to objects since the latter can be confused with function arguments.\n\nx &lt;- 1\nx = 1\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the difference between the following two assignments? Use the help function to find out.\nx &lt;- 1\nx &lt;&lt;- 1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are other ways in which you can assign values to objects?"
  },
  {
    "objectID": "01-overview/index.html#using-functions-and-piping",
    "href": "01-overview/index.html#using-functions-and-piping",
    "title": "Overview",
    "section": "1.4 Using functions and piping",
    "text": "1.4 Using functions and piping\nIn R, we use functions to perform operations on objects. Functions are implemented as function_name ( argument_1 , argument_2 , ... ). For example, the mean() function takes a vector of numbers and returns the mean of the values:\n\nx &lt;- c(1, 2, 3) # The c() function creates a vector\nmean(x)\n## [1] 2\n\nFurthermore, we can use the pipe operator (|&gt;) to improve readability. The pipe operator takes the output of the left-hand side expression and passes it as the first argument of the right-hand side expression. Our previous example could be rewritten as:\n\nc(1, 2, 3) |&gt; mean()\n## [1] 2"
  },
  {
    "objectID": "01-overview/index.html#data-structures",
    "href": "01-overview/index.html#data-structures",
    "title": "Overview",
    "section": "1.5 Data structures",
    "text": "1.5 Data structures\nAtomic types are the minimal building blocks of R. They are logical, integer, double, character, complex, raw:\n\nx_logical   &lt;- TRUE\nx_integer   &lt;- 1L\nx_double    &lt;- 1.0\nx_character &lt;- \"a\"\nx_complex   &lt;- 1i\nx_raw       &lt;- charToRaw(\"a\")\n\nUnlike other languages, we do not need to declare the data type before creating the object; R will infer it from the value.\n\n\n\n\n\n\nPro-tip\n\n\n\nAdding the L suffix to the value is good practice when dealing with integers. Some R packages like data.table (Barrett, Dowle, and Srinivasan 2023) have internal checks that will throw an error if you are not explicit about the data type.\n\n\nThe next type is the vector. A vector is a collection of elements of the same type. The most common way to create a vector is with the c() function:\n\nx_integer &lt;- c(1, 2, 3)\nx_double  &lt;- c(1.0, 2.0, 3.0)\nx_logical &lt;- c(TRUE, FALSE, TRUE)\n# etc.\n\nR will coerce the data types to the most general type. For example, if we mix integers and doubles, R will coerce the integers into doubles. The coercion order is logical &lt; integer &lt; double &lt; character\n\n\n\n\n\n\nQuestion\n\n\n\nWhy is the coercion order logical &lt; integer &lt; double &lt; character?\n\n\nThe next data structure is the list. A list is a collection of elements of any type. We can create a list with the list() function:\n\nx_list       &lt;- list(1, 2.0, TRUE, \"a\")\nx_list_named &lt;- list(a = 1, b = 2.0, c = TRUE, d = \"a\")\n\nTo access elements in a list, we have two options: by position or by name, the latter only if the elements are named:\n\nx_list[[1]]\n## [1] 1\nx_list_named[[\"a\"]]\n## [1] 1\nx_list_named$a\n## [1] 1\n\nAfter lists, we have matrices. A matrix is a collection of elements of the same type arranged in a two-dimensional grid. We can create a matrix with the matrix() function:\n\nx_matrix &lt;- matrix(1:9, nrow = 3, ncol = 3)\nx_matrix\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n\n# We can access elements in a matrix by row column, or position:\nx_matrix[1, 2]\n## [1] 4\nx_matrix[cbind(1, 2)]\n## [1] 4\nx_matrix[4]\n## [1] 4\n\n\n\n\n\n\n\nMatrix is a vector\n\n\n\nMatrices in R are vectors with dimensions. In base R, matrices are stored in column-major order. This means that the elements are stored column by column. This is important to know when we are accessing elements in a matrix\n\n\nThe two last data structures are arrays and data frames. An array is a collection of elements of the same type arranged in a multi-dimensional grid. We can create an array with the array() function:\n\nx_array &lt;- array(1:27, dim = c(3, 3, 3))\n\n# We can access elements in an array by row, column, and dimension, or\n# position:\nx_array[1, 2, 3]\n## [1] 22\nx_array[cbind(1, 2, 3)]\n## [1] 22\nx_array[22]\n## [1] 22\n\nData frames are the most common data structure in R. In principle, these objects are lists of vectors of the same length, each vector representing a column. Columns (lists) in data frames can be of different types, but elements in each column must be of the same type. We can create a data frame with the data.frame() function:\n\nx_data_frame &lt;- data.frame(\n  a = 1:3,\n  b = c(\"a\", \"b\", \"c\"),\n  c = c(TRUE, FALSE, TRUE)\n)\n\n# We can access elements in a data frame by row, column, or position:\nx_data_frame[1, 2]\n## [1] \"a\"\nx_data_frame[cbind(1, 2)]\n## [1] \"a\"\nx_data_frame$b[1]    # Like a list\n## [1] \"a\"\nx_data_frame[[2]][1] # Like a list too\n## [1] \"a\""
  },
  {
    "objectID": "01-overview/index.html#functions",
    "href": "01-overview/index.html#functions",
    "title": "Overview",
    "section": "1.6 Functions",
    "text": "1.6 Functions\nFunctions are the most important building blocks of R. A function is a set of instructions that takes one or more inputs and returns one or more outputs. We can create a function with the function() function:\n\n# This function has two arguments (y is optional)\nf &lt;- function(x, y = 1) {\n  x + 1\n}\n\nf(1)\n## [1] 2\n\nStarting with R 4, we can use the lambda syntax to create functions:\n\nf &lt;- \\(x, y) x + 1\n\nf(1)\n## [1] 2"
  },
  {
    "objectID": "01-overview/index.html#control-flow",
    "href": "01-overview/index.html#control-flow",
    "title": "Overview",
    "section": "1.7 Control flow",
    "text": "1.7 Control flow\nControl flow statements allow us to control the execution of the code. The most common control flow statements are if, for, while, and repeat. We can create a control flow statement with the if(), for(), while(), and repeat() functions:\n\n# if\nif (TRUE) {\n  \"a\"\n} else {\n  \"b\"\n}\n## [1] \"a\"\n\n# for\nfor (i in 1:3) {\n  cat(\"This is the number \", i, \"\\n\")\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3\n\n# while\ni &lt;- 1\nwhile (i &lt;= 3) {\n  cat(\"This is the number \", i, \"\\n\")\n  i &lt;- i + 1\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3\n\n# repeat\ni &lt;- 1\nrepeat {\n  cat(\"This is the number \", i, \"\\n\")\n  i &lt;- i + 1\n  if (i &gt; 3) {\n    break\n  }\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3"
  },
  {
    "objectID": "01-overview/index.html#r-packages",
    "href": "01-overview/index.html#r-packages",
    "title": "Overview",
    "section": "1.8 R packages",
    "text": "1.8 R packages\nR is so powerful because of its extensions. R extensions (different from other programming languages) are called packages. Packages are collections of functions, data, and documentation that provide additional functionality to R. Although anyone can create and distribute R packages to other users, the Comprehensive R Archive Network [CRAN] is the official repository of R packages. All packages posted on CRAN are thoroughly tested, so generally, their quality is high.\nTo install R packages, we use the install.packages() function; to load them, we use the library() function. For example, the following code chunk installs the ergm package and loads it:\n\ninstall.packages(\"ergm\")\nlibrary(ergm)"
  },
  {
    "objectID": "01-overview/index.html#hypothesis-testing",
    "href": "01-overview/index.html#hypothesis-testing",
    "title": "Overview",
    "section": "2.1 Hypothesis testing",
    "text": "2.1 Hypothesis testing\nAccording to Wikipedia\n\nA statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis. More generally, hypothesis testing allows us to make probabilistic statements about population parameters. More informally, hypothesis testing is the processes of making decisions under uncertainty. Typically, hypothesis testing procedures involve a user selected tradeoff between false positives and false negatives. – Wiki\n\nIn a nutshell, hypothesis testing is performed by following these steps:\n\nState the null and alternative hypotheses. In general, the null hypothesis is a statement about the population parameter that challenges our research question; for example, given the question of whether two networks are different, the null hypothesis would be that the two networks are the same.\nCompute the corresponding test statistic. It is a data function that reduces the information to a single number.\nCompare the observed test statistic with the distribution of the test statistic under the null hypothesis. The sometimes infamous p-value: ``[…] the probability that the chosen test statistic would have been at least as large as its observed value if every model assumption were correct, including the test hypothesis.’’ (Greenland et al. 2016) 3\n\n\nReport the observed effect and p-value, i.e., \\Pr(t \\in H_0)\n\nWe usually say that we either reject the null hypothesis or fail to reject it (we never accept the null hypothesis,) but, in my view, it is always better to talk about it in terms of “suggests evidence for” or “suggests evidence against.”\nWe will illustrate statistical concepts more concretely in the next section."
  },
  {
    "objectID": "01-overview/index.html#probability-distributions",
    "href": "01-overview/index.html#probability-distributions",
    "title": "Overview",
    "section": "3.1 Probability distributions",
    "text": "3.1 Probability distributions\nR has a standard way of naming probability functions. The naming structure is [type of function][distribution], where [type of function] can be d for density, p for cumulative distribution function, q for quantile function, and r for random generation. For example, the normal distribution has the following functions:\n\ndnorm(0, mean = 0, sd = 1)\n## [1] 0.3989423\npnorm(0, mean = 0, sd = 1)\n## [1] 0.5\nqnorm(0.5, mean = 0, sd = 1)\n## [1] 0\n\nNow, if we wanted to know what is the probability of observing a value smaller than -2 comming from a standard normal distribution, we would do:\n\npnorm(-2, mean = 0, sd = 1)\n## [1] 0.02275013\n\nCurrently, R has a wide range of probability distributions implemented.\n\n\n\n\n\n\nQuestion\n\n\n\nHow many probability distributions are implemented in R’s stats package?"
  },
  {
    "objectID": "01-overview/index.html#random-number-generation",
    "href": "01-overview/index.html#random-number-generation",
    "title": "Overview",
    "section": "3.2 Random number generation",
    "text": "3.2 Random number generation\nRandom numbers, and more precisely, pseudo-random numbers, are a vital component of statistical programming. Pure randomness is hard to come by, and so we rely on pseudo-random number generators (PRNGs) to generate random numbers. These generators are deterministic algorithms that produce sequences of numbers we can then use to generate random samples from probability distributions. Because of the latter, PRNGs need a starting point called the seed. As a statistical computing program, R has a variety of PRNGs. As suggested in the previous subsection, we can generate random numbers from a probability distribution with the r function. In what follows, we will draw random numbers from a few distributions and plot histograms of the results:\n\nset.seed(1)\n\n# Saving the current graphical parameters\nop &lt;- par(mfrow = c(2,2))\nrnorm(1000) |&gt; hist(main = \"Normal distribution\")\nrunif(1000) |&gt; hist(main = \"Uniform distribution\")\nrpois(1000, lambda = 1) |&gt; hist(main = \"Poisson distribution\")\nrbinom(1000, size = 10, prob = 0.1) |&gt; hist(main = \"Binomial distribution\")\n\n\n\npar(op)"
  },
  {
    "objectID": "01-overview/index.html#simulations-and-sampling",
    "href": "01-overview/index.html#simulations-and-sampling",
    "title": "Overview",
    "section": "3.3 Simulations and sampling",
    "text": "3.3 Simulations and sampling\nSimulations are front and center in statistical programming. We can use them to test the properties of statistical methods, generate data, and perform statistical inference. The following example uses the sample function in R to compute the bootstrap standard error of the mean (see Casella and Berger 2021):\n\nset.seed(1)\nx &lt;- rnorm(1000)\n\n# Bootstrap standard error of the mean\nn &lt;- length(x)\nB &lt;- 1000\n\n# We will store the results in a vector\nres &lt;- numeric(B)\n\nfor (i in 1:B) {\n  # Sample with replacement\n  res[i] &lt;- sample(x, size = n, replace = TRUE) |&gt;\n    mean()\n}\n\n# Plot the results\nhist(res, main = \"Bootstrap standard error of the mean\")\n\n\n\n\nSince the previous example is rather extensive, let us review it in detail.\n\nset.seed(1) sets the seed of the PRNG to 1. It ensures we get the same results every time we run the code.\nrnorm() generates a sample of 1,000 standard-normal values.\nn &lt;- length(x) stores the length of the vector in the n variable.\nB &lt;- 1000 stores the number of bootstrap samples in the B variable.\nres &lt;- numeric(B) creates a vector of length B to store the results.\nfor (i in 1:B) is a for loop that iterates from 1 to B.\nres[i] &lt;- sample(x, size = n, replace = TRUE) |&gt; mean() samples n values from x with replacement and computes the mean of the sample.\nThe pipe operator (|&gt;) passes the output of the left-hand side expression as the first argument of the right-hand side expression.\nhist(res, main = \"Bootstrap standard error of the mean\") plots the histogram of the results."
  },
  {
    "objectID": "01-overview/index.html#footnotes",
    "href": "01-overview/index.html#footnotes",
    "title": "Overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough, not for network science in general.↩︎\nIn mathematics and computer science, a binary operator is a function that takes two arguments. In R, binary operators are implemented as variable 1 [operator] variable 2. For example, 1 + 2 is a binary operation.↩︎\nThe discussion about interpreting p-values and hypothesis testing is vast and relevant. Although we will not review this here, I recommend looking into the work of Andrew Gelman Gelman (2018).↩︎"
  },
  {
    "objectID": "02-random-graphs/index.html",
    "href": "02-random-graphs/index.html",
    "title": "Random graphs",
    "section": "",
    "text": "In this section, we will focus on reviewing the most common random graph models, how these are used, and what things are important to consider when using them. Later on in the course, we will focus on Exponential-Family Random Graph Models [ERGMs], which are a generalization of the models we will discuss here."
  },
  {
    "objectID": "02-random-graphs/index.html#code-example",
    "href": "02-random-graphs/index.html#code-example",
    "title": "Random graphs",
    "section": "2.1 Code example",
    "text": "2.1 Code example\n\n# Model parameters\nn &lt;- 40\np &lt;- 0.1\n\n# Generating the graph, version 1\nset.seed(3312)\ng &lt;- matrix(as.integer(runif(n * n) &lt; p), nrow = n, ncol = n)\ndiag(g) &lt;- 0\n\n# Visualizing the network\nlibrary(igraph)\nlibrary(netplot)\nnplot(graph_from_adjacency_matrix(g))\n\n\n\n\nChallenge 1: How would you generate the graph using the two-step process described above?\nChallenge 2: Using a Generalized-Linear-Model [GLM], estimate p and its variance from the above network."
  },
  {
    "objectID": "02-random-graphs/index.html#code-example-1",
    "href": "02-random-graphs/index.html#code-example-1",
    "title": "Random graphs",
    "section": "3.1 Code example",
    "text": "3.1 Code example\n\n# Creating a ring\nn &lt;- 10\nV &lt;- 1:n\nk &lt;- 3\np &lt;- .2\n\nE &lt;- NULL\nfor (i in 1:k) {\n  E &lt;- rbind(E, cbind(V, c(V[-c(1:i)], V[1:i])))\n}\n\n# Generating the ring layout\nlo &lt;- layout_in_circle(graph_from_edgelist(E))\n\n# Plotting with netplot\nnplot(\n  graph_from_edgelist(E),\n  layout = lo\n  )\n\n\n\n# Rewiring\nids &lt;- which(runif(nrow(E)) &lt; p)\nE[ids, 2] &lt;- sample(V, length(ids), replace = TRUE)\nnplot(\n  graph_from_edgelist(E),\n  layout = lo\n  )"
  },
  {
    "objectID": "02-random-graphs/index.html#code-example-2",
    "href": "02-random-graphs/index.html#code-example-2",
    "title": "Random graphs",
    "section": "4.1 Code example",
    "text": "4.1 Code example\n\n# Model parameters\nn &lt;- 500\nm &lt;- 2\n\n# Generating the graph\nset.seed(3312)\ng &lt;- matrix(0, nrow = n, ncol = n)\ng[1:m, 1:m] &lt;- 1\ndiag(g) &lt;- 0\n\n# Adding nodes\nfor (i in (m + 1):n) {\n\n  # Selecting the nodes to connect to\n  ids &lt;- sample(\n    x       = 1:(i-1), # Up to i-1\n    size    = m,       # m nodes\n    replace = FALSE,   # No replacement\n    # Probability proportional to the degree\n    prob    = colSums(g[, 1:(i-1), drop = FALSE])\n    )\n\n  # Adding the edges\n  g[i, ids] &lt;- 1\n  g[ids, i] &lt;- 1\n\n}\n\n# Visualizing the degree distribution\nlibrary(ggplot2)\ndata.frame(degree = colSums(g)) |&gt;\n  ggplot(aes(degree)) +\n  geom_histogram() +\n  scale_x_log10() +\n  labs(\n    x = \"Degree\\n(log10 scale)\",\n    y = \"Count\"\n  )"
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html",
    "href": "03-behavior-and-coevolution/index.html",
    "title": "Behavior and coevolution",
    "section": "",
    "text": "This section focuses on inference involving network and a secondary outcome. While there are many ways of studying the coevolution or dependence between network and behavior, this section focuses on two classes of analysis: When the network is fixed and when both network and behavior influence each other.\nWhether we treat the network as given or endogenous sets the complexity of conducting statistical inference. Data analysis becomes much more straightforward if our research focuses on individual-level outcomes embedded in a network and not on the network itself. Here, we will deal with three particular cases: (1) when network effects are lagged, (2) egocentric networks, and (3) when network effects are contemporaneous."
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-lagged-exposure",
    "href": "03-behavior-and-coevolution/index.html#code-example-lagged-exposure",
    "title": "Behavior and coevolution",
    "section": "2.1 Code example: Lagged exposure",
    "text": "2.1 Code example: Lagged exposure\nThe following code example shows how to estimate a lagged exposure effect using the glm function in R. The model we will simulate and estimate features a Bernoulli graph with 1,000 nodes and a density of 0.01.\n\ny_{it} = \\theta_1 + \\rho \\text{Exposure}_{it} + \\theta_2 w_i + \\varepsilon\n\nwhere \\text{Exposure}_{it} is the exposure statistic defined above, and w_i is a vector of covariates.\n\n# Simulating data\nn &lt;- 1000\ntime &lt;- 2\ntheta &lt;- c(-1, 3)\n\n# Sampling a bernoilli network\nset.seed(3132)\np &lt;- 0.01\nX &lt;- matrix(rbinom(n^2, 1, p), nrow = n)\ndiag(X) &lt;- 0\n\n# Covariate\nW &lt;- matrix(rnorm(n), nrow = n)\n\n# Simulating the outcome\nrho &lt;- 0.5\nY0 &lt;- cbind(rnorm(n))\n\n# The lagged exposure\nexpo &lt;- (X %*% Y0)/rowSums(X)\nY1 &lt;- theta[1] + rho * expo + W * theta[2] + rnorm(n)\n\nNow we fit the model using GLM, in this case, linear Regression\n\nfit &lt;- glm(Y1 ~ expo + W, family = \"gaussian\")\nsummary(fit)\n\n\nCall:\nglm(formula = Y1 ~ expo + W, family = \"gaussian\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.07187    0.03284 -32.638  &lt; 2e-16 ***\nexpo         0.61170    0.10199   5.998  2.8e-09 ***\nW            3.00316    0.03233  92.891  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.071489)\n\n    Null deviance: 10319.3  on 999  degrees of freedom\nResidual deviance:  1068.3  on 997  degrees of freedom\nAIC: 2911.9\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-egocentric-networks",
    "href": "03-behavior-and-coevolution/index.html#code-example-egocentric-networks",
    "title": "Behavior and coevolution",
    "section": "3.1 Code example: Egocentric networks",
    "text": "3.1 Code example: Egocentric networks\nFor this example, we will simulate a stream of 1,000 Bernoulli graphs looking into the probability of school dropout. Each network will have between 4 and 10 nodes and have a density of 0.4. The data-generating process is as follows:\n\n{\\Pr{}}_{\\bm{\\theta}}\\left(Y_i=1\\right) = \\text{logit}^{-1}\\left(\\bm{\\theta}_x s(\\bm{X}_i) \\right)\n\nWhere s(X) \\equiv \\left(\\text{density}, \\text{n mutual ties}\\right), and \\bm{\\theta}_x = (0.5, -1). This model only features sufficient statistics. We start by simulating the networks\n\nset.seed(331)\nn &lt;- 1000\nsizes &lt;- sample(4:10, n, replace = TRUE)\n\n# Simulating the networks\nX &lt;- lapply(sizes, function(x) matrix(rbinom(x^2, 1, 0.4), nrow = x))\nX &lt;- lapply(X, \\(x) {diag(x) &lt;- 0; x})\n\n# Inspecting the first 5\nhead(X, 5)\n\n[[1]]\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    1    1    1    0\n[2,]    0    0    0    0    0\n[3,]    0    1    0    0    0\n[4,]    0    0    0    0    0\n[5,]    1    0    0    1    0\n\n[[2]]\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    1    0    1    0\n\n[[3]]\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    0    1    0    1    0    0\n[2,]    0    0    0    0    0    0\n[3,]    0    1    0    0    0    1\n[4,]    0    0    0    0    1    0\n[5,]    0    0    0    0    0    0\n[6,]    0    0    0    0    0    0\n\n[[4]]\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    1    0    1    0\n[2,]    0    0    0    0    1\n[3,]    0    1    0    0    0\n[4,]    0    1    1    0    1\n[5,]    1    0    1    0    0\n\n[[5]]\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    1    0    0    0    0\n[3,]    1    0    0    0    0\n[4,]    0    0    0    0    0\n[5,]    1    0    0    0    0\n\n\nUsing the ergm R package (Handcock et al. 2023; Hunter et al. 2008), we can extract the associated sufficient statistics of the egocentric networks:\n\nlibrary(ergm)\nstats &lt;- lapply(X, \\(x) summary_formula(x ~ density + mutual))\n\n# Turning the list into a matrix\nstats &lt;- do.call(rbind, stats)\n\n# Inspecting the first 5\nhead(stats, 5)\n\n       density mutual\n[1,] 0.3000000      0\n[2,] 0.1666667      0\n[3,] 0.1666667      0\n[4,] 0.4500000      0\n[5,] 0.1500000      0\n\n\nWe now simulate the outcomes\n\ny &lt;- rbinom(n, 1, plogis(stats %*% c(0.5, -1)))\nglm(y ~ stats, family = binomial(link = \"logit\")) |&gt;\n  summary()\n\n\nCall:\nglm(formula = y ~ stats, family = binomial(link = \"logit\"))\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.07319    0.41590   0.176    0.860    \nstatsdensity  0.42568    1.26942   0.335    0.737    \nstatsmutual  -1.14804    0.12166  -9.436   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 768.96  on 999  degrees of freedom\nResidual deviance: 518.78  on 997  degrees of freedom\nAIC: 524.78\n\nNumber of Fisher Scoring iterations: 7"
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-sar",
    "href": "03-behavior-and-coevolution/index.html#code-example-sar",
    "title": "Behavior and coevolution",
    "section": "4.1 Code example: SAR",
    "text": "4.1 Code example: SAR\nSimulation of SAR models can be done using the following observation: Although the outcome shows on both sides of the equation, we can isolate it in one side and solve for it; formally:\n\n\\bm{y} = \\rho \\bm{X} \\bm{y} + \\bm{\\theta}^{\\mathbf{t}}\\bm{W} + \\varepsilon \\implies \\bm{y} = \\left(\\bm{I} - \\rho \\bm{X}\\right)^{-1}\\bm{\\theta}^{\\mathbf{t}}\\bm{W} + \\left(\\bm{I} - \\rho \\bm{X}\\right)^{-1}\\varepsilon\n\nThe following code chunk simulates a SAR model with a Bernoulli graph with 1,000 nodes and a density of 0.01. The data-generating process is as follows:\n\nset.seed(4114)\nn &lt;- 1000\n\n# Simulating the network\np &lt;- 0.01\nX &lt;- matrix(rbinom(n^2, 1, p), nrow = n)\n\n# Covariate\nW &lt;- matrix(rnorm(n), nrow = n)\n\n# Simulating the outcome\nrho &lt;- 0.5\nlibrary(MASS) # For the mvrnorm function\n\n# Identity minus rho * X\nX_rowstoch &lt;- X / rowSums(X)\nI &lt;- diag(n) - rho * X_rowstoch\n\n# The outcome\nY &lt;- solve(I) %*% (2 * W) + solve(I) %*% mvrnorm(1, rep(0, n), diag(n))\n\nUsing the spatialreg R package, we can fit the model using the lagsarlm function:\n\nlibrary(spdep) # for the function mat2listw\nlibrary(spatialreg)\nfit &lt;- lagsarlm(\n  Y ~ W,\n  data  = as.data.frame(X),\n  listw = mat2listw(X_rowstoch)\n  )\n\n\n# Using texreg to get a pretty print\ntexreg::knitreg(fit, single.row = TRUE)\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n-0.01 (0.03)\n\n\n\n\nW\n\n\n1.97 (0.03)***\n\n\n\n\nrho\n\n\n0.54 (0.04)***\n\n\n\n\nNum. obs.\n\n\n1000\n\n\n\n\nParameters\n\n\n4\n\n\n\n\nLog Likelihood\n\n\n-1373.02\n\n\n\n\nAIC (Linear model)\n\n\n2920.37\n\n\n\n\nAIC (Spatial model)\n\n\n2754.05\n\n\n\n\nLR test: statistic\n\n\n168.32\n\n\n\n\nLR test: p-value\n\n\n0.00\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\nThe interpretation of this model is almost the same as a linear regression, with the difference that we have the autocorrelation effect (rho). As expected, the model got an estimate close enough to the population parameter: \\rho = 0.5."
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-alaam",
    "href": "03-behavior-and-coevolution/index.html#code-example-alaam",
    "title": "Behavior and coevolution",
    "section": "4.2 Code example: ALAAM",
    "text": "4.2 Code example: ALAAM\nTo date, there is no R package implementing the ALAAM framework. Nevertheless, you can fit ALAAMs using the PNet software developed by the Melnet group at the University of Melbourne (click here).\nBecause of the similarities, ALAAMs can be implemented using ERGMs. Because of the novelty of it, the coding example will be left as a potential class project. We will post a fully-featured example after the workshop."
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-siena",
    "href": "03-behavior-and-coevolution/index.html#code-example-siena",
    "title": "Behavior and coevolution",
    "section": "5.1 Code example: Siena",
    "text": "5.1 Code example: Siena\nThis example was adapted from the RSiena R package (see ?sienaGOF-auxiliary page).We start by loading the package and taking a look at the data we will use:\n\nlibrary(RSiena)\n\n# Visualizing the adjacency matrix & behavior\nop &lt;- par(mfrow=c(2, 2))\nimage(s501, main = \"Net: s501\")\nimage(s502, main = \"Net: s502\")\nhist(s50a[,1], main = \"Beh1\")\nhist(s50a[,2], main = \"Beh2\")\n\n\n\npar(op)\n\nThe next step is the data preparation process. RSiena does not receive raw data as is. We need to explicitly declare the networks and outcome variable. Siena models can also model network changes\n\n# Initializing the dependent variable (network)\nmynet1 &lt;- sienaDependent(array(c(s501, s502), dim=c(50, 50, 2)))\nmynet1\n\nType         oneMode             \nObservations 2                   \nNodeset      Actors (50 elements)\n\nmybeh  &lt;- sienaDependent(s50a[,1:2], type=\"behavior\")\nmybeh\n\nType         behavior            \nObservations 2                   \nNodeset      Actors (50 elements)\n\n# Node-level covariates (artificial)\nmycov  &lt;- c(rep(1:3,16),1,2)\n\n# Edge-level covariates (also artificial)\nmydycov &lt;- matrix(rep(1:5, 500), 50, 50) \n\n\n# Creating the data object\nmydata &lt;- sienaDataCreate(mynet1, mybeh)\n\n# Adding the effects (first get them!)\nmyeff &lt;- getEffects(mydata)\n\n# Notice that Siena adds some default effects\nmyeff\n##   name   effectName                  include fix   test  initialValue parm\n## 1 mynet1 basic rate parameter mynet1 TRUE    FALSE FALSE    4.69604   0   \n## 2 mynet1 outdegree (density)         TRUE    FALSE FALSE   -1.48852   0   \n## 3 mynet1 reciprocity                 TRUE    FALSE FALSE    0.00000   0   \n## 4 mybeh  rate mybeh period 1         TRUE    FALSE FALSE    0.70571   0   \n## 5 mybeh  mybeh linear shape          TRUE    FALSE FALSE    0.32247   0   \n## 6 mybeh  mybeh quadratic shape       TRUE    FALSE FALSE    0.00000   0\n\n# Adding a few extra effects (automatically prints them out)\nmyeff &lt;- includeEffects(myeff, transTies, cycle3)\n##   effectName      include fix   test  initialValue parm\n## 1 3-cycles        TRUE    FALSE FALSE          0   0   \n## 2 transitive ties TRUE    FALSE FALSE          0   0\n\nTo add more effects, first, call the function effectsDocumentation(myeff). It will show you explicitly how to add a particular effect. For instance, if we wanted to add network exposure (avExposure,) under the documentation of effectsDocumentation(myeff) we need to pass the following arguments:\n\n# And now, exposure effect\nmyeff &lt;- includeEffects(\n  myeff,\n  avExposure,\n  # These last three are specified by effectsDocum...\n  name         = \"mybeh\",\n  interaction1 = \"mynet1\",\n  type         = \"rate\"\n  )\n\n  effectName                            include fix   test  initialValue parm\n1 average exposure effect on rate mybeh TRUE    FALSE FALSE          0   0   \n\n\nThe next step involves creating the model with (sienaAlgorithmCreate,) where we specify all the parameters for fitting the model (e.g., MCMC steps.) Here, we modified the values of n3 and nsub to half of the default values to reduce the time it would take to fit the model; yet this degrades the quality of the fit.\n\n# Shorter phases 2 and 3, just for example:\nmyalgorithm &lt;- sienaAlgorithmCreate(\n  nsub = 2, n3 = 500, seed = 122, projname = NULL\n  )\n## If you use this algorithm object, siena07 will create/use an output file Siena.txt .\n \n# Fitting and printing the model\nans &lt;- siena07(\n  myalgorithm,\n  data = mydata, effects = myeff,\n  returnDeps = TRUE, batch = TRUE\n  )\n## \n## Start phase 0 \n## theta:  4.696 -1.489  0.000  0.000  0.000  0.706  0.000  0.322  0.000 \n## \n## Start phase 1 \n## Phase 1 Iteration 1 Progress: 0%\n## Phase 1 Iteration 2 Progress: 0%\n## Phase 1 Iteration 3 Progress: 0%\n## Phase 1 Iteration 4 Progress: 0%\n## Phase 1 Iteration 5 Progress: 0%\n## Phase 1 Iteration 10 Progress: 1%\n## Phase 1 Iteration 15 Progress: 1%\n## Phase 1 Iteration 20 Progress: 1%\n## Phase 1 Iteration 25 Progress: 2%\n## Phase 1 Iteration 30 Progress: 2%\n## Phase 1 Iteration 35 Progress: 2%\n## Phase 1 Iteration 40 Progress: 3%\n## Phase 1 Iteration 45 Progress: 3%\n## Phase 1 Iteration 50 Progress: 3%\n## theta:  5.380 -1.734  0.481  0.147  0.166  1.168 -0.340  0.250  0.140 \n## \n## Start phase 2.1\n## Phase 2 Subphase 1 Iteration 1 Progress: 33%\n## Phase 2 Subphase 1 Iteration 2 Progress: 33%\n## theta  6.044 -1.877  0.840  0.300  0.473  1.096 -0.332  0.112  0.191 \n## ac  0.375 -0.183  3.835  4.574 23.412  0.922  0.908  0.626  3.302 \n## Phase 2 Subphase 1 Iteration 3 Progress: 33%\n## Phase 2 Subphase 1 Iteration 4 Progress: 33%\n## theta  6.1075 -2.0372  1.2512  0.0341  0.5207  1.2593 -0.1534  0.3018  0.1928 \n## ac  0.9859  0.2280 -0.0703 -2.2973 -2.7455  1.1518  1.0749  0.8732  0.5399 \n## Phase 2 Subphase 1 Iteration 5 Progress: 33%\n## Phase 2 Subphase 1 Iteration 6 Progress: 33%\n## theta  6.8650 -2.3185  1.7577  0.2694  0.7636  1.1997 -0.0433  0.3000  0.1762 \n## ac  0.4789  0.4883  0.0199 -1.9449 -2.2609  1.1444  1.0397  0.9282  0.6470 \n## Phase 2 Subphase 1 Iteration 7 Progress: 33%\n## Phase 2 Subphase 1 Iteration 8 Progress: 33%\n## theta  6.801140 -2.485273  1.966471  0.301197  0.817324  1.228010  0.000627  0.377719  0.072274 \n## ac  0.4538  0.4933 -0.0158 -1.9326 -2.2588  1.1102  1.0395  0.9239  0.6905 \n## Phase 2 Subphase 1 Iteration 9 Progress: 33%\n## Phase 2 Subphase 1 Iteration 10 Progress: 33%\n## theta  6.1258 -2.5124  1.8704  0.1206  0.6054  1.4538 -0.0145  0.5091 -0.0671 \n## ac  0.472  0.103 -0.330 -1.625 -1.991  1.152  1.090  0.767  0.514 \n## Phase 2 Subphase 1 Iteration 1 Progress: 33%\n## Phase 2 Subphase 1 Iteration 2 Progress: 33%\n## Phase 2 Subphase 1 Iteration 3 Progress: 33%\n## Phase 2 Subphase 1 Iteration 4 Progress: 33%\n## Phase 2 Subphase 1 Iteration 5 Progress: 33%\n## Phase 2 Subphase 1 Iteration 6 Progress: 34%\n## Phase 2 Subphase 1 Iteration 7 Progress: 34%\n## Phase 2 Subphase 1 Iteration 8 Progress: 34%\n## Phase 2 Subphase 1 Iteration 9 Progress: 34%\n## Phase 2 Subphase 1 Iteration 10 Progress: 34%\n## theta  6.4976 -2.5900  1.9265  0.2750  0.8543  1.0420  0.0446  0.3234 -0.0686 \n## ac -0.212 -0.697 -0.818 -0.831 -0.765 -0.442 -0.268 -0.240 -0.267 \n## theta:  6.4976 -2.5900  1.9265  0.2750  0.8543  1.0420  0.0446  0.3234 -0.0686 \n## \n## Start phase 2.2\n## Phase 2 Subphase 2 Iteration 1 Progress: 48%\n## Phase 2 Subphase 2 Iteration 2 Progress: 48%\n## Phase 2 Subphase 2 Iteration 3 Progress: 48%\n## Phase 2 Subphase 2 Iteration 4 Progress: 48%\n## Phase 2 Subphase 2 Iteration 5 Progress: 48%\n## Phase 2 Subphase 2 Iteration 6 Progress: 48%\n## Phase 2 Subphase 2 Iteration 7 Progress: 49%\n## Phase 2 Subphase 2 Iteration 8 Progress: 49%\n## Phase 2 Subphase 2 Iteration 9 Progress: 49%\n## Phase 2 Subphase 2 Iteration 10 Progress: 49%\n## theta  6.7988 -2.5810  1.9335  0.3941  0.7843  1.1111  0.0358  0.3252 -0.0415 \n## ac -0.0757 -0.3672 -0.4025 -0.3676 -0.4157 -0.0166  0.0222 -0.0874  0.0651 \n## theta:  6.7988 -2.5810  1.9335  0.3941  0.7843  1.1111  0.0358  0.3252 -0.0415 \n## \n## Start phase 3 \n## Phase 3 Iteration 500 Progress 100%\n\nans\n## Estimates, standard errors and convergence t-ratios\n## \n##                                                 Estimate   Standard   Convergence \n##                                                              Error      t-ratio   \n## Network Dynamics \n##   1. rate basic rate parameter mynet1            6.7988  ( 1.2496   )   -0.0460   \n##   2. eval outdegree (density)                   -2.5810  ( 0.1505   )    0.0113   \n##   3. eval reciprocity                            1.9335  ( 0.2627   )    0.0205   \n##   4. eval 3-cycles                               0.3941  ( 0.2742   )   -0.0658   \n##   5. eval transitive ties                        0.7843  ( 0.2379   )   -0.0014   \n## \n## Behavior Dynamics\n##   6. rate rate mybeh period 1                    1.1111  ( 1.3096   )   -0.0199   \n##   7. rate average exposure effect on rate mybeh  0.0358  ( 0.4373   )   -0.0190   \n##   8. eval mybeh linear shape                     0.3252  ( 0.2329   )   -0.0048   \n##   9. eval mybeh quadratic shape                 -0.0415  ( 0.1139   )    0.0992   \n## \n## Overall maximum convergence ratio:    0.2163 \n## \n## \n## Total of 940 iteration steps.\n\nAs a rule of thumb, absolute t-values below 0.1 show good convergence, below 0.2 we say “reasonably well,” and above is no convergence. Let’s highlight two of the effects we have in our model\n\nTransitive ties (number five) are positive 0.78 with a t-value of smaller than 0.01. Thus, we say that the network has a tendency towards transitivity (balance) that is significant.\nExposure effect (number seven) is also positive, but small, 0.03, but still significant (t-value of -0.01)\n\n:Like with ERGMs, we also do goodness-of-fit:\n\nsienaGOF(ans, OutdegreeDistribution, varName=\"mynet1\") |&gt;\n  plot()\n\n\n\nsienaGOF(ans, BehaviorDistribution, varName = \"mybeh\") |&gt; \n  plot()\n\nNote: some statistics are not plotted because their variance is 0.\nThis holds for the statistic: 5."
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-ernm",
    "href": "03-behavior-and-coevolution/index.html#code-example-ernm",
    "title": "Behavior and coevolution",
    "section": "5.2 Code example: ERNM",
    "text": "5.2 Code example: ERNM\nTo date, there is no CRAN release for the ERNM model. The only implementation I am aware of is one of the leading authors, which is available on GitHub: https://github.com/fellstat/ernm. Unfortunately, the current version of the package seems to be broken.\nJust like the ALAAM case, as ERNMs are closely related to ERGMS, building an example using the ERGM package could be a great opportunity for a class project!"
  },
  {
    "objectID": "04-lab-1/index.html",
    "href": "04-lab-1/index.html",
    "title": "Lab 1",
    "section": "",
    "text": "1 Types of problems\nOverall, we can classify the problems according to two dimensions:\n\nBehavior Is the individual level behavior random or deterministic?\nNetwork Is the network random or deterministic?\n\nOf the following cases, which ones can be treated with “regular statistical methods”?\n\n\nEgocentric study 1: Analyze how the network structure affects health outcomes.\nEgocentric study 2: Investigate what network structures are more prevalent in a population sample.\nEgocentric study 3: Elucidate whether the prevalence of a given network structure is higher in one population than in another.\nCountry-level networks: Analyze whether neighboring countries tend to adopt international treaties at the same time.\nPhylogenomics: Study a given phenotype in a population of organisms related by a phylogenetic tree.\n\n\n\n\n2 Programming\nSimulating convolutions: Using what you have learned about statistical functions in R, simulate the convolution of two normal distributions, one with (\\mu, \\sigma^2) = (-3, 1) and the other with (\\mu, \\sigma^2) = (2, 2). Plot the histogram of the results. Draw 1,000 samples.\n\n\nCode\nset.seed(1)\nx &lt;- rnorm(1000, mean = -3, sd = 1)\ny &lt;- rnorm(1000, mean = 2, sd = 2)\nz &lt;- x + y\n\nhist(z)\n\n\nBimodal distribution: Using the previous two normal distributions, simulate a bimodal distribution where the probability of sampling from the first distribution is 0.3 and the probability of sampling from the second distribution is 0.7. Plot the histogram of the results. (Hint: use a combination of runif() and ifelse()).\n\n\nCode\nz &lt;- ifelse(runif(1000) &lt; 0.3, x, y)\ndensity(z) |&gt; plot()"
  },
  {
    "objectID": "05-intro-to-ergms/index.html",
    "href": "05-intro-to-ergms/index.html",
    "title": "Introduction to ERGMs",
    "section": "",
    "text": "Have you ever wondered if the friend of your friend is your friend? Or if the people you consider to be your friends feel the same about you? Or if age is related to whether you seek advice from others? All these (and many others certainly more creative) questions can be answered using Exponential-Family Random Graph Models."
  },
  {
    "objectID": "05-intro-to-ergms/index.html#the-logistic-distribution",
    "href": "05-intro-to-ergms/index.html#the-logistic-distribution",
    "title": "Introduction to ERGMs",
    "section": "3.1 The logistic distribution",
    "text": "3.1 The logistic distribution\nLet’s start by stating the result: Conditioning on all graphs that are not y_{ij}, the probability of a tie Y_{ij} is distributed Logistic; formally:\n\nP_{\\mathcal{Y}, \\bm{\\theta}}(Y_{ij}=1 | \\bm{y}_{-ij}) = \\frac{1}{1 + \\exp \\left(\\bm{\\theta}^{\\mathbf{t}}\\delta_{ij}(\\bm{y}){}\\right)},\n\nwhere \\delta_{ij}(\\bm{y}){}\\equiv s_{ij}^+(\\bm{y}) - s_{ij}^-(\\bm{y}) is the change statistic, and s_{ij}^+(\\bm{y}) and s_{ij}^-(\\bm{y}) are the statistics of the graph with and without the tie Y_{ij}, respectively.\nThe importance of this result is two-fold: (a) we can use this equation to interpret fitted models in the context of a single graph (like using odds,) and (b) we can use this equation to simulate from the model, without touching the normalizing constant."
  },
  {
    "objectID": "05-intro-to-ergms/index.html#the-ratio-of-loglikelihoods",
    "href": "05-intro-to-ergms/index.html#the-ratio-of-loglikelihoods",
    "title": "Introduction to ERGMs",
    "section": "3.2 The ratio of loglikelihoods",
    "text": "3.2 The ratio of loglikelihoods\nThe second significant result is that the ratio of loglikelihoods can be approximated through simulation. It is based on the following observation by Geyer and Thompson (1992):\n\n\\frac{\\kappa(\\bm{\\theta})}{\\kappa(\\bm{\\theta}_0)} = \\mathbb{E}_{\\mathcal{Y}, \\bm{\\theta}_0}\\left((\\bm{\\theta} - \\bm{\\theta}_0)s(\\bm{y})^{\\mathbf{t}}\\right),\n\nUsing the latter, we can approximate the following loglikelihood ratio:\n\\begin{align*}\nl(\\bm{\\theta}) - l(\\bm{\\theta}_0) = & (\\bm{\\theta} - \\bm{\\theta}_0)^{\\mathbf{t}}s(\\bm{y}) - \\log\\left[\\frac{\\kappa(\\bm{\\theta})}{\\kappa(\\bm{\\theta}_0)}\\right]\\\\\n\\approx & (\\bm{\\theta} - \\bm{\\theta}_0)^{\\mathbf{t}}s(\\bm{y}) - \\log\\left[M^{-1}\\sum_{\\bm{y}^{(m)}} (\\bm{\\theta} - \\bm{\\theta}_0)^{\\mathbf{t}}s(\\bm{y}^{(m)})\\right]\n\\end{align*}\nWhere \\bm{\\theta}_0 is an arbitrary vector of parameters, and \\bm{y}^{(m)} are sampled from the distribution P_{\\mathcal{Y}, \\bm{\\theta}_0}. In the words of Geyer and Thompson (1992), “[…] it is possible to approximate \\bm{\\theta} by using simulations from one distribution P_{\\mathcal{Y}, \\bm{\\theta}_0} no matter which \\bm{\\theta}_0 in the parameter space is.”"
  },
  {
    "objectID": "05-intro-to-ergms/index.html#inspect-the-data",
    "href": "05-intro-to-ergms/index.html#inspect-the-data",
    "title": "Introduction to ERGMs",
    "section": "4.1 Inspect the data",
    "text": "4.1 Inspect the data\nFor the sake of time, we will not take the time to investigate our network properly. However, you should always do so. Make sure you do descriptive statistics (density, centrality, modularity, etc.), check missing values, isolates (disconnected nodes), and inspect your data visually through “notepad” and visualizations before jumping into your ERG model."
  },
  {
    "objectID": "05-intro-to-ergms/index.html#start-with-endogenous-effects-first",
    "href": "05-intro-to-ergms/index.html#start-with-endogenous-effects-first",
    "title": "Introduction to ERGMs",
    "section": "4.2 Start with endogenous effects first",
    "text": "4.2 Start with endogenous effects first\nThe step is to check whether we can fit an ERGM or not. We can do so with the Bernoulli graph:\n\nmodel_1 &lt;- ergm(Y ~ edges)\nsummary(model_1)\n\nCall:\nergm(formula = Y ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -3.78885    0.06833      0  -55.45   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 13724  on 9900  degrees of freedom\n Residual Deviance:  2102  on 9899  degrees of freedom\n \nAIC: 2104  BIC: 2112  (Smaller is better. MC Std. Err. = 0)\n\n\nIt is rare to see a model in which the edgecount is not significant. The next term we will add is reciprocity (mutual in the ergm package)\n\nmodel_2 &lt;- ergm(Y ~ edges + mutual) \nsummary(model_2)\n## Call:\n## ergm(formula = Y ~ edges + mutual)\n## \n## Monte Carlo Maximum Likelihood Results:\n## \n##        Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \n## edges  -3.93277    0.07833      0 -50.205   &lt;1e-04 ***\n## mutual  2.15152    0.31514      0   6.827   &lt;1e-04 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 13724  on 9900  degrees of freedom\n##  Residual Deviance:  2064  on 9898  degrees of freedom\n##  \n## AIC: 2068  BIC: 2083  (Smaller is better. MC Std. Err. = 0.8083)\n\nAs expected, reciprocity is significant (we made it like this!.) Notwithstanding, there is a difference between this model and the previous one. This model was not fitted using MLE. Instead, since the reciprocity term involves more than one tie, the model cannot be reduced to a Logistic regression, so it needs to be estimated using one of the other available estimation methods in the ergm package.\nThe model starts gaining complexity as we add higher-order terms involving more ties. An infamous example is the number of triangles. Although highly important for social sciences, including triangle terms in your ERGMs results in a degenerate model (when the MCMC chain jumps between empty and fully connected graphs). One exception is if you deal with small networks. To address this, Snijders et al. (2006) and Hunter (2007) introduced various new terms that significantly reduce the risk of degeneracy. Here, we will illustrate the use of the term “geometrically weighted dyadic shared partner” (gwdsp,) which Prof. David Hunter proposed. The gwdsp term is akin to triadic closure but reduces the chances of degeneracy.\n\n# Fitting two more models (output message suppressed)\nmodel_3 &lt;- ergm(Y ~ edges + mutual + gwdsp(.5, fixed = TRUE))\n# model_4 &lt;- ergm(Y ~ edges + triangles) # bad idea\n\nRight after fitting a model, we want to inspect the results. An excellent tool for this is the R package texreg (Leifeld 2013):\nlibrary(texreg)\nknitreg(list(model_1, model_2, model_3))\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\n\n\n\nedges\n\n\n-3.79***\n\n\n-3.93***\n\n\n-3.84***\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.08)\n\n\n(0.21)\n\n\n\n\nmutual\n\n\n \n\n\n2.15***\n\n\n2.14***\n\n\n\n\n \n\n\n \n\n\n(0.32)\n\n\n(0.29)\n\n\n\n\ngwdsp.OTP.fixed.0.5\n\n\n \n\n\n \n\n\n-0.02\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.05)\n\n\n\n\nAIC\n\n\n2104.43\n\n\n2068.22\n\n\n2071.94\n\n\n\n\nBIC\n\n\n2111.63\n\n\n2082.62\n\n\n2093.54\n\n\n\n\nLog Likelihood\n\n\n-1051.22\n\n\n-1032.11\n\n\n-1032.97\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nSo far, model_2 is winning. We will continue with this model."
  },
  {
    "objectID": "05-intro-to-ergms/index.html#lets-add-a-little-bit-of-structure",
    "href": "05-intro-to-ergms/index.html#lets-add-a-little-bit-of-structure",
    "title": "Introduction to ERGMs",
    "section": "4.3 Let’s add a little bit of structure",
    "text": "4.3 Let’s add a little bit of structure\nNow that we only have a model featuring endogenous terms, we can add vertex/edge-covariates. Starting with \"fav_music\", there are a couple of different ways to use this node feature:\n\nDirectly through homophily (assortative mixing): Using the nodematch term, we can control for the propensity of individuals to connect based on shared music taste.\nHomophily (v2): We could activate the option diff = TRUE using the same term. By doing this, the homophily term is operationalized differently, adding as many terms as options in the vertex attribute.\nMixing: We can use the term nodemix for individuals’ tendency to mix between musical tastes.\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn this context, what would be the different hypotheses behind each decision?\n\n\n\nmodel_4 &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\"))\nmodel_5 &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\", diff = TRUE))\nmodel_6 &lt;- ergm(Y ~ edges + mutual + nodemix(\"fav_music\"))\n\nNow, let’s inspect what we have so far:\nknitreg(list(`2` = model_2, `4` = model_4, `5` = model_5, `6` = model_6))\n\n\n\nStatistical models\n\n\n\n\n \n\n\n2\n\n\n4\n\n\n5\n\n\n6\n\n\n\n\n\n\nedges\n\n\n-3.93***\n\n\n-4.29***\n\n\n-4.29***\n\n\n-3.56***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.11)\n\n\n(0.11)\n\n\n(0.24)\n\n\n\n\nmutual\n\n\n2.15***\n\n\n1.99***\n\n\n2.00***\n\n\n2.02***\n\n\n\n\n \n\n\n(0.32)\n\n\n(0.30)\n\n\n(0.30)\n\n\n(0.30)\n\n\n\n\nnodematch.fav_music\n\n\n \n\n\n0.85***\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.14)\n\n\n \n\n\n \n\n\n\n\nnodematch.fav_music.jazz\n\n\n \n\n\n \n\n\n0.74**\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.25)\n\n\n \n\n\n\n\nnodematch.fav_music.pop\n\n\n \n\n\n \n\n\n0.82***\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.18)\n\n\n \n\n\n\n\nnodematch.fav_music.rock\n\n\n \n\n\n \n\n\n0.87***\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.17)\n\n\n \n\n\n\n\nmix.fav_music.pop.jazz\n\n\n \n\n\n \n\n\n \n\n\n-0.54\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.34)\n\n\n\n\nmix.fav_music.rock.jazz\n\n\n \n\n\n \n\n\n \n\n\n-0.75*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.38)\n\n\n\n\nmix.fav_music.jazz.pop\n\n\n \n\n\n \n\n\n \n\n\n-0.60\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.35)\n\n\n\n\nmix.fav_music.pop.pop\n\n\n \n\n\n \n\n\n \n\n\n0.10\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.27)\n\n\n\n\nmix.fav_music.rock.pop\n\n\n \n\n\n \n\n\n \n\n\n-0.50\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.31)\n\n\n\n\nmix.fav_music.jazz.rock\n\n\n \n\n\n \n\n\n \n\n\n-1.40**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.44)\n\n\n\n\nmix.fav_music.pop.rock\n\n\n \n\n\n \n\n\n \n\n\n-0.86*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.33)\n\n\n\n\nmix.fav_music.rock.rock\n\n\n \n\n\n \n\n\n \n\n\n0.15\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.27)\n\n\n\n\nAIC\n\n\n2068.22\n\n\n2030.85\n\n\n2033.57\n\n\n2036.37\n\n\n\n\nBIC\n\n\n2082.62\n\n\n2052.45\n\n\n2069.57\n\n\n2108.38\n\n\n\n\nLog Likelihood\n\n\n-1032.11\n\n\n-1012.42\n\n\n-1011.79\n\n\n-1008.19\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nAlthough model 5 has a higher loglikelihood, using AIC or BIC suggests model 4 is a better candidate. For the sake of time, we will jump ahead and add nodematch(\"female\") as the last term of our model. The next step is to assess (a) convergence and (b) goodness-of-fit.\nmodel_final &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\") + nodematch(\"female\"))\n\n# Printing the pretty table\nknitreg(list(`2` = model_2, `4` = model_4, `Final` = model_final))\n\n\n\nStatistical models\n\n\n\n\n \n\n\n2\n\n\n4\n\n\nFinal\n\n\n\n\n\n\nedges\n\n\n-3.93***\n\n\n-4.29***\n\n\n-3.95***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.11)\n\n\n(0.12)\n\n\n\n\nmutual\n\n\n2.15***\n\n\n1.99***\n\n\n1.86***\n\n\n\n\n \n\n\n(0.32)\n\n\n(0.30)\n\n\n(0.33)\n\n\n\n\nnodematch.fav_music\n\n\n \n\n\n0.85***\n\n\n0.81***\n\n\n\n\n \n\n\n \n\n\n(0.14)\n\n\n(0.14)\n\n\n\n\nnodematch.female\n\n\n \n\n\n \n\n\n-0.74***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.15)\n\n\n\n\nAIC\n\n\n2068.22\n\n\n2030.85\n\n\n2002.18\n\n\n\n\nBIC\n\n\n2082.62\n\n\n2052.45\n\n\n2030.98\n\n\n\n\nLog Likelihood\n\n\n-1032.11\n\n\n-1012.42\n\n\n-997.09\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "05-intro-to-ergms/index.html#footnotes",
    "href": "05-intro-to-ergms/index.html#footnotes",
    "title": "Introduction to ERGMs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile ERG Models can be used to predict individual ties (which is another way of describing them), the focus is on the processes that give origin to the network.↩︎"
  },
  {
    "objectID": "06-ergms-cont/index.html",
    "href": "06-ergms-cont/index.html",
    "title": "Convergence on ERGMs",
    "section": "",
    "text": "Let’s continue what we left off in the previous session: Evaluating our ERGMs. Like most models, ERGMs are a mix between art and science. Here is again a list of steps/considerations to have when fitting ERGMs:\n\nInspect the data.\nStart with endogenous effects first.\nAfter structure is controlled.\nEvaluate your results: Once you have a model you are happy with, the last couple of steps are (a) assess convergence (which is usually done automagically by the ergm package,) and (b) assess goodness-of-fit, which in this context means how good was our model to capture (not-controlled for) properties of the network.\n\nWe are in the last step. Let’s recall what was our final model with the simulated data (code folded intentionally):\n\n\nCode\n# Loading the packages\nlibrary(ergm)\nlibrary(sna)\n\n# Simulating the covariates (vertex attribute)\nset.seed(1235)\n\n# Simulating the data\nY &lt;- network(100, directed = TRUE)\nY %v% \"fav_music\" &lt;- sample(c(\"rock\", \"jazz\", \"pop\"), 100, replace = TRUE)\nY %v% \"female\"    &lt;- rbinom(100, 1, 0.5) \n\n# Simulating the ERGM\nY &lt;- ergm::simulate_formula(\n  Y ~\n    edges +\n    nodematch(\"fav_music\") + \n    nodematch(\"female\") +\n    mutual,\n  coef = c(-4, 1, -1, 2)\n  )\n\n\n\ngplot(Y, vertex.col = c(\"green\", \"red\")[Y %v% \"female\" + 1])\n\n\n\nmodel_final &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\") + nodematch(\"female\"))\n\nsummary(model_final)\n\nCall:\nergm(formula = Y ~ edges + mutual + nodematch(\"fav_music\") + \n    nodematch(\"female\"))\n\nMonte Carlo Maximum Likelihood Results:\n\n                    Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges                -3.9573     0.1112      0 -35.588   &lt;1e-04 ***\nmutual                1.8950     0.2853      0   6.643   &lt;1e-04 ***\nnodematch.fav_music   0.8295     0.1291      0   6.426   &lt;1e-04 ***\nnodematch.female     -0.7436     0.1379      0  -5.394   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 13724  on 9900  degrees of freedom\n Residual Deviance:  1994  on 9896  degrees of freedom\n \nAIC: 2002  BIC: 2031  (Smaller is better. MC Std. Err. = 0.8189)\n\n\n\n1 Convergence\nAs our model was fitted using MCMC, we must ensure the chains converged. We can use the mcmc.diagnostics function from the ergm package to check model convergence. This function looks at the last set of simulations of the MCMC model and generates various diagnostics for the user.\nUnder the hood, the fitting algorithm generates a stream of networks based on those parameters for each new proposed set of model parameters. The last stream of networks is thus simulated using the final state of the model. The mcmc.diagnostics function takes that stream of networks and plots the sequence of the sufficient statistics included in the model. A converged model should show a stationary statistics sequence, moving around a fixed value without (a) becoming stuck at any point and (b) chaining the tendency. This model shows both:\n\nmcmc.diagnostics(model_final, which   = c(\"plots\"))\n\n\n\n\n\n\n\n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nNow that we know our model was good enough to represent the observed statistics (sample them, actually,) let’s see how good it is at capturing other features of the network that were not included in the model.\n\n\n2 Goodness-of-fit\nThis would be the last step in the sequence of steps to fit an ERGM. As we mentioned before, the idea of Goodness-of-fit [GOF] in ERG models is to see how well our model captures other properties of the graph that were not included in the model. By default, the gof function from the ergm package computes GOF for:\n\nThe model statistics.\nThe outdegree distribution.\nThe indegree distribution.\nThe distribution of edge-wise shared partners.\nThe distribution of the geodesic distances (shortest path).\n\nThe process of evaluating GOF is relatively straightforward. Using samples from the posterior distribution, we check whether the observed statistics from above are covered (fall within the CI) of our model. If they do, we say that the model has a good fit. Otherwise, if we observe significant anomalies, we return to the bench and try to improve our model.\nAs with all simulated data, our gof() call shows that our selected model was an excellent choice for the observed graph:\n\ngof_final &lt;- gof(model_final)\nprint(gof_final)\n## \n## Goodness-of-fit for in-degree \n## \n##           obs min  mean max MC p-value\n## idegree0   11   3 10.93  20       1.00\n## idegree1   22  16 24.80  34       0.58\n## idegree2   23  17 26.54  36       0.52\n## idegree3   30   9 19.41  31       0.02\n## idegree4   10   4 10.94  20       0.94\n## idegree5    3   1  4.97  11       0.46\n## idegree6    1   0  1.68   5       0.96\n## idegree7    0   0  0.58   3       1.00\n## idegree8    0   0  0.14   1       1.00\n## idegree10   0   0  0.01   1       1.00\n## \n## Goodness-of-fit for out-degree \n## \n##           obs min  mean max MC p-value\n## odegree0   10   3 10.71  18       1.00\n## odegree1   30  13 24.67  35       0.28\n## odegree2   23  18 27.50  40       0.38\n## odegree3   17  12 19.01  28       0.74\n## odegree4   10   3 10.72  18       1.00\n## odegree5    8   1  4.84  10       0.12\n## odegree6    2   0  1.79   6       1.00\n## odegree7    0   0  0.57   3       1.00\n## odegree8    0   0  0.15   2       1.00\n## odegree9    0   0  0.03   1       1.00\n## odegree10   0   0  0.01   1       1.00\n## \n## Goodness-of-fit for edgewise shared partner \n## \n##          obs min   mean max MC p-value\n## esp.OTP0 212 178 209.21 241       0.78\n## esp.OTP1   7   3  10.47  22       0.44\n## esp.OTP2   0   0   0.40   3       1.00\n## \n## Goodness-of-fit for minimum geodesic distance \n## \n##      obs min    mean  max MC p-value\n## 1    219 190  220.08  255       0.94\n## 2    449 326  459.25  619       0.98\n## 3    790 499  842.85 1183       0.90\n## 4   1151 708 1252.11 1960       0.80\n## 5   1245 775 1396.75 2266       0.66\n## 6   1043 656 1181.38 1680       0.44\n## 7    745 499  802.34 1211       0.72\n## 8    434 194  466.85  791       0.86\n## 9    198  57  244.78  569       0.72\n## 10    80  11  121.93  408       0.76\n## 11    10   1   60.50  285       0.42\n## 12     1   0   30.83  251       0.46\n## 13     0   0   15.96  186       0.74\n## 14     0   0    8.42  133       1.00\n## 15     0   0    5.02   96       1.00\n## 16     0   0    3.03   65       1.00\n## 17     0   0    1.83   60       1.00\n## 18     0   0    1.05   47       1.00\n## 19     0   0    0.54   23       1.00\n## 20     0   0    0.18    7       1.00\n## 21     0   0    0.07    3       1.00\n## 22     0   0    0.01    1       1.00\n## Inf 3535 776 2784.24 4680       0.38\n## \n## Goodness-of-fit for model statistics \n## \n##                     obs min   mean max MC p-value\n## edges               219 190 220.08 255       0.94\n## mutual               16   9  16.36  25       1.00\n## nodematch.fav_music 123  97 123.55 152       1.00\n## nodematch.female     72  49  71.53  90       0.90\n\nIt is easier to see the results using the plot function:\n\n# Plotting the result (5 figures)\nop &lt;- par(mfrow = c(3,2))\nplot(gof_final)\npar(op)"
  },
  {
    "objectID": "07-odd-balls/index.html",
    "href": "07-odd-balls/index.html",
    "title": "Odd balls",
    "section": "",
    "text": "Other common scenarios involve more convoluted/complex questions. For instance, in the case of dyadic behavior Bell et al. (2019).\nIn Tanaka and Vega Yon (2024), we study the prevalence of perception-based network motifs. While the ERGM framework would be a natural choice, as a first approach, we used non-parametric tests for hypothesis testing.\n\n\n\n\nReferences\n\nBell, Brooke M., Donna Spruijt-Metz, George G. Vega Yon, Abu S. Mondol, Ridwan Alam, Meiyi Ma, Ifat Emi, John Lach, John A. Stankovic, and Kayla De La Haye. 2019. “Sensing Eating Mimicry Among Family Members.” Translational Behavioral Medicine. https://doi.org/10.1093/tbm/ibz051.\n\n\nTanaka, Kyosuke, and George G. Vega Yon. 2024. “Imaginary Network Motifs: Structural Patterns of False Positives and Negatives in Social Networks.” Social Networks 78 (July): 65–80. https://doi.org/10.1016/j.socnet.2023.11.005."
  },
  {
    "objectID": "08-lab-2/index.html",
    "href": "08-lab-2/index.html",
    "title": "Lab II",
    "section": "",
    "text": "The ergm package comes with a handful of vignettes (extended R examples) that you can use to learn more about the package. The vignette with the same name as the package shows an example fitting a model to the network faux.mesa.high. Address the following questions:\n\nWhat type of ERG model is this?\nLooking at the plot in the vignette, what other term(s) could you consider worth adding to the model? Why?\nThe vignette did not include a goodness-of-fit analysis. Can you add one? What do you find?\nChallenge: without using ergm to fit the model, estimate the following p1 model: ~ edges + nodefactor(\"Race\"). Compare your results with what you get using ergm."
  },
  {
    "objectID": "09-advanced-ergms/index.html",
    "href": "09-advanced-ergms/index.html",
    "title": "Advanced ERGMs: Constraints",
    "section": "",
    "text": "For this section, we will dive in into ERGM constranints. Using constraints, you will be able to modify the sampling space of the model to things such as:\n\nPool (multilevel) models.\nAccounting for data generating process.\nMake your model behave (with caution.)\nEven fit Discrete-Exponential Family Models (DEFM.)\n\nWe will start with formally understanding what constraining the space means and then continue with some examples."
  },
  {
    "objectID": "09-advanced-ergms/index.html#footnotes",
    "href": "09-advanced-ergms/index.html#footnotes",
    "title": "Advanced ERGMs: Constraints",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to Laura Koehly, who devised this complicated model.↩︎\nAfter writing this example, it became apparent the use of the Log() transformation function may not be ideal. Since many terms used in ERGMs can be zero, e.g., triangles, the term Log(~ ostar(2)) is undefined when ostar(2) = 0. In practice, the ERGM package sets a lower limit for the log of 0, so, instead of having Log(0) ~ -Inf, they set it to be a really large negative number. This causes all sorts of issues to the estimates; in our example, an overestimation of the population parameter and a positive log-likelihood. Therefore, I wouldn’t recommend using this transformation too often.↩︎"
  },
  {
    "objectID": "10-new-topics/index.html",
    "href": "10-new-topics/index.html",
    "title": "New topics in network modeling",
    "section": "",
    "text": "With more data and computing resources, the things that we can ask and do with networks are becoming increasingly (even more) exciting and complex.\nIn this section, I will introduce some of the latest advancements and forthcoming topics in network modeling."
  },
  {
    "objectID": "10-new-topics/index.html#overview",
    "href": "10-new-topics/index.html#overview",
    "title": "New topics in network modeling",
    "section": "",
    "text": "With more data and computing resources, the things that we can ask and do with networks are becoming increasingly (even more) exciting and complex.\nIn this section, I will introduce some of the latest advancements and forthcoming topics in network modeling."
  },
  {
    "objectID": "10-new-topics/index.html#mutli-ergms",
    "href": "10-new-topics/index.html#mutli-ergms",
    "title": "New topics in network modeling",
    "section": "1.1 Mutli-ERGMs",
    "text": "1.1 Mutli-ERGMs\n\n\n\nIn Krivitsky, Coletti, and Hens (2023a), the authors present a start-to-finish pooled ERGM example featuring heterogeneous data sources.\nThey increase power and allow exploring heterogeneous effects across types/classes of networks."
  },
  {
    "objectID": "10-new-topics/index.html#statistical-power-of-soam",
    "href": "10-new-topics/index.html#statistical-power-of-soam",
    "title": "New topics in network modeling",
    "section": "1.2 Statistical power of SOAM",
    "text": "1.2 Statistical power of SOAM\n\n\nSOAM Stadtfeld et al. (2020) proposes ways to perform power analysis for Siena models. At the center of their six-step approach is simulation."
  },
  {
    "objectID": "10-new-topics/index.html#bayesian-alaam",
    "href": "10-new-topics/index.html#bayesian-alaam",
    "title": "New topics in network modeling",
    "section": "1.3 Bayesian ALAAM",
    "text": "1.3 Bayesian ALAAM\nEver wondered how to model influence exclusively?\n\n\n\nThe Auto-Logistic Actor Attribute Model [ALAAM] is a model that allows us to do just that.\nKoskinen and Daraganova (2022) extends the ALAAM model to a Bayesian framework.\n\n\n\nIt provides greater flexibility to accommodate more complicated models and add extensions such as hierarchical models."
  },
  {
    "objectID": "10-new-topics/index.html#relational-event-models",
    "href": "10-new-topics/index.html#relational-event-models",
    "title": "New topics in network modeling",
    "section": "1.4 Relational Event Models",
    "text": "1.4 Relational Event Models\n\nREMs are great for modeling sequences of ties (instead of panel or cross-sectional.)\nButts et al. (2023) provides a general overview of Relational Event Models [REMs,] new methods, and future steps.\n\n\n\n\nFigure 3 reproduced from Brandenberger (2020)"
  },
  {
    "objectID": "10-new-topics/index.html#big-ergms",
    "href": "10-new-topics/index.html#big-ergms",
    "title": "New topics in network modeling",
    "section": "1.5 Big ERGMs",
    "text": "1.5 Big ERGMs\n\n\n\nERGMs In A. Stivala, Robins, and Lomi (2020), a new method is proposed to estimate large ERGMs (featuring millions of nodes).\n\n\n\n\n\nPartial map of the Internet based on the January 15, 2005 data found on opte.org. – Wiki"
  },
  {
    "objectID": "10-new-topics/index.html#exponential-random-network-models",
    "href": "10-new-topics/index.html#exponential-random-network-models",
    "title": "New topics in network modeling",
    "section": "1.6 Exponential Random Network Models",
    "text": "1.6 Exponential Random Network Models\n\nWang, Fellows, and Handcock recently published a re-introduction of the ERNM framework (Wang, Fellows, and Handcock 2023).\nERNMs generalize ERGMs to incorporate behavior and are the cross-sectional causing of SIENA models.\n\n\\begin{align*}\n\\text{ER\\textbf{G}M}: & P_{\\mathcal{Y}, \\bm{\\theta}}(\\bm{Y}=\\bm{y} | \\bm{X}=\\bm{x}) \\\\\n\\text{ER\\textbf{N}M}: & P_{\\mathcal{Y}, \\bm{\\theta}}(\\bm{Y}=\\bm{y}, \\bm{X}=\\bm{x})\n\\end{align*}"
  },
  {
    "objectID": "10-new-topics/index.html#ergmitos-small-ergms",
    "href": "10-new-topics/index.html#ergmitos-small-ergms",
    "title": "New topics in network modeling",
    "section": "2.1 ERGMitos: Small ERGMs",
    "text": "2.1 ERGMitos: Small ERGMs\n\nERGMitos1 (Vega Yon, Slaughter, and Haye 2021) leverage small network sizes to use exact statistics.\n\n\n\n\nFive small networks from the ergmito R package"
  },
  {
    "objectID": "10-new-topics/index.html#discrete-exponential-family-models",
    "href": "10-new-topics/index.html#discrete-exponential-family-models",
    "title": "New topics in network modeling",
    "section": "2.2 Discrete Exponential-family Models",
    "text": "2.2 Discrete Exponential-family Models\n\n\n\nERGMs are a particular case of Random Markov fields.\nWe can use the ERGM framework for modeling vectors of binary outcomes, e.g., the consumption of \\{tobacco, MJ, alcohol\\}"
  },
  {
    "objectID": "10-new-topics/index.html#power-analysis-in-ergms",
    "href": "10-new-topics/index.html#power-analysis-in-ergms",
    "title": "New topics in network modeling",
    "section": "2.3 Power analysis in ERGMs",
    "text": "2.3 Power analysis in ERGMs\n\nUsing conditional ERGMs (closely related to constrained), we can do power analysis for network samples (Vega Yon 2023).\n\n\n\n\nReproduced from Krivitsky, Coletti, and Hens (2023b)"
  },
  {
    "objectID": "10-new-topics/index.html#two-step-estimation-ergms",
    "href": "10-new-topics/index.html#two-step-estimation-ergms",
    "title": "New topics in network modeling",
    "section": "2.4 Two-step estimation ERGMs",
    "text": "2.4 Two-step estimation ERGMs\n\n\nConditioning the ERGM on an observed statistic “drops” the associated coefficient.\nHypothesis: As n increases, conditional ERGM estimates are consistent with the full model:\n\n\n\n\n\nSimulation study trying to demonstrate the concept (Work in progress)"
  },
  {
    "objectID": "10-new-topics/index.html#thanks",
    "href": "10-new-topics/index.html#thanks",
    "title": "New topics in network modeling",
    "section": "2.5 Thanks!",
    "text": "2.5 Thanks!"
  },
  {
    "objectID": "10-new-topics/index.html#bonus-track-why-network-scientists-dont-use-ergms",
    "href": "10-new-topics/index.html#bonus-track-why-network-scientists-dont-use-ergms",
    "title": "New topics in network modeling",
    "section": "2.6 Bonus track: Why network scientists don’t use ERGMs?",
    "text": "2.6 Bonus track: Why network scientists don’t use ERGMs?\n\nAttempts to overcome these problems by extending the blockmodel have focused particularly on the use of (more complicated) p∗ or exponential random graph models, but while these are conceptually appealing, they quickly lose the analytic tractability of the original blockmodel as their complexity increases.\n– Karrer and Newman (2011)"
  },
  {
    "objectID": "10-new-topics/index.html#footnotes",
    "href": "10-new-topics/index.html#footnotes",
    "title": "New topics in network modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom the Spanish suffix meaning small.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "If you are reading this, it is because you know that networks are everywhere. Network science is a rapidly growing field that has been applied to many different disciplines, from biology to sociology, from computer science to physics. In this course, we will go over advanced network science topics; particularly, statistical inference in networks. The course contents are:\n\nOverview of statistical inference.\nIntroduction to network science inference.\nMotif detection.\nGlobal statistics (e.g., modularity).\nRandom graphs (static).\nRandom graphs (dynamic).\nCoevolution of networks and behavior.\nAdvanced topics (sampling and conditional models)."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Home",
    "section": "",
    "text": "If you are reading this, it is because you know that networks are everywhere. Network science is a rapidly growing field that has been applied to many different disciplines, from biology to sociology, from computer science to physics. In this course, we will go over advanced network science topics; particularly, statistical inference in networks. The course contents are:\n\nOverview of statistical inference.\nIntroduction to network science inference.\nMotif detection.\nGlobal statistics (e.g., modularity).\nRandom graphs (static).\nRandom graphs (dynamic).\nCoevolution of networks and behavior.\nAdvanced topics (sampling and conditional models)."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Home",
    "section": "4 References",
    "text": "4 References\n\n\n“1.1: Basic Definitions and Concepts.” 2014. Statistics LibreTexts. https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/01%3A_Introduction_to_Statistics/1.01%3A_Basic_Definitions_and_Concepts.\n\n\nBarrett, Tyson, Matt Dowle, and Arun Srinivasan. 2023. Data.table: Extension of ‘Data.frame‘. https://r-datatable.com.\n\n\nBell, Brooke M., Donna Spruijt-Metz, George G. Vega Yon, Abu S. Mondol, Ridwan Alam, Meiyi Ma, Ifat Emi, John Lach, John A. Stankovic, and Kayla De La Haye. 2019. “Sensing Eating Mimicry Among Family Members.” Translational Behavioral Medicine. https://doi.org/10.1093/tbm/ibz051.\n\n\nBrandenberger, Laurence. 2020. “Interdependencies in Conflict Dynamics: Analyzing Endogenous Patterns in Conflict Event Data Using Relational Event Models.” In Computational Conflict Research, edited by Emanuel Deutschmann, Jan Lorenz, Luis G. Nardin, Davide Natalini, and Adalbert F. X. Wilhelm, 67–80. Computational Social Sciences. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-29333-8_4.\n\n\nBrooks, S., A. Gelman, G. Jones, and X. L. Meng. 2011. Handbook of Markov Chain Monte Carlo. CRC Press.\n\n\nButts, Carter T., Alessandro Lomi, Tom A. B. Snijders, and Christoph Stadtfeld. 2023. “Relational Event Models in Network Science.” Network Science 11 (2): 175–83. https://doi.org/10.1017/nws.2023.9.\n\n\nCasella, George, and Roger L. Berger. 2021. Statistical Inference. Cengage Learning.\n\n\nFellows, Ian E. 2012. “Exponential Family Random Network Models.” ProQuest Dissertations and Theses. PhD thesis. https://login.ezproxy.lib.utah.edu/login?url=https://www.proquest.com/dissertations-theses/exponential-family-random-network-models/docview/1221548720/se-2.\n\n\nFrank, O, and David Strauss. 1986. “Markov graphs.” Journal of the American Statistical Association 81 (395): 832–42. https://doi.org/10.2307/2289017.\n\n\nGelman, Andrew. 2018. “The Failure of Null Hypothesis Significance Testing When Studying Incremental Changes, and What to Do About It.” Personality and Social Psychology Bulletin 44 (1): 16–23. https://doi.org/10.1177/0146167217729162.\n\n\nGeyer, Charles J., and Elizabeth A. Thompson. 1992. “Constrained Monte Carlo Maximum Likelihood for Dependent Data.” Journal of the Royal Statistical Society. Series B (Methodological) 54 (3): 657–99. https://www.jstor.org/stable/2345852.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. 2016. “Statistical Tests, P Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31 (4): 337–50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nHandcock, Mark S., David R. Hunter, Carter T. Butts, Steven M. Goodreau, Pavel N. Krivitsky, and Martina Morris. 2023. Ergm: Fit, Simulate and Diagnose Exponential-Family Models for Networks. The Statnet Project (https://statnet.org). https://CRAN.R-project.org/package=ergm.\n\n\nHaye, Kayla de la, Heesung Shin, George G. Vega Yon, and Thomas W. Valente. 2019. “Smoking Diffusion Through Networks of Diverse, Urban American Adolescents over the High School Period.” Journal of Health and Social Behavior. https://doi.org/10.1177/0022146519870521.\n\n\nHolland, Paul W., and Samuel Leinhardt. 1981. “An exponential family of probability distributions for directed graphs.” Journal of the American Statistical Association 76 (373): 33–50. https://doi.org/10.2307/2287037.\n\n\nHunter, David R. 2007. “Curved Exponential Family Models for Social Networks.” Social Networks 29 (2): 216–30. https://doi.org/10.1016/j.socnet.2006.08.005.\n\n\nHunter, David R., Mark S. Handcock, Carter T. Butts, Steven M. Goodreau, and Martina Morris. 2008. “ergm: A Package to Fit, Simulate and Diagnose Exponential-Family Models for Networks.” Journal of Statistical Software 24 (3): 1–29. https://doi.org/10.18637/jss.v024.i03.\n\n\nHunter, David R., Pavel N. Krivitsky, and Michael Schweinberger. 2012. “Computational Statistical Methods for Social Network Models.” Journal of Computational and Graphical Statistics 21 (4): 856–82. https://doi.org/10.1080/10618600.2012.732921.\n\n\nKarrer, Brian, and M. E. J. Newman. 2011. “Stochastic Blockmodels and Community Structure in Networks.” Physical Review E 83 (1): 016107. https://doi.org/10.1103/PhysRevE.83.016107.\n\n\nKoskinen, Johan, and Galina Daraganova. 2022. “Bayesian Analysis of Social Influence.” Journal of the Royal Statistical Society Series A: Statistics in Society 185 (4): 1855–81. https://doi.org/10.1111/rssa.12844.\n\n\nKoskinen, Johan, Pete Jones, Darkhan Medeuov, Artem Antonyuk, Kseniia Puzyreva, and Nikita Basov. 2023. “Analysing Networks of Networks.” Social Networks 74 (July): 102–17. https://doi.org/10.1016/j.socnet.2023.02.002.\n\n\nKoskinen, Johan, Peng Wang, Garry Robins, and Philippa Pattison. 2018. “Outliers and Influential Observations in Exponential Random Graph Models.” Psychometrika 83 (4): 809–30. https://doi.org/10.1007/s11336-018-9635-8.\n\n\nKrivitsky, Pavel N. 2012. “Exponential-Family Random Graph Models for Valued Networks.” Electronic Journal of Statistics 6: 1100–1128. https://doi.org/10.1214/12-EJS696.\n\n\n———. 2017. “Using Contrastive Divergence to Seed Monte Carlo MLE for Exponential-Family Random Graph Models.” Computational Statistics & Data Analysis 107 (March): 149–61. https://doi.org/10.1016/j.csda.2016.10.015.\n\n\n———. 2023. Ergm.multi: Fit, Simulate and Diagnose Exponential-Family Models for Multiple or Multilayer Networks. The Statnet Project (https://statnet.org). https://CRAN.R-project.org/package=ergm.multi.\n\n\nKrivitsky, Pavel N., Pietro Coletti, and Niel Hens. 2023a. “A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks.” Journal of the American Statistical Association 0 (0): 1–21. https://doi.org/10.1080/01621459.2023.2242627.\n\n\n———. 2023b. “Rejoinder to Discussion of ‘A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks’.” Journal of the American Statistical Association 118 (544): 2235–38. https://doi.org/10.1080/01621459.2023.2280383.\n\n\nKrivitsky, Pavel N., David R. Hunter, Martina Morris, and Chad Klumb. 2023. “Ergm 4: New Features for Analyzing Exponential-Family Random Graph Models.” Journal of Statistical Software 105 (January): 1–44. https://doi.org/10.18637/jss.v105.i06.\n\n\nLeifeld, Philip. 2013. “Texreg : Conversion of Statistical Model Output in R to L A T E X and HTML Tables.” Journal of Statistical Software 55 (8). https://doi.org/10.18637/jss.v055.i08.\n\n\nLeSage, James P. 2008. “An Introduction to Spatial Econometrics.” Revue d’économie Industrielle 123 (123): 19–44. https://doi.org/10.4000/rei.3887.\n\n\nLeSage, James P., and R. Kelley Pace. 2014. “The Biggest Myth in Spatial Econometrics.” Econometrics 2 (4): 217–49. https://doi.org/10.2139/ssrn.1725503.\n\n\nLusher, Dean, Johan Koskinen, and Garry Robins. 2013. Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications. Cambridge University Press.\n\n\nMilo, R., S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon. 2002. “Network Motifs: Simple Building Blocks of Complex Networks.” Science 298 (5594): 824–27. https://doi.org/10.1126/science.298.5594.824.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRobins, Garry, Philippa Pattison, and Peter Elliott. 2001. “Network Models for Social Influence Processes.” Psychometrika 66 (2): 161–89. https://doi.org/10.1007/BF02294834.\n\n\nRobins, Garry, Pip Pattison, Yuval Kalish, and Dean Lusher. 2007. “An introduction to exponential random graph (p*) models for social networks.” Social Networks 29 (2): 173–91. https://doi.org/10.1016/j.socnet.2006.08.002.\n\n\nRoger Bivand. 2022. “R Packages for Analyzing Spatial Data: A Comparative Case Study with Areal Data.” Geographical Analysis 54 (3): 488–518. https://doi.org/10.1111/gean.12319.\n\n\nSlaughter, Andrew J., and Laura M. Koehly. 2016. “Multilevel Models for Social Networks: Hierarchical Bayesian Approaches to Exponential Random Graph Modeling.” Social Networks 44: 334–45. https://doi.org/10.1016/j.socnet.2015.11.002.\n\n\nSnijders, Tom a B. 1996. “Stochastic Actor-Oriented Models for Network Change.” The Journal of Mathematical Sociology 21 (1-2): 149–72. https://doi.org/10.1080/0022250X.1996.9990178.\n\n\nSnijders, Tom A B, Philippa E Pattison, Garry L Robins, and Mark S Handcock. 2006. “New specifications for exponential random graph models.” Sociological Methodology 36 (1): 99–153. https://doi.org/10.1111/j.1467-9531.2006.00176.x.\n\n\nSnijders, Tom A. B. 2017. “Stochastic Actor-Oriented Models for Network Dynamics.” Annual Review of Statistics and Its Application 4 (1): 343–63. https://doi.org/10.1146/annurev-statistics-060116-054035.\n\n\nSnijders, Tom A. B., and Stephen P. Borgatti. 1999. “Non-Parametric Standard Errors and Tests for Network Statistics.” Connections 22 (2): 1–10.\n\n\nStadtfeld, Christoph, Tom A. B. Snijders, Christian Steglich, and Marijtje van Duijn. 2020. “Statistical Power in Longitudinal Network Studies.” Sociological Methods & Research 49 (4): 1103–32. https://doi.org/10.1177/0049124118769113.\n\n\nStivala, Alex D., H. Colin Gallagher, David A. Rolls, Peng Wang, and Garry L. Robins. 2020. “Using Sampled Network Data With The Autologistic Actor Attribute Model.” arXiv. https://doi.org/10.48550/arXiv.2002.00849.\n\n\nStivala, Alex, Garry Robins, and Alessandro Lomi. 2020. “Exponential Random Graph Model Parameter Estimation for Very Large Directed Networks.” PLoS ONE 15 (1): 1–23. https://doi.org/10.1371/journal.pone.0227804.\n\n\nTanaka, Kyosuke, and George G. Vega Yon. 2024. “Imaginary Network Motifs: Structural Patterns of False Positives and Negatives in Social Networks.” Social Networks 78 (July): 65–80. https://doi.org/10.1016/j.socnet.2023.11.005.\n\n\nValente, Thomas W., and George G. Vega Yon. 2020. “Diffusion/Contagion Processes on Social Networks.” Health Education & Behavior 47 (2): 235–48. https://doi.org/10.1177/1090198120901497.\n\n\nValente, Thomas W., Heather Wipfli, and George G. Vega Yon. 2019. “Network Influences on Policy Implementation: Evidence from a Global Health Treaty.” Social Science and Medicine. https://doi.org/10.1016/j.socscimed.2019.01.008.\n\n\nVega Yon, George. 2020. ergmito: Exponential Random Graph Models for Small Networks. CRAN. https://cran.r-project.org/package=ergmito.\n\n\nVega Yon, George G. 2023. “Power and Multicollinearity in Small Networks: A Discussion of ‘Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks’ by Krivitsky, Coletti & Hens.” Journal Of The American Statistical Association.\n\n\nVega Yon, George G., Andrew Slaughter, and Kayla de la Haye. 2021. “Exponential Random Graph Models for Little Networks.” Social Networks 64 (August 2020): 225–38. https://doi.org/10.1016/j.socnet.2020.07.005.\n\n\nWang, Zeyi, Ian E. Fellows, and Mark S. Handcock. 2023. “Understanding Networks with Exponential-Family Random Network Models.” Social Networks, August, S0378873323000497. https://doi.org/10.1016/j.socnet.2023.07.003.\n\n\nWasserman, Stanley, and Philippa Pattison. 1996. “Logit models and logistic regressions for social networks: I. An introduction to Markov graphs and p*.” Psychometrika 61 (3): 401–25. https://doi.org/10.1007/BF02294547."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Home",
    "section": "2 Disclaimer",
    "text": "2 Disclaimer\nThis is an ongoing project. The course is being developed and will be updated as we go. If you have any comments or suggestions, please let me know. The generation of the course materials was assisted by AI tools, namely, GitHub copilot."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Home",
    "section": "3 Code of Conduct",
    "text": "3 Code of Conduct\nPlease note that the networks-udd2024 project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms."
  }
]