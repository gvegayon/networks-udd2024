[
  {
    "objectID": "00-overview/index.html",
    "href": "00-overview/index.html",
    "title": "Overview",
    "section": "",
    "text": "When networks are involved, statistical inference becomes tricky.\nThe IID assumption is violated by construction.\nAlthough it is tempting to use the same tools as in the IID case, they are not valid."
  },
  {
    "objectID": "00-overview/index.html#when-the-network-is-deterministic",
    "href": "00-overview/index.html#when-the-network-is-deterministic",
    "title": "Overview",
    "section": "2.1 When the network is deterministic",
    "text": "2.1 When the network is deterministic\n\n2.1.1 Single network\nIf the network is fixed (or treated as if it were fixed,) it is most likely that a traditional statistical analysis can be performed. For instance, if we are interested in influence behavior between adolescents, and we assume influence is a process that takes time, then a lagged regression may suffice (Haye et al. 2019; Valente and Vega Yon 2020; Valente, Wipfli, and Vega Yon 2019).\n\n\\mathbf{y}_t = \\rho \\mathbf{W} \\mathbf{y}_{t-1} + \\mathbf{X} \\bm{\\beta} + \\bm{\\varepsilon}, \\quad \\varepsilon_i \\sim \\text{N}\\left(0, \\sigma^2\\right)\n\\tag{1}\nwhere \\mathbf{y}_t is a vector of behaviors at time t, \\mathbf{W} is the row-stochastic adjacency matrix of the network, \\mathbf{X} is a matrix of covariates, and the elements of \\bm{\\varepsilon}\\equiv \\{\\varepsilon_i\\} distribute normal with mean zero and variance \\sigma^2.\nNonetheless, if assuming a lagged influence effect is no longer possible, then the regular regression model is no longer valid. Instead, we can resort to a Spatial Autocorrelation Regression Model [SAR] (see LeSage 2008):\n\n\\mathbf{y} = \\rho \\mathbf{W} \\mathbf{y} + \\mathbf{X} \\bm{\\beta} + \\bm{\\varepsilon},\\quad \\bm{\\varepsilon} \\sim \\text{MVN}\\left(0, \\Sigma^2\\right)\n\\tag{2}\nfurthermore\n\n\\mathbf{y} = \\left(I - \\rho\\mathbf{W}\\right)^{-1}\\left(\\mathbf{X}\\bm{\\beta} + \\bm{\\varepsilon}\\right)\n\nWhere \\bm{\\varepsilon} now distributes Multivariate Normal with mean zero and covariance matrix \\Sigma^2 \\mathbf{I}.\n\n\n\n\n\n\nTip\n\n\n\nWhat is the appropriate network to use in the SAR model? According to LeSage and Pace (2014), it is not very important. Since (I_n - \\rho \\mathbf{W})^{-1} = \\rho \\mathbf{W} + \\rho^2 \\mathbf{W}^2 + \\dots.\n\n\n\n\n2.1.2 Multiple networks\nSometimes, instead of a single network, we are interested in understanding how network-level properties affect the behavior of individuals. For instance, we may be interested in understanding the relation between triadic closure and income within a sample of independent egocentric networks; in such a case, as the networks are independent, a simple regression analysis may suffice."
  },
  {
    "objectID": "00-overview/index.html#when-the-network-is-random",
    "href": "00-overview/index.html#when-the-network-is-random",
    "title": "Overview",
    "section": "2.2 When the network is random",
    "text": "2.2 When the network is random\n\n2.2.1 Deterministic behavior\nIn this case, the behavior is treated as given, i.e., a covariate/feature of the model. When such is the case, the method of choice is the Exponential Random Graph Model [ERGM] (Lusher, Koskinen, and Robins 2013; Krivitsky 2012 and others).\n\n\n2.2.2 Random behavior"
  },
  {
    "objectID": "00-overview/index.html#non-parametric-approaches",
    "href": "00-overview/index.html#non-parametric-approaches",
    "title": "Overview",
    "section": "2.3 Non-parametric approaches",
    "text": "2.3 Non-parametric approaches\nOther common scenarios involve more convoluted/complex questions. For instance, in the case of dyadic behavior Bell et al. (2019).\nIn Tanaka and Vega Yon (2024), we study the prevalence of perception-based network motifs. While the ERGM framework would be a natural choice, as a first approach, we used non-parametric tests for hypothesis testing."
  },
  {
    "objectID": "01-fundamentals/index.html",
    "href": "01-fundamentals/index.html",
    "title": "Fundamentals",
    "section": "",
    "text": "Before jumping into network science details, we need to cover some fundamentals. I assume that most of the contents here are well known to you–we will be brief–but I want to ensure we are all on the same page."
  },
  {
    "objectID": "01-fundamentals/index.html#getting-help",
    "href": "01-fundamentals/index.html#getting-help",
    "title": "Fundamentals",
    "section": "1.1 Getting help",
    "text": "1.1 Getting help\nUnlike other languages, R’s documentation is highly reliable. The Comprehensive R Archive Network [CRAN] is the official repository of R packages. All packages posted on CRAN must pass a series of tests to ensure the quality of the code, including the documentation.\nTo get help on a function, we can use the help() function. For example, if we wanted to get help on the mean() function, we would do:\n\nhelp(\"mean\")"
  },
  {
    "objectID": "01-fundamentals/index.html#naming-conventions",
    "href": "01-fundamentals/index.html#naming-conventions",
    "title": "Fundamentals",
    "section": "1.2 Naming conventions",
    "text": "1.2 Naming conventions\nR has a set of naming conventions that we should follow to avoid confusion. The most important ones are:\n\nUse lowercase letters (optional)\nUse underscores to separate words (optional)\nDo not start with a number\nDo not use special characters\nDo not use reserved words\n\n\n\n\n\n\n\nQuestion\n\n\n\nOf the following list, which are valid names and which are valid but to be avoided?\n_my.var\nmy.var\nmy_var\nmyVar\nmyVar1\n1myVar\nmy var\nmy-var"
  },
  {
    "objectID": "01-fundamentals/index.html#assignment",
    "href": "01-fundamentals/index.html#assignment",
    "title": "Fundamentals",
    "section": "1.3 Assignment",
    "text": "1.3 Assignment\nIn R, we have two (four) ways of assigning values to objects: the &lt;- and = binary operators2. Although both are equivalent, the former is the preferred way of assigning values to objects since the latter can be confused with function arguments.\n\nx &lt;- 1\nx = 1\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the difference between the following two assignments? Use the help function to find out.\nx &lt;- 1\nx &lt;&lt;- 1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are other ways in which you can assign values to objects?"
  },
  {
    "objectID": "01-fundamentals/index.html#using-functions-and-piping",
    "href": "01-fundamentals/index.html#using-functions-and-piping",
    "title": "Fundamentals",
    "section": "1.4 Using functions and piping",
    "text": "1.4 Using functions and piping\nIn R, we use functions to perform operations on objects. Functions are implemented as function_name ( argument_1 , argument_2 , ... ). For example, the mean() function takes a vector of numbers and returns the mean of the values:\n\nx &lt;- c(1, 2, 3) # The c() function creates a vector\nmean(x)\n## [1] 2\n\nFurthermore, we can use the pipe operator (|&gt;) to improve readability. The pipe operator takes the output of the left-hand side expression and passes it as the first argument of the right-hand side expression. Our previous example could be rewritten as:\n\nc(1, 2, 3) |&gt; mean()\n## [1] 2"
  },
  {
    "objectID": "01-fundamentals/index.html#data-structures",
    "href": "01-fundamentals/index.html#data-structures",
    "title": "Fundamentals",
    "section": "1.5 Data structures",
    "text": "1.5 Data structures\nAtomic types are the minimal building blocks of R. They are logical, integer, double, character, complex, raw:\n\nx_logical   &lt;- TRUE\nx_integer   &lt;- 1L\nx_double    &lt;- 1.0\nx_character &lt;- \"a\"\nx_complex   &lt;- 1i\nx_raw       &lt;- charToRaw(\"a\")\n\nUnlike other languages, we do not need to declare the data type before creating the object; R will infer it from the value.\n\n\n\n\n\n\nPro-tip\n\n\n\nAdding the L suffix to the value is good practice when dealing with integers. Some R packages like data.table (Barrett, Dowle, and Srinivasan 2023) have internal checks that will throw an error if you are not explicit about the data type.\n\n\nThe next type is the vector. A vector is a collection of elements of the same type. The most common way to create a vector is with the c() function:\n\nx_integer &lt;- c(1, 2, 3)\nx_double  &lt;- c(1.0, 2.0, 3.0)\nx_logical &lt;- c(TRUE, FALSE, TRUE)\n# etc.\n\nR will coerce the data types to the most general type. For example, if we mix integers and doubles, R will coerce the integers into doubles. The coercion order is logical &lt; integer &lt; double &lt; character\n\n\n\n\n\n\nQuestion\n\n\n\nWhy is the coercion order logical &lt; integer &lt; double &lt; character?\n\n\nThe next data structure is the list. A list is a collection of elements of any type. We can create a list with the list() function:\n\nx_list       &lt;- list(1, 2.0, TRUE, \"a\")\nx_list_named &lt;- list(a = 1, b = 2.0, c = TRUE, d = \"a\")\n\nTo access elements in a list, we have two options: by position or by name, the latter only if the elements are named:\n\nx_list[[1]]\n## [1] 1\nx_list_named[[\"a\"]]\n## [1] 1\nx_list_named$a\n## [1] 1\n\nAfter lists, we have matrices. A matrix is a collection of elements of the same type arranged in a two-dimensional grid. We can create a matrix with the matrix() function:\n\nx_matrix &lt;- matrix(1:9, nrow = 3, ncol = 3)\nx_matrix\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n\n# We can access elements in a matrix by row column, or position:\nx_matrix[1, 2]\n## [1] 4\nx_matrix[cbind(1, 2)]\n## [1] 4\nx_matrix[4]\n## [1] 4\n\n\n\n\n\n\n\nMatrix is a vector\n\n\n\nMatrices in R are vectors with dimensions. In base R, matrices are stored in column-major order. This means that the elements are stored column by column. This is important to know when we are accessing elements in a matrix\n\n\nThe two last data structures are arrays and data frames. An array is a collection of elements of the same type arranged in a multi-dimensional grid. We can create an array with the array() function:\n\nx_array &lt;- array(1:27, dim = c(3, 3, 3))\n\n# We can access elements in an array by row, column, and dimension, or\n# position:\nx_array[1, 2, 3]\n## [1] 22\nx_array[cbind(1, 2, 3)]\n## [1] 22\nx_array[22]\n## [1] 22\n\nData frames are the most common data structure in R. In principle, these objects are lists of vectors of the same length, each vector representing a column. Columns (lists) in data frames can be of different types, but elements in each column must be of the same type. We can create a data frame with the data.frame() function:\n\nx_data_frame &lt;- data.frame(\n  a = 1:3,\n  b = c(\"a\", \"b\", \"c\"),\n  c = c(TRUE, FALSE, TRUE)\n)\n\n# We can access elements in a data frame by row, column, or position:\nx_data_frame[1, 2]\n## [1] \"a\"\nx_data_frame[cbind(1, 2)]\n## [1] \"a\"\nx_data_frame$b[1]    # Like a list\n## [1] \"a\"\nx_data_frame[[2]][1] # Like a list too\n## [1] \"a\""
  },
  {
    "objectID": "01-fundamentals/index.html#functions",
    "href": "01-fundamentals/index.html#functions",
    "title": "Fundamentals",
    "section": "1.6 Functions",
    "text": "1.6 Functions\nFunctions are the most important building blocks of R. A function is a set of instructions that takes one or more inputs and returns one or more outputs. We can create a function with the function() function:\n\n# This function has two arguments (y is optional)\nf &lt;- function(x, y = 1) {\n  x + 1\n}\n\nf(1)\n## [1] 2\n\nStarting with R 4, we can use the lambda syntax to create functions:\n\nf &lt;- \\(x, y) x + 1\n\nf(1)\n## [1] 2"
  },
  {
    "objectID": "01-fundamentals/index.html#control-flow",
    "href": "01-fundamentals/index.html#control-flow",
    "title": "Fundamentals",
    "section": "1.7 Control flow",
    "text": "1.7 Control flow\nControl flow statements allow us to control the execution of the code. The most common control flow statements are if, for, while, and repeat. We can create a control flow statement with the if(), for(), while(), and repeat() functions:\n\n# if\nif (TRUE) {\n  \"a\"\n} else {\n  \"b\"\n}\n## [1] \"a\"\n\n# for\nfor (i in 1:3) {\n  cat(\"This is the number \", i, \"\\n\")\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3\n\n# while\ni &lt;- 1\nwhile (i &lt;= 3) {\n  cat(\"This is the number \", i, \"\\n\")\n  i &lt;- i + 1\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3\n\n# repeat\ni &lt;- 1\nrepeat {\n  cat(\"This is the number \", i, \"\\n\")\n  i &lt;- i + 1\n  if (i &gt; 3) {\n    break\n  }\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3"
  },
  {
    "objectID": "01-fundamentals/index.html#hypothesis-testing",
    "href": "01-fundamentals/index.html#hypothesis-testing",
    "title": "Fundamentals",
    "section": "2.1 Hypothesis testing",
    "text": "2.1 Hypothesis testing\nAccording to Wikipedia\n\nA statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis. More generally, hypothesis testing allows us to make probabilistic statements about population parameters. More informally, hypothesis testing is the processes of making decisions under uncertainty. Typically, hypothesis testing procedures involve a user selected tradeoff between false positives and false negatives. – Wiki\n\nIn a nutshell, hypothesis testing is performed by following these steps:\n\nState the null and alternative hypotheses. In general, the null hypothesis is a statement about the population parameter that challenges our research question; for example, given the question of whether two networks are different, the null hypothesis would be that the two networks are the same.\nCompute the corresponding test statistic. It is a data function that reduces the information to a single number.\nCompare the observed test statistic with the distribution of the test statistic under the null hypothesis. The sometimes infamous p-value: ``[…] the probability that the chosen test statistic would have been at least as large as its observed value if every model assumption were correct, including the test hypothesis.’’ (Greenland et al. 2016) 3\n\n\nReport the observed effect and p-value, i.e., \\Pr(t \\in H_0)\n\nWe usually say that we either reject the null hypothesis or fail to reject it (we never accept the null hypothesis,) but, in my view, it is always better to talk about it in terms of “suggests evidence for” or “suggests evidence against.”\nWe will illustrate statistical concepts more concretely in the next section."
  },
  {
    "objectID": "01-fundamentals/index.html#probability-distributions",
    "href": "01-fundamentals/index.html#probability-distributions",
    "title": "Fundamentals",
    "section": "3.1 Probability distributions",
    "text": "3.1 Probability distributions\nR has a standard way of naming probability functions. The naming structure is [type of function][distribution], where [type of function] can be d for density, p for cumulative distribution function, q for quantile function, and r for random generation. For example, the normal distribution has the following functions:\n\ndnorm(0, mean = 0, sd = 1)\n## [1] 0.3989423\npnorm(0, mean = 0, sd = 1)\n## [1] 0.5\nqnorm(0.5, mean = 0, sd = 1)\n## [1] 0\n\nNow, if we wanted to know what is the probability of observing a value smaller than -2 comming from a standard normal distribution, we would do:\n\npnorm(-2, mean = 0, sd = 1)\n## [1] 0.02275013\n\nCurrently, R has a wide range of probability distributions implemented.\n\n\n\n\n\n\nQuestion\n\n\n\nHow many probability distributions are implemented in R’s stats package?"
  },
  {
    "objectID": "01-fundamentals/index.html#random-number-generation",
    "href": "01-fundamentals/index.html#random-number-generation",
    "title": "Fundamentals",
    "section": "3.2 Random number generation",
    "text": "3.2 Random number generation\nRandom numbers, and more precisely, pseudo-random numbers, are a vital component of statistical programming. Pure randomness is hard to come by, and so we rely on pseudo-random number generators (PRNGs) to generate random numbers. These generators are deterministic algorithms that produce sequences of numbers we can then use to generate random samples from probability distributions. Because of the latter, PRNGs need a starting point called the seed. As a statistical computing program, R has a variety of PRNGs. As suggested in the previous subsection, we can generate random numbers from a probability distribution with the r function. In what follows, we will draw random numbers from a few distributions and plot histograms of the results:\n\nset.seed(1)\n\n# Saving the current graphical parameters\nop &lt;- par(mfrow = c(2,2))\nrnorm(1000) |&gt; hist(main = \"Normal distribution\")\nrunif(1000) |&gt; hist(main = \"Uniform distribution\")\nrpois(1000, lambda = 1) |&gt; hist(main = \"Poisson distribution\")\nrbinom(1000, size = 10, prob = 0.1) |&gt; hist(main = \"Binomial distribution\")\n\n\n\npar(op)"
  },
  {
    "objectID": "01-fundamentals/index.html#simulations-and-sampling",
    "href": "01-fundamentals/index.html#simulations-and-sampling",
    "title": "Fundamentals",
    "section": "3.3 Simulations and sampling",
    "text": "3.3 Simulations and sampling\nSimulations are front and center in statistical programming. We can use them to test the properties of statistical methods, generate data, and perform statistical inference. The following example uses the sample function in R to compute the bootstrap standard error of the mean (see Casella and Berger 2021):\n\nset.seed(1)\nx &lt;- rnorm(1000)\n\n# Bootstrap standard error of the mean\nn &lt;- length(x)\nB &lt;- 1000\n\n# We will store the results in a vector\nres &lt;- numeric(B)\n\nfor (i in 1:B) {\n  # Sample with replacement\n  res[i] &lt;- sample(x, size = n, replace = TRUE) |&gt;\n    mean()\n}\n\n# Plot the results\nhist(res, main = \"Bootstrap standard error of the mean\")\n\n\n\n\nSince the previous example is rather extensive, let us review it in detail.\n\nset.seed(1) sets the seed of the PRNG to 1. It ensures we get the same results every time we run the code.\nrnorm() generates a sample of 1,000 standard-normal values.\nn &lt;- length(x) stores the length of the vector in the n variable.\nB &lt;- 1000 stores the number of bootstrap samples in the B variable.\nres &lt;- numeric(B) creates a vector of length B to store the results.\nfor (i in 1:B) is a for loop that iterates from 1 to B.\nres[i] &lt;- sample(x, size = n, replace = TRUE) |&gt; mean() samples n values from x with replacement and computes the mean of the sample.\nThe pipe operator (|&gt;) passes the output of the left-hand side expression as the first argument of the right-hand side expression.\nhist(res, main = \"Bootstrap standard error of the mean\") plots the histogram of the results.\n\n\n\n\n\n\n\nQuestion\n\n\n\nSimulating convolutions: Using what you have learned about statistical functions in R, simulate the convolution of two normal distributions, one with (\\mu, \\sigma^2) = (-3, 1) and the other with (\\mu, \\sigma^2) = (2, 2). Plot the histogram of the results. Draw 1,000 samples.\n\n\nCode\nset.seed(1)\nx &lt;- rnorm(1000, mean = -3, sd = 1)\ny &lt;- rnorm(1000, mean = 2, sd = 2)\nz &lt;- x + y\n\nhist(z)\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBimodal distribution: Using the previous two normal distributions, simulate a bimodal distribution where the probability of sampling from the first distribution is 0.3 and the probability of sampling from the second distribution is 0.7. Plot the histogram of the results. (Hint: use a combination of runif() and ifelse()).\n\n\nCode\nz &lt;- ifelse(runif(1000) &lt; 0.3, x, y)\ndensity(z) |&gt; plot()"
  },
  {
    "objectID": "01-fundamentals/index.html#footnotes",
    "href": "01-fundamentals/index.html#footnotes",
    "title": "Fundamentals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough, not for network science in general.↩︎\nIn mathematics and computer science, a binary operator is a function that takes two arguments. In R, binary operators are implemented as variable 1 [operator] variable 2. For example, 1 + 2 is a binary operation.↩︎\nThe discussion about interpreting p-values and hypothesis testing is vast and relevant. Although we will not review this here, I recommend looking into the work of Andrew Gelman Gelman (2018).↩︎"
  },
  {
    "objectID": "02-random-graphs/index.html",
    "href": "02-random-graphs/index.html",
    "title": "Random graphs",
    "section": "",
    "text": "In this section, we will focus on reviewing the most common random graph models, how these are used, and what things are important to consider when using them. Later on in the course, we will focus on Exponential-Family Random Graph Models (ERGMs), which are a generalization of the models we will discuss here."
  },
  {
    "objectID": "02-random-graphs/index.html#code-example",
    "href": "02-random-graphs/index.html#code-example",
    "title": "Random graphs",
    "section": "2.1 Code example",
    "text": "2.1 Code example\n\n# Model parameters\nn &lt;- 40\np &lt;- 0.1\n\n# Generating the graph, version 1\nset.seed(3312)\ng &lt;- matrix(as.integer(runif(n * n) &lt; p), nrow = n, ncol = n)\ndiag(g) &lt;- 0\n\n# Visualizing the network\nlibrary(igraph)\nlibrary(netplot)\nnplot(graph_from_adjacency_matrix(g))\n\n\n\n\nChallenge 1: How would you generate the graph using the two-step process described above?\nChallenge 2: Using a Generalized-Linear-Model [GLM], estimate p and its variance from the above network."
  },
  {
    "objectID": "02-random-graphs/index.html#code-example-1",
    "href": "02-random-graphs/index.html#code-example-1",
    "title": "Random graphs",
    "section": "3.1 Code example",
    "text": "3.1 Code example\n\n# Creating a ring\nn &lt;- 10\nV &lt;- 1:n\nk &lt;- 3\np &lt;- .2\n\nE &lt;- NULL\nfor (i in 1:k) {\n  E &lt;- rbind(E, cbind(V, c(V[-c(1:i)], V[1:i])))\n}\n\n# Generating the ring layout\nlo &lt;- layout_in_circle(graph_from_edgelist(E))\n\n# Plotting with netplot\nnplot(\n  graph_from_edgelist(E),\n  layout = lo\n  )\n\n\n\n# Rewiring\nids &lt;- which(runif(nrow(E)) &lt; p)\nE[ids, 2] &lt;- sample(V, length(ids), replace = TRUE)\nnplot(\n  graph_from_edgelist(E),\n  layout = lo\n  )"
  },
  {
    "objectID": "02-random-graphs/index.html#code-example-2",
    "href": "02-random-graphs/index.html#code-example-2",
    "title": "Random graphs",
    "section": "4.1 Code example",
    "text": "4.1 Code example\n\n# Model parameters\nn &lt;- 500\nm &lt;- 2\n\n# Generating the graph\nset.seed(3312)\ng &lt;- matrix(0, nrow = n, ncol = n)\ng[1:m, 1:m] &lt;- 1\ndiag(g) &lt;- 0\n\n# Adding nodes\nfor (i in (m + 1):n) {\n\n  # Selecting the nodes to connect to\n  ids &lt;- sample(\n    x       = 1:(i-1), # Up to i-1\n    size    = m,       # m nodes\n    replace = FALSE,   # No replacement\n    # Probability proportional to the degree\n    prob    = colSums(g[, 1:(i-1), drop = FALSE])\n    )\n\n  # Adding the edges\n  g[i, ids] &lt;- 1\n  g[ids, i] &lt;- 1\n\n}\n\n# Visualizing the degree distribution\nlibrary(ggplot2)\ndata.frame(degree = colSums(g)) |&gt;\n  ggplot(aes(degree)) +\n  geom_histogram() +\n  scale_x_log10() +\n  labs(\n    x = \"Degree\\n(log10 scale)\",\n    y = \"Count\"\n  )"
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html",
    "href": "03-behavior-and-coevolution/index.html",
    "title": "Behavior and coevolution",
    "section": "",
    "text": "This section focuses on inference involving network and a secondary outcome. While there are many ways of studying the coevolution or dependence between network and behavior, this section focuses on two classes of analysis: When the network is fixed and when both network and behavior influence each other.\nWhether we treat the network as given or endogenous sets the complexity of conducting statistical inference. Data analysis becomes much more straightforward if our research focuses on individual-level outcomes embedded in a network and not on the network itself. Here, we will deal with three particular cases: (1) when network effects are lagged, (2) egocentric networks, and (3) when network effects are contemporaneous."
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-lagged-exposure",
    "href": "03-behavior-and-coevolution/index.html#code-example-lagged-exposure",
    "title": "Behavior and coevolution",
    "section": "2.1 Code example: Lagged exposure",
    "text": "2.1 Code example: Lagged exposure\nThe following code example shows how to estimate a lagged exposure effect using the glm function in R. The model we will simulate and estimate features a Bernoulli graph with 1,000 nodes and a density of 0.01.\n\ny_{it} = \\theta_1 + \\rho \\text{Exposure}_{it} + \\theta_2 w_i + \\varepsilon\n\nwhere \\text{Exposure}_{it} is the exposure statistic defined above, and w_i is a vector of covariates.\n\n# Simulating data\nn &lt;- 1000\ntime &lt;- 2\ntheta &lt;- c(-1, 3)\n\n# Sampling a bernoilli network\nset.seed(3132)\np &lt;- 0.01\nX &lt;- matrix(rbinom(n^2, 1, p), nrow = n)\ndiag(X) &lt;- 0\n\n# Covariate\nW &lt;- matrix(rnorm(n), nrow = n)\n\n# Simulating the outcome\nrho &lt;- 0.5\nY0 &lt;- cbind(rnorm(n))\n\n# The lagged exposure\nexpo &lt;- (X %*% Y0)/rowSums(X)\nY1 &lt;- theta[1] + rho * expo + W * theta[2] + rnorm(n)\n\nNow we fit the model using GLM, in this case, linear Regression\n\nfit &lt;- glm(Y1 ~ expo + W, family = \"gaussian\")\nsummary(fit)\n\n\nCall:\nglm(formula = Y1 ~ expo + W, family = \"gaussian\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.07187    0.03284 -32.638  &lt; 2e-16 ***\nexpo         0.61170    0.10199   5.998  2.8e-09 ***\nW            3.00316    0.03233  92.891  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.071489)\n\n    Null deviance: 10319.3  on 999  degrees of freedom\nResidual deviance:  1068.3  on 997  degrees of freedom\nAIC: 2911.9\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-egocentric-networks",
    "href": "03-behavior-and-coevolution/index.html#code-example-egocentric-networks",
    "title": "Behavior and coevolution",
    "section": "3.1 Code example: Egocentric networks",
    "text": "3.1 Code example: Egocentric networks\nFor this example, we will simulate a stream of 1,000 Bernoulli graphs looking into the probability of school dropout. Each network will have between 4 and 10 nodes and have a density of 0.4. The data-generating process is as follows:\n\n{\\Pr{}}_{\\bm{\\theta}}\\left(Y_i=1\\right) = \\text{logit}^{-1}\\left(\\bm{\\theta}_x s(\\bm{X}_i) \\right)\n\nWhere s(X) \\equiv \\left(\\text{density}, \\text{n mutual ties}\\right), and \\bm{\\theta}_x = (0.5, -1). This model only features sufficient statistics. We start by simulating the networks\n\nset.seed(331)\nn &lt;- 1000\nsizes &lt;- sample(4:10, n, replace = TRUE)\n\n# Simulating the networks\nX &lt;- lapply(sizes, function(x) matrix(rbinom(x^2, 1, 0.4), nrow = x))\nX &lt;- lapply(X, \\(x) {diag(x) &lt;- 0; x})\n\n# Inspecting the first 5\nhead(X, 5)\n\n[[1]]\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    1    1    1    0\n[2,]    0    0    0    0    0\n[3,]    0    1    0    0    0\n[4,]    0    0    0    0    0\n[5,]    1    0    0    1    0\n\n[[2]]\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    1    0    1    0\n\n[[3]]\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    0    1    0    1    0    0\n[2,]    0    0    0    0    0    0\n[3,]    0    1    0    0    0    1\n[4,]    0    0    0    0    1    0\n[5,]    0    0    0    0    0    0\n[6,]    0    0    0    0    0    0\n\n[[4]]\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    1    0    1    0\n[2,]    0    0    0    0    1\n[3,]    0    1    0    0    0\n[4,]    0    1    1    0    1\n[5,]    1    0    1    0    0\n\n[[5]]\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    1    0    0    0    0\n[3,]    1    0    0    0    0\n[4,]    0    0    0    0    0\n[5,]    1    0    0    0    0\n\n\nUsing the ergm R package (Handcock et al. 2023; Hunter et al. 2008), we can extract the associated sufficient statistics of the egocentric networks:\n\nlibrary(ergm)\nstats &lt;- lapply(X, \\(x) summary_formula(x ~ density + mutual))\n\n# Turning the list into a matrix\nstats &lt;- do.call(rbind, stats)\n\n# Inspecting the first 5\nhead(stats, 5)\n\n       density mutual\n[1,] 0.3000000      0\n[2,] 0.1666667      0\n[3,] 0.1666667      0\n[4,] 0.4500000      0\n[5,] 0.1500000      0\n\n\nWe now simulate the outcomes\n\ny &lt;- rbinom(n, 1, plogis(stats %*% c(0.5, -1)))\nglm(y ~ stats, family = binomial(link = \"logit\")) |&gt;\n  summary()\n\n\nCall:\nglm(formula = y ~ stats, family = binomial(link = \"logit\"))\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.07319    0.41590   0.176    0.860    \nstatsdensity  0.42568    1.26942   0.335    0.737    \nstatsmutual  -1.14804    0.12166  -9.436   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 768.96  on 999  degrees of freedom\nResidual deviance: 518.78  on 997  degrees of freedom\nAIC: 524.78\n\nNumber of Fisher Scoring iterations: 7"
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-sar",
    "href": "03-behavior-and-coevolution/index.html#code-example-sar",
    "title": "Behavior and coevolution",
    "section": "4.1 Code example: SAR",
    "text": "4.1 Code example: SAR\nSimulation of SAR models can be done using the following observation: Although the outcome shows on both sides of the equation, we can isolate it in one side and solve for it; formally:\n\n\\bm{y} = \\rho \\bm{X} \\bm{y} + \\bm{\\theta}^{\\mathbf{t}}\\bm{W} + \\varepsilon \\implies \\bm{y} = \\left(\\bm{I} - \\rho \\bm{X}\\right)^{-1}\\bm{\\theta}^{\\mathbf{t}}\\bm{W} + \\left(\\bm{I} - \\rho \\bm{X}\\right)^{-1}\\varepsilon\n\n\nset.seed(4114)\nn &lt;- 1000\n\n# Simulating the network\np &lt;- 0.01\nX &lt;- matrix(rbinom(n^2, 1, p), nrow = n)\n\n# Covariate\nW &lt;- matrix(rnorm(n), nrow = n)\n\n# Simulating the outcome\nrho &lt;- 0.5\nlibrary(MASS) # For the mvrnorm function\n\n# Identity minus rho * X\nX_rowstoch &lt;- X / rowSums(X)\nI &lt;- diag(n) - rho * X_rowstoch\n\n# The outcome\nY &lt;- solve(I) %*% (2 * W) + solve(I) %*% mvrnorm(1, rep(0, n), diag(n))\n\nUsing the spatialreg R package:\n\nlibrary(spdep) # for the function mat2listw\nlibrary(spatialreg)\nfit &lt;- lagsarlm(Y ~ W, data = as.data.frame(X), listw = mat2listw(X_rowstoch))\nsummary(fit)\n\n\nCall:\nlagsarlm(formula = Y ~ W, data = as.data.frame(X), listw = mat2listw(X_rowstoch))\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-2.971462 -0.659254 -0.019626  0.644577  2.915956 \n\nType: lag \nCoefficients: (asymptotic standard errors) \n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.0084993  0.0304940 -0.2787   0.7805\nW            1.9737311  0.0297407 66.3647   &lt;2e-16\n\nRho: 0.53961, LR test value: 168.32, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.039338\n    z-value: 13.717, p-value: &lt; 2.22e-16\nWald statistic: 188.16, p-value: &lt; 2.22e-16\n\nLog likelihood: -1373.025 for lag model\nML residual variance (sigma squared): 0.91028, (sigma: 0.95409)\nNumber of observations: 1000 \nNumber of parameters estimated: 4 \nAIC: NA (not available for weighted model), (AIC for lm: 2920.4)\nLM test for residual autocorrelation\ntest value: 0.051083, p-value: 0.82119"
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-alaam",
    "href": "03-behavior-and-coevolution/index.html#code-example-alaam",
    "title": "Behavior and coevolution",
    "section": "4.2 Code example: ALAAM",
    "text": "4.2 Code example: ALAAM\nTo date, there is no R package implementing the ALAAM framework. Nevertheless, you can fit ALAAMs using the PNet software developed by the Melnet group at the University of Melbourne (click here).\nBecause of the similarities, ALAAMs can be implemented using ERGMs. Because of the novelty of it, the coding example will be left as a potential class project. We will post a fully-featured example after the workshop."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "If you are reading this, it is because you know that networks are everywhere. Network science is a rapidly growing field that has been applied to many different disciplines, from biology to sociology, from computer science to physics. In this course, we will go over advanced network science topics; particularly, statistical inference in networks. The course contents are:\n\nOverview of statistical inference.\nIntroduction to network science inference.\nMotif detection.\nGlobal statistics (e.g., modularity).\nRandom graphs (static).\nRandom graphs (dynamic).\nCoevolution of networks and behavior.\nAdvanced topics (sampling and conditional models)."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Home",
    "section": "",
    "text": "If you are reading this, it is because you know that networks are everywhere. Network science is a rapidly growing field that has been applied to many different disciplines, from biology to sociology, from computer science to physics. In this course, we will go over advanced network science topics; particularly, statistical inference in networks. The course contents are:\n\nOverview of statistical inference.\nIntroduction to network science inference.\nMotif detection.\nGlobal statistics (e.g., modularity).\nRandom graphs (static).\nRandom graphs (dynamic).\nCoevolution of networks and behavior.\nAdvanced topics (sampling and conditional models)."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Home",
    "section": "2 References",
    "text": "2 References\n\n\n“1.1: Basic Definitions and Concepts.” 2014. Statistics LibreTexts. https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/01%3A_Introduction_to_Statistics/1.01%3A_Basic_Definitions_and_Concepts.\n\n\nBarrett, Tyson, Matt Dowle, and Arun Srinivasan. 2023. Data.table: Extension of ‘Data.frame‘. https://r-datatable.com.\n\n\nBell, Brooke M., Donna Spruijt-Metz, George G. Vega Yon, Abu S. Mondol, Ridwan Alam, Meiyi Ma, Ifat Emi, John Lach, John A. Stankovic, and Kayla De La Haye. 2019. “Sensing Eating Mimicry Among Family Members.” Translational Behavioral Medicine. https://doi.org/10.1093/tbm/ibz051.\n\n\nBrooks, S., A. Gelman, G. Jones, and X. L. Meng. 2011. Handbook of Markov Chain Monte Carlo. CRC Press.\n\n\nCasella, George, and Roger L. Berger. 2021. Statistical Inference. Cengage Learning.\n\n\nFellows, Ian E. 2012. “Exponential Family Random Network Models.” ProQuest Dissertations and Theses. PhD thesis. https://login.ezproxy.lib.utah.edu/login?url=https://www.proquest.com/dissertations-theses/exponential-family-random-network-models/docview/1221548720/se-2.\n\n\nFrank, O, and David Strauss. 1986. “Markov graphs.” Journal of the American Statistical Association 81 (395): 832–42. https://doi.org/10.2307/2289017.\n\n\nGelman, Andrew. 2018. “The Failure of Null Hypothesis Significance Testing When Studying Incremental Changes, and What to Do About It.” Personality and Social Psychology Bulletin 44 (1): 16–23. https://doi.org/10.1177/0146167217729162.\n\n\nGeyer, Charles J., and Elizabeth A. Thompson. 1992. “Constrained Monte Carlo Maximum Likelihood for Dependent Data.” Journal of the Royal Statistical Society. Series B (Methodological) 54 (3): 657–99. https://www.jstor.org/stable/2345852.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. 2016. “Statistical Tests, P Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31 (4): 337–50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nHandcock, Mark S., David R. Hunter, Carter T. Butts, Steven M. Goodreau, Pavel N. Krivitsky, and Martina Morris. 2023. Ergm: Fit, Simulate and Diagnose Exponential-Family Models for Networks. The Statnet Project (https://statnet.org). https://CRAN.R-project.org/package=ergm.\n\n\nHaye, Kayla de la, Heesung Shin, George G. Vega Yon, and Thomas W. Valente. 2019. “Smoking Diffusion Through Networks of Diverse, Urban American Adolescents over the High School Period.” Journal of Health and Social Behavior. https://doi.org/10.1177/0022146519870521.\n\n\nHolland, Paul W., and Samuel Leinhardt. 1981. “An exponential family of probability distributions for directed graphs.” Journal of the American Statistical Association 76 (373): 33–50. https://doi.org/10.2307/2287037.\n\n\nHunter, David R. 2007. “Curved Exponential Family Models for Social Networks.” Social Networks 29 (2): 216–30. https://doi.org/10.1016/j.socnet.2006.08.005.\n\n\nHunter, David R., Mark S. Handcock, Carter T. Butts, Steven M. Goodreau, and Martina Morris. 2008. “ergm: A Package to Fit, Simulate and Diagnose Exponential-Family Models for Networks.” Journal of Statistical Software 24 (3): 1–29. https://doi.org/10.18637/jss.v024.i03.\n\n\nKoskinen, Johan, Pete Jones, Darkhan Medeuov, Artem Antonyuk, Kseniia Puzyreva, and Nikita Basov. 2023. “Analysing Networks of Networks.” Social Networks 74 (July): 102–17. https://doi.org/10.1016/j.socnet.2023.02.002.\n\n\nKoskinen, Johan, Peng Wang, Garry Robins, and Philippa Pattison. 2018. “Outliers and Influential Observations in Exponential Random Graph Models.” Psychometrika 83 (4): 809–30. https://doi.org/10.1007/s11336-018-9635-8.\n\n\nKrivitsky, Pavel N. 2012. “Exponential-Family Random Graph Models for Valued Networks.” Electronic Journal of Statistics 6: 1100–1128. https://doi.org/10.1214/12-EJS696.\n\n\n———. 2023. Ergm.multi: Fit, Simulate and Diagnose Exponential-Family Models for Multiple or Multilayer Networks. The Statnet Project (https://statnet.org). https://CRAN.R-project.org/package=ergm.multi.\n\n\nKrivitsky, Pavel N., Pietro Coletti, and Niel Hens. 2023. “A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks.” Journal of the American Statistical Association 0 (0): 1–21. https://doi.org/10.1080/01621459.2023.2242627.\n\n\nKrivitsky, Pavel N., David R. Hunter, Martina Morris, and Chad Klumb. 2023. “Ergm 4: New Features for Analyzing Exponential-Family Random Graph Models.” Journal of Statistical Software 105 (January): 1–44. https://doi.org/10.18637/jss.v105.i06.\n\n\nLeifeld, Philip. 2013. “Texreg : Conversion of Statistical Model Output in R to L A T E X and HTML Tables.” Journal of Statistical Software 55 (8). https://doi.org/10.18637/jss.v055.i08.\n\n\nLeSage, James P. 2008. “An Introduction to Spatial Econometrics.” Revue d’économie Industrielle 123 (123): 19–44. https://doi.org/10.4000/rei.3887.\n\n\nLeSage, James P., and R. Kelley Pace. 2014. “The Biggest Myth in Spatial Econometrics.” Econometrics 2 (4): 217–49. https://doi.org/10.2139/ssrn.1725503.\n\n\nLusher, Dean, Johan Koskinen, and Garry Robins. 2013. Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications. Cambridge University Press.\n\n\nMilo, R., S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon. 2002. “Network Motifs: Simple Building Blocks of Complex Networks.” Science 298 (5594): 824–27. https://doi.org/10.1126/science.298.5594.824.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRobins, Garry, Philippa Pattison, and Peter Elliott. 2001. “Network Models for Social Influence Processes.” Psychometrika 66 (2): 161–89. https://doi.org/10.1007/BF02294834.\n\n\nRobins, Garry, Pip Pattison, Yuval Kalish, and Dean Lusher. 2007. “An introduction to exponential random graph (p*) models for social networks.” Social Networks 29 (2): 173–91. https://doi.org/10.1016/j.socnet.2006.08.002.\n\n\nRoger Bivand. 2022. “R Packages for Analyzing Spatial Data: A Comparative Case Study with Areal Data.” Geographical Analysis 54 (3): 488–518. https://doi.org/10.1111/gean.12319.\n\n\nSlaughter, Andrew J., and Laura M. Koehly. 2016. “Multilevel Models for Social Networks: Hierarchical Bayesian Approaches to Exponential Random Graph Modeling.” Social Networks 44: 334–45. https://doi.org/10.1016/j.socnet.2015.11.002.\n\n\nSnijders, Tom A B, Philippa E Pattison, Garry L Robins, and Mark S Handcock. 2006. “New specifications for exponential random graph models.” Sociological Methodology 36 (1): 99–153. https://doi.org/10.1111/j.1467-9531.2006.00176.x.\n\n\nSnijders, Tom A. B., and Stephen P. Borgatti. 1999. “Non-Parametric Standard Errors and Tests for Network Statistics.” Connections 22 (2): 1–10.\n\n\nStivala, Alex, Garry Robins, and Alessandro Lomi. 2020. “Exponential Random Graph Model Parameter Estimation for Very Large Directed Networks.” PLoS ONE 15 (1): 1–23. https://doi.org/10.1371/journal.pone.0227804.\n\n\nTanaka, Kyosuke, and George G. Vega Yon. 2024. “Imaginary Network Motifs: Structural Patterns of False Positives and Negatives in Social Networks.” Social Networks 78 (July): 65–80. https://doi.org/10.1016/j.socnet.2023.11.005.\n\n\nValente, Thomas W., and George G. Vega Yon. 2020. “Diffusion/Contagion Processes on Social Networks.” Health Education & Behavior 47 (2): 235–48. https://doi.org/10.1177/1090198120901497.\n\n\nValente, Thomas W., Heather Wipfli, and George G. Vega Yon. 2019. “Network Influences on Policy Implementation: Evidence from a Global Health Treaty.” Social Science and Medicine. https://doi.org/10.1016/j.socscimed.2019.01.008.\n\n\nVega Yon, George. 2020. ergmito: Exponential Random Graph Models for Small Networks. CRAN. https://cran.r-project.org/package=ergmito.\n\n\nVega Yon, George G. 2023. “Power and Multicollinearity in Small Networks: A Discussion of ‘Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks’ by Krivitsky, Coletti & Hens.” Journal Of The American Statistical Association.\n\n\nVega Yon, George G., Andrew Slaughter, and Kayla de la Haye. 2021. “Exponential Random Graph Models for Little Networks.” Social Networks 64 (August 2020): 225–38. https://doi.org/10.1016/j.socnet.2020.07.005.\n\n\nWang, Zeyi, Ian E. Fellows, and Mark S. Handcock. 2023. “Understanding Networks with Exponential-Family Random Network Models.” Social Networks, August, S0378873323000497. https://doi.org/10.1016/j.socnet.2023.07.003.\n\n\nWasserman, Stanley, and Philippa Pattison. 1996. “Logit models and logistic regressions for social networks: I. An introduction to Markov graphs and p*.” Psychometrika 61 (3): 401–25. https://doi.org/10.1007/BF02294547."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Home",
    "section": "3 Disclaimer",
    "text": "3 Disclaimer\nThis is an ongoing project. The course is being developed and will be updated as we go. If you have any comments or suggestions, please let me know. The generation of the course materials was assisted by AI tools, namely, GitHub copilot."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Home",
    "section": "4 Code of Conduct",
    "text": "4 Code of Conduct\nPlease note that the networks-udd2024 project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms."
  },
  {
    "objectID": "01-fundamentals/index.html#r-packages",
    "href": "01-fundamentals/index.html#r-packages",
    "title": "Fundamentals",
    "section": "1.8 R packages",
    "text": "1.8 R packages\nR is so powerful because of its extensions. R extensions (different from other programming languages) are called packages. Packages are collections of functions, data, and documentation that provide additional functionality to R. Although anyone can create and distribute R packages to other users, the Comprehensive R Archive Network [CRAN] is the official repository of R packages. All packages posted on CRAN are thoroughly tested, so generally, their quality is high.\nTo install R packages, we use the install.packages() function; to load them, we use the library() function. For example, the following code chunk installs the ergm package and loads it:\n\ninstall.packages(\"ergm\")\nlibrary(ergm)"
  },
  {
    "objectID": "04-intro-to-ergms/index.html",
    "href": "04-intro-to-ergms/index.html",
    "title": "Introduction to ERGMs",
    "section": "",
    "text": "Have you ever wondered if the friend of your friend is your friend? Or if the people you consider to be your friends feel the same about you? Or if age is related to whether you seek advice from others? All these (and many others certainly more creative) questions can be answered using Exponential-Family Random Graph Models."
  },
  {
    "objectID": "04-intro-to-ergms/index.html#the-logistic-distribution",
    "href": "04-intro-to-ergms/index.html#the-logistic-distribution",
    "title": "Introduction to ERGMs",
    "section": "3.1 The logistic distribution",
    "text": "3.1 The logistic distribution\nLet’s start by stating the result: Conditioning on all graphs that are not y_{ij}, the probability of a tie Y_{ij} is distributed Logistic; formally:\n\nP_{\\mathcal{Y}, \\bm{\\theta}}(Y_{ij}=1 | \\bm{y}_{-ij}) = \\frac{1}{1 + \\exp \\left(\\bm{\\theta}^{\\mathbf{t}}\\delta_{ij}(\\bm{y}){}\\right)},\n\nwhere \\delta_{ij}(\\bm{y}){}\\equiv s_{ij}^+(\\bm{y}) - s_{ij}^-(\\bm{y}) is the change statistic, and s_{ij}^+(\\bm{y}) and s_{ij}^-(\\bm{y}) are the statistics of the graph with and without the tie Y_{ij}, respectively.\nThe importance of this result is two-fold: (a) we can use this equation to interpret fitted models in the context of a single graph (like using odds,) and (b) we can use this equation to simulate from the model, without touching the normalizing constant."
  },
  {
    "objectID": "04-intro-to-ergms/index.html#the-ratio-of-normalizing-constants",
    "href": "04-intro-to-ergms/index.html#the-ratio-of-normalizing-constants",
    "title": "Introduction to ERGMs",
    "section": "1.2 The ratio of normalizing constants",
    "text": "1.2 The ratio of normalizing constants\nThe second significant result is that the ratio of normalizing constants can be approximated through simulation. Formally:\n\n\\frac{\\kappa(\\bm{\\theta})}{\\kappa(\\bm{\\theta}')} = \\mathbb{E}_{\\mathcal{Y}, \\bm{\\theta}'}\\left((\\bm{\\theta} - \\bm{\\theta}')s(\\bm{y})^{\\mathbf{t}}\\right) \\approx \\frac{1}{N}\\sum_{i=1}^N (\\bm{\\theta} - \\bm{\\theta}')s(\\bm{y}_i)^{\\mathbf{t}},\n\nWhere \\bm{\\theta}' is an arbitrary vector of parameters, and \\bm{y}_i is a sample from the distribution P_{\\mathcal{Y}, \\bm{\\theta}'}."
  },
  {
    "objectID": "04-intro-to-ergms/index.html#the-ratio-of-loglikelihoods",
    "href": "04-intro-to-ergms/index.html#the-ratio-of-loglikelihoods",
    "title": "Introduction to ERGMs",
    "section": "3.2 The ratio of loglikelihoods",
    "text": "3.2 The ratio of loglikelihoods\nThe second significant result is that the ratio of loglikelihoods can be approximated through simulation. It is based on the following observation by Geyer and Thompson (1992):\n\n\\frac{\\kappa(\\bm{\\theta})}{\\kappa(\\bm{\\theta}_0)} = \\mathbb{E}_{\\mathcal{Y}, \\bm{\\theta}_0}\\left((\\bm{\\theta} - \\bm{\\theta}_0)s(\\bm{y})^{\\mathbf{t}}\\right),\n\nUsing the latter, we can approximate the following loglikelihood ratio:\n\\begin{align*}\nl(\\bm{\\theta}) - l(\\bm{\\theta}_0) = & (\\bm{\\theta} - \\bm{\\theta}_0)^{\\mathbf{t}}s(\\bm{y}) - \\log\\left[\\frac{\\kappa(\\bm{\\theta})}{\\kappa(\\bm{\\theta}_0)}\\right]\\\\\n\\approx & (\\bm{\\theta} - \\bm{\\theta}_0)^{\\mathbf{t}}s(\\bm{y}) - \\log\\left[M^{-1}\\sum_{\\bm{y}^{(m)}} (\\bm{\\theta} - \\bm{\\theta}_0)^{\\mathbf{t}}s(\\bm{y}^{(m)})\\right]\n\\end{align*}\nWhere \\bm{\\theta}_0 is an arbitrary vector of parameters, and \\bm{y}^{(m)} are sampled from the distribution P_{\\mathcal{Y}, \\bm{\\theta}_0}. In the words of Geyer and Thompson (1992), “[…] it is possible to approximate \\bm{\\theta} by using simulations from one distribution P_{\\mathcal{Y}, \\bm{\\theta}_0} no matter which \\bm{\\theta}_0 in the parameter space is.”"
  },
  {
    "objectID": "00-fundamentals/index.html",
    "href": "00-fundamentals/index.html",
    "title": "Fundamentals",
    "section": "",
    "text": "Before jumping into network science details, we need to cover some fundamentals. I assume that most of the contents here are well known to you–we will be brief–but I want to ensure we are all on the same page."
  },
  {
    "objectID": "00-fundamentals/index.html#getting-help",
    "href": "00-fundamentals/index.html#getting-help",
    "title": "Fundamentals",
    "section": "1.1 Getting help",
    "text": "1.1 Getting help\nUnlike other languages, R’s documentation is highly reliable. The Comprehensive R Archive Network [CRAN] is the official repository of R packages. All packages posted on CRAN must pass a series of tests to ensure the quality of the code, including the documentation.\nTo get help on a function, we can use the help() function. For example, if we wanted to get help on the mean() function, we would do:\n\nhelp(\"mean\")"
  },
  {
    "objectID": "00-fundamentals/index.html#naming-conventions",
    "href": "00-fundamentals/index.html#naming-conventions",
    "title": "Fundamentals",
    "section": "1.2 Naming conventions",
    "text": "1.2 Naming conventions\nR has a set of naming conventions that we should follow to avoid confusion. The most important ones are:\n\nUse lowercase letters (optional)\nUse underscores to separate words (optional)\nDo not start with a number\nDo not use special characters\nDo not use reserved words\n\n\n\n\n\n\n\nQuestion\n\n\n\nOf the following list, which are valid names and which are valid but to be avoided?\n_my.var\nmy.var\nmy_var\nmyVar\nmyVar1\n1myVar\nmy var\nmy-var"
  },
  {
    "objectID": "00-fundamentals/index.html#assignment",
    "href": "00-fundamentals/index.html#assignment",
    "title": "Fundamentals",
    "section": "1.3 Assignment",
    "text": "1.3 Assignment\nIn R, we have two (four) ways of assigning values to objects: the &lt;- and = binary operators2. Although both are equivalent, the former is the preferred way of assigning values to objects since the latter can be confused with function arguments.\n\nx &lt;- 1\nx = 1\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the difference between the following two assignments? Use the help function to find out.\nx &lt;- 1\nx &lt;&lt;- 1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are other ways in which you can assign values to objects?"
  },
  {
    "objectID": "00-fundamentals/index.html#using-functions-and-piping",
    "href": "00-fundamentals/index.html#using-functions-and-piping",
    "title": "Fundamentals",
    "section": "1.4 Using functions and piping",
    "text": "1.4 Using functions and piping\nIn R, we use functions to perform operations on objects. Functions are implemented as function_name ( argument_1 , argument_2 , ... ). For example, the mean() function takes a vector of numbers and returns the mean of the values:\n\nx &lt;- c(1, 2, 3) # The c() function creates a vector\nmean(x)\n## [1] 2\n\nFurthermore, we can use the pipe operator (|&gt;) to improve readability. The pipe operator takes the output of the left-hand side expression and passes it as the first argument of the right-hand side expression. Our previous example could be rewritten as:\n\nc(1, 2, 3) |&gt; mean()\n## [1] 2"
  },
  {
    "objectID": "00-fundamentals/index.html#data-structures",
    "href": "00-fundamentals/index.html#data-structures",
    "title": "Fundamentals",
    "section": "1.5 Data structures",
    "text": "1.5 Data structures\nAtomic types are the minimal building blocks of R. They are logical, integer, double, character, complex, raw:\n\nx_logical   &lt;- TRUE\nx_integer   &lt;- 1L\nx_double    &lt;- 1.0\nx_character &lt;- \"a\"\nx_complex   &lt;- 1i\nx_raw       &lt;- charToRaw(\"a\")\n\nUnlike other languages, we do not need to declare the data type before creating the object; R will infer it from the value.\n\n\n\n\n\n\nPro-tip\n\n\n\nAdding the L suffix to the value is good practice when dealing with integers. Some R packages like data.table (Barrett, Dowle, and Srinivasan 2023) have internal checks that will throw an error if you are not explicit about the data type.\n\n\nThe next type is the vector. A vector is a collection of elements of the same type. The most common way to create a vector is with the c() function:\n\nx_integer &lt;- c(1, 2, 3)\nx_double  &lt;- c(1.0, 2.0, 3.0)\nx_logical &lt;- c(TRUE, FALSE, TRUE)\n# etc.\n\nR will coerce the data types to the most general type. For example, if we mix integers and doubles, R will coerce the integers into doubles. The coercion order is logical &lt; integer &lt; double &lt; character\n\n\n\n\n\n\nQuestion\n\n\n\nWhy is the coercion order logical &lt; integer &lt; double &lt; character?\n\n\nThe next data structure is the list. A list is a collection of elements of any type. We can create a list with the list() function:\n\nx_list       &lt;- list(1, 2.0, TRUE, \"a\")\nx_list_named &lt;- list(a = 1, b = 2.0, c = TRUE, d = \"a\")\n\nTo access elements in a list, we have two options: by position or by name, the latter only if the elements are named:\n\nx_list[[1]]\n## [1] 1\nx_list_named[[\"a\"]]\n## [1] 1\nx_list_named$a\n## [1] 1\n\nAfter lists, we have matrices. A matrix is a collection of elements of the same type arranged in a two-dimensional grid. We can create a matrix with the matrix() function:\n\nx_matrix &lt;- matrix(1:9, nrow = 3, ncol = 3)\nx_matrix\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n\n# We can access elements in a matrix by row column, or position:\nx_matrix[1, 2]\n## [1] 4\nx_matrix[cbind(1, 2)]\n## [1] 4\nx_matrix[4]\n## [1] 4\n\n\n\n\n\n\n\nMatrix is a vector\n\n\n\nMatrices in R are vectors with dimensions. In base R, matrices are stored in column-major order. This means that the elements are stored column by column. This is important to know when we are accessing elements in a matrix\n\n\nThe two last data structures are arrays and data frames. An array is a collection of elements of the same type arranged in a multi-dimensional grid. We can create an array with the array() function:\n\nx_array &lt;- array(1:27, dim = c(3, 3, 3))\n\n# We can access elements in an array by row, column, and dimension, or\n# position:\nx_array[1, 2, 3]\n## [1] 22\nx_array[cbind(1, 2, 3)]\n## [1] 22\nx_array[22]\n## [1] 22\n\nData frames are the most common data structure in R. In principle, these objects are lists of vectors of the same length, each vector representing a column. Columns (lists) in data frames can be of different types, but elements in each column must be of the same type. We can create a data frame with the data.frame() function:\n\nx_data_frame &lt;- data.frame(\n  a = 1:3,\n  b = c(\"a\", \"b\", \"c\"),\n  c = c(TRUE, FALSE, TRUE)\n)\n\n# We can access elements in a data frame by row, column, or position:\nx_data_frame[1, 2]\n## [1] \"a\"\nx_data_frame[cbind(1, 2)]\n## [1] \"a\"\nx_data_frame$b[1]    # Like a list\n## [1] \"a\"\nx_data_frame[[2]][1] # Like a list too\n## [1] \"a\""
  },
  {
    "objectID": "00-fundamentals/index.html#functions",
    "href": "00-fundamentals/index.html#functions",
    "title": "Fundamentals",
    "section": "1.6 Functions",
    "text": "1.6 Functions\nFunctions are the most important building blocks of R. A function is a set of instructions that takes one or more inputs and returns one or more outputs. We can create a function with the function() function:\n\n# This function has two arguments (y is optional)\nf &lt;- function(x, y = 1) {\n  x + 1\n}\n\nf(1)\n## [1] 2\n\nStarting with R 4, we can use the lambda syntax to create functions:\n\nf &lt;- \\(x, y) x + 1\n\nf(1)\n## [1] 2"
  },
  {
    "objectID": "00-fundamentals/index.html#control-flow",
    "href": "00-fundamentals/index.html#control-flow",
    "title": "Fundamentals",
    "section": "1.7 Control flow",
    "text": "1.7 Control flow\nControl flow statements allow us to control the execution of the code. The most common control flow statements are if, for, while, and repeat. We can create a control flow statement with the if(), for(), while(), and repeat() functions:\n\n# if\nif (TRUE) {\n  \"a\"\n} else {\n  \"b\"\n}\n## [1] \"a\"\n\n# for\nfor (i in 1:3) {\n  cat(\"This is the number \", i, \"\\n\")\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3\n\n# while\ni &lt;- 1\nwhile (i &lt;= 3) {\n  cat(\"This is the number \", i, \"\\n\")\n  i &lt;- i + 1\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3\n\n# repeat\ni &lt;- 1\nrepeat {\n  cat(\"This is the number \", i, \"\\n\")\n  i &lt;- i + 1\n  if (i &gt; 3) {\n    break\n  }\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3"
  },
  {
    "objectID": "00-fundamentals/index.html#r-packages",
    "href": "00-fundamentals/index.html#r-packages",
    "title": "Fundamentals",
    "section": "1.8 R packages",
    "text": "1.8 R packages\nR is so powerful because of its extensions. R extensions (different from other programming languages) are called packages. Packages are collections of functions, data, and documentation that provide additional functionality to R. Although anyone can create and distribute R packages to other users, the Comprehensive R Archive Network [CRAN] is the official repository of R packages. All packages posted on CRAN are thoroughly tested, so generally, their quality is high.\nTo install R packages, we use the install.packages() function; to load them, we use the library() function. For example, the following code chunk installs the ergm package and loads it:\n\ninstall.packages(\"ergm\")\nlibrary(ergm)"
  },
  {
    "objectID": "00-fundamentals/index.html#hypothesis-testing",
    "href": "00-fundamentals/index.html#hypothesis-testing",
    "title": "Fundamentals",
    "section": "2.1 Hypothesis testing",
    "text": "2.1 Hypothesis testing\nAccording to Wikipedia\n\nA statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis. More generally, hypothesis testing allows us to make probabilistic statements about population parameters. More informally, hypothesis testing is the processes of making decisions under uncertainty. Typically, hypothesis testing procedures involve a user selected tradeoff between false positives and false negatives. – Wiki\n\nIn a nutshell, hypothesis testing is performed by following these steps:\n\nState the null and alternative hypotheses. In general, the null hypothesis is a statement about the population parameter that challenges our research question; for example, given the question of whether two networks are different, the null hypothesis would be that the two networks are the same.\nCompute the corresponding test statistic. It is a data function that reduces the information to a single number.\nCompare the observed test statistic with the distribution of the test statistic under the null hypothesis. The sometimes infamous p-value: ``[…] the probability that the chosen test statistic would have been at least as large as its observed value if every model assumption were correct, including the test hypothesis.’’ (Greenland et al. 2016) 3\n\n\nReport the observed effect and p-value, i.e., \\Pr(t \\in H_0)\n\nWe usually say that we either reject the null hypothesis or fail to reject it (we never accept the null hypothesis,) but, in my view, it is always better to talk about it in terms of “suggests evidence for” or “suggests evidence against.”\nWe will illustrate statistical concepts more concretely in the next section."
  },
  {
    "objectID": "00-fundamentals/index.html#probability-distributions",
    "href": "00-fundamentals/index.html#probability-distributions",
    "title": "Fundamentals",
    "section": "3.1 Probability distributions",
    "text": "3.1 Probability distributions\nR has a standard way of naming probability functions. The naming structure is [type of function][distribution], where [type of function] can be d for density, p for cumulative distribution function, q for quantile function, and r for random generation. For example, the normal distribution has the following functions:\n\ndnorm(0, mean = 0, sd = 1)\n## [1] 0.3989423\npnorm(0, mean = 0, sd = 1)\n## [1] 0.5\nqnorm(0.5, mean = 0, sd = 1)\n## [1] 0\n\nNow, if we wanted to know what is the probability of observing a value smaller than -2 comming from a standard normal distribution, we would do:\n\npnorm(-2, mean = 0, sd = 1)\n## [1] 0.02275013\n\nCurrently, R has a wide range of probability distributions implemented.\n\n\n\n\n\n\nQuestion\n\n\n\nHow many probability distributions are implemented in R’s stats package?"
  },
  {
    "objectID": "00-fundamentals/index.html#random-number-generation",
    "href": "00-fundamentals/index.html#random-number-generation",
    "title": "Fundamentals",
    "section": "3.2 Random number generation",
    "text": "3.2 Random number generation\nRandom numbers, and more precisely, pseudo-random numbers, are a vital component of statistical programming. Pure randomness is hard to come by, and so we rely on pseudo-random number generators (PRNGs) to generate random numbers. These generators are deterministic algorithms that produce sequences of numbers we can then use to generate random samples from probability distributions. Because of the latter, PRNGs need a starting point called the seed. As a statistical computing program, R has a variety of PRNGs. As suggested in the previous subsection, we can generate random numbers from a probability distribution with the r function. In what follows, we will draw random numbers from a few distributions and plot histograms of the results:\n\nset.seed(1)\n\n# Saving the current graphical parameters\nop &lt;- par(mfrow = c(2,2))\nrnorm(1000) |&gt; hist(main = \"Normal distribution\")\nrunif(1000) |&gt; hist(main = \"Uniform distribution\")\nrpois(1000, lambda = 1) |&gt; hist(main = \"Poisson distribution\")\nrbinom(1000, size = 10, prob = 0.1) |&gt; hist(main = \"Binomial distribution\")\n\n\n\npar(op)"
  },
  {
    "objectID": "00-fundamentals/index.html#simulations-and-sampling",
    "href": "00-fundamentals/index.html#simulations-and-sampling",
    "title": "Fundamentals",
    "section": "3.3 Simulations and sampling",
    "text": "3.3 Simulations and sampling\nSimulations are front and center in statistical programming. We can use them to test the properties of statistical methods, generate data, and perform statistical inference. The following example uses the sample function in R to compute the bootstrap standard error of the mean (see Casella and Berger 2021):\n\nset.seed(1)\nx &lt;- rnorm(1000)\n\n# Bootstrap standard error of the mean\nn &lt;- length(x)\nB &lt;- 1000\n\n# We will store the results in a vector\nres &lt;- numeric(B)\n\nfor (i in 1:B) {\n  # Sample with replacement\n  res[i] &lt;- sample(x, size = n, replace = TRUE) |&gt;\n    mean()\n}\n\n# Plot the results\nhist(res, main = \"Bootstrap standard error of the mean\")\n\n\n\n\nSince the previous example is rather extensive, let us review it in detail.\n\nset.seed(1) sets the seed of the PRNG to 1. It ensures we get the same results every time we run the code.\nrnorm() generates a sample of 1,000 standard-normal values.\nn &lt;- length(x) stores the length of the vector in the n variable.\nB &lt;- 1000 stores the number of bootstrap samples in the B variable.\nres &lt;- numeric(B) creates a vector of length B to store the results.\nfor (i in 1:B) is a for loop that iterates from 1 to B.\nres[i] &lt;- sample(x, size = n, replace = TRUE) |&gt; mean() samples n values from x with replacement and computes the mean of the sample.\nThe pipe operator (|&gt;) passes the output of the left-hand side expression as the first argument of the right-hand side expression.\nhist(res, main = \"Bootstrap standard error of the mean\") plots the histogram of the results.\n\n\n\n\n\n\n\nQuestion\n\n\n\nSimulating convolutions: Using what you have learned about statistical functions in R, simulate the convolution of two normal distributions, one with (\\mu, \\sigma^2) = (-3, 1) and the other with (\\mu, \\sigma^2) = (2, 2). Plot the histogram of the results. Draw 1,000 samples.\n\n\nCode\nset.seed(1)\nx &lt;- rnorm(1000, mean = -3, sd = 1)\ny &lt;- rnorm(1000, mean = 2, sd = 2)\nz &lt;- x + y\n\nhist(z)\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBimodal distribution: Using the previous two normal distributions, simulate a bimodal distribution where the probability of sampling from the first distribution is 0.3 and the probability of sampling from the second distribution is 0.7. Plot the histogram of the results. (Hint: use a combination of runif() and ifelse()).\n\n\nCode\nz &lt;- ifelse(runif(1000) &lt; 0.3, x, y)\ndensity(z) |&gt; plot()"
  },
  {
    "objectID": "00-fundamentals/index.html#footnotes",
    "href": "00-fundamentals/index.html#footnotes",
    "title": "Fundamentals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough, not for network science in general.↩︎\nIn mathematics and computer science, a binary operator is a function that takes two arguments. In R, binary operators are implemented as variable 1 [operator] variable 2. For example, 1 + 2 is a binary operation.↩︎\nThe discussion about interpreting p-values and hypothesis testing is vast and relevant. Although we will not review this here, I recommend looking into the work of Andrew Gelman Gelman (2018).↩︎"
  },
  {
    "objectID": "01-overview/index.html",
    "href": "01-overview/index.html",
    "title": "Overview",
    "section": "",
    "text": "When networks are involved, statistical inference becomes tricky.\nThe IID assumption is violated by construction.\nAlthough it is tempting to use the same tools as in the IID case, they are not valid."
  },
  {
    "objectID": "01-overview/index.html#when-the-network-is-deterministic",
    "href": "01-overview/index.html#when-the-network-is-deterministic",
    "title": "Overview",
    "section": "2.1 When the network is deterministic",
    "text": "2.1 When the network is deterministic\n\n2.1.1 Single network\nIf the network is fixed (or treated as if it were fixed,) it is most likely that a traditional statistical analysis can be performed. For instance, if we are interested in influence behavior between adolescents, and we assume influence is a process that takes time, then a lagged regression may suffice (Haye et al. 2019; Valente and Vega Yon 2020; Valente, Wipfli, and Vega Yon 2019).\n\n\\mathbf{y}_t = \\rho \\mathbf{W} \\mathbf{y}_{t-1} + \\mathbf{X} \\bm{\\beta} + \\bm{\\varepsilon}, \\quad \\varepsilon_i \\sim \\text{N}\\left(0, \\sigma^2\\right)\n\\tag{1}\nwhere \\mathbf{y}_t is a vector of behaviors at time t, \\mathbf{W} is the row-stochastic adjacency matrix of the network, \\mathbf{X} is a matrix of covariates, and the elements of \\bm{\\varepsilon}\\equiv \\{\\varepsilon_i\\} distribute normal with mean zero and variance \\sigma^2.\nNonetheless, if assuming a lagged influence effect is no longer possible, then the regular regression model is no longer valid. Instead, we can resort to a Spatial Autocorrelation Regression Model [SAR] (see LeSage 2008):\n\n\\mathbf{y} = \\rho \\mathbf{W} \\mathbf{y} + \\mathbf{X} \\bm{\\beta} + \\bm{\\varepsilon},\\quad \\bm{\\varepsilon} \\sim \\text{MVN}\\left(0, \\Sigma^2\\right)\n\\tag{2}\nfurthermore\n\n\\mathbf{y} = \\left(I - \\rho\\mathbf{W}\\right)^{-1}\\left(\\mathbf{X}\\bm{\\beta} + \\bm{\\varepsilon}\\right)\n\nWhere \\bm{\\varepsilon} now distributes Multivariate Normal with mean zero and covariance matrix \\Sigma^2 \\mathbf{I}.\n\n\n\n\n\n\nTip\n\n\n\nWhat is the appropriate network to use in the SAR model? According to LeSage and Pace (2014), it is not very important. Since (I_n - \\rho \\mathbf{W})^{-1} = \\rho \\mathbf{W} + \\rho^2 \\mathbf{W}^2 + \\dots.\n\n\n\n\n2.1.2 Multiple networks\nSometimes, instead of a single network, we are interested in understanding how network-level properties affect the behavior of individuals. For instance, we may be interested in understanding the relation between triadic closure and income within a sample of independent egocentric networks; in such a case, as the networks are independent, a simple regression analysis may suffice."
  },
  {
    "objectID": "01-overview/index.html#when-the-network-is-random",
    "href": "01-overview/index.html#when-the-network-is-random",
    "title": "Overview",
    "section": "2.2 When the network is random",
    "text": "2.2 When the network is random\n\n2.2.1 Deterministic behavior\nIn this case, the behavior is treated as given, i.e., a covariate/feature of the model. When such is the case, the method of choice is the Exponential Random Graph Model [ERGM] (Lusher, Koskinen, and Robins 2013; Krivitsky 2012 and others).\n\n\n2.2.2 Random behavior"
  },
  {
    "objectID": "01-overview/index.html#non-parametric-approaches",
    "href": "01-overview/index.html#non-parametric-approaches",
    "title": "Overview",
    "section": "2.3 Non-parametric approaches",
    "text": "2.3 Non-parametric approaches\nOther common scenarios involve more convoluted/complex questions. For instance, in the case of dyadic behavior Bell et al. (2019).\nIn Tanaka and Vega Yon (2024), we study the prevalence of perception-based network motifs. While the ERGM framework would be a natural choice, as a first approach, we used non-parametric tests for hypothesis testing."
  },
  {
    "objectID": "04-intro-to-ergms/index.html#data",
    "href": "04-intro-to-ergms/index.html#data",
    "title": "Introduction to ERGMs",
    "section": "5.1 Data",
    "text": "5.1 Data\n\n\n\n\n\n\nTake your time\n\n\n\nHere, we will not take the time to investigate our network properly. However, you should always do so. Make sure you do descriptive statistics (density, centrality, modularity, etc.), check missing values, and inspect your data visually through “notepad” and visualizations before jumping into your ERG model."
  },
  {
    "objectID": "04-intro-to-ergms/index.html#simplest-model-bernoulli-graph",
    "href": "04-intro-to-ergms/index.html#simplest-model-bernoulli-graph",
    "title": "Introduction to ERGMs",
    "section": "5.2 Simplest model: Bernoulli graph",
    "text": "5.2 Simplest model: Bernoulli graph\nThe step is to check whether we can fit an ERGM or not. We can do so with the Bernoulli graph:\n\nmodel_1 &lt;- ergm(Y ~ edges)\n\nStarting maximum pseudolikelihood estimation (MPLE):\n\n\nObtaining the responsible dyads.\n\n\nEvaluating the predictor and response matrix.\n\n\nMaximizing the pseudolikelihood.\n\n\nFinished MPLE.\n\n\nEvaluating log-likelihood at the estimate. \n\nsummary(model_1)\n\nCall:\nergm(formula = Y ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -3.78885    0.06833      0  -55.45   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 13724  on 9900  degrees of freedom\n Residual Deviance:  2102  on 9899  degrees of freedom\n \nAIC: 2104  BIC: 2112  (Smaller is better. MC Std. Err. = 0)\n\n\n\n# Fitting two more models (output message suppressed)\nmodel_2 &lt;- ergm(Y ~ edges + mutual) \nmodel_3 &lt;- ergm(Y ~ edges + mutual + gwdsp(.5, fixed = TRUE))\n# model_4 &lt;- ergm(Y ~ edges + triangles) # bad idea\n\nRight after fitting a model, we want to inspect the results. An excellent tool for this is the R package texreg (Leifeld 2013):\nlibrary(texreg)\nknitreg(list(model_1, model_2, model_3))\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\n\n\n\nedges\n\n\n-3.79***\n\n\n-3.93***\n\n\n-3.84***\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.08)\n\n\n(0.21)\n\n\n\n\nmutual\n\n\n \n\n\n2.15***\n\n\n2.14***\n\n\n\n\n \n\n\n \n\n\n(0.32)\n\n\n(0.29)\n\n\n\n\ngwdsp.OTP.fixed.0.5\n\n\n \n\n\n \n\n\n-0.02\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.05)\n\n\n\n\nAIC\n\n\n2104.43\n\n\n2068.22\n\n\n2071.94\n\n\n\n\nBIC\n\n\n2111.63\n\n\n2082.62\n\n\n2093.54\n\n\n\n\nLog Likelihood\n\n\n-1051.22\n\n\n-1032.11\n\n\n-1032.97\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nSo far, model_2 is winning. We will continue with this model."
  },
  {
    "objectID": "04-intro-to-ergms/index.html#lets-add-a-little-bit-of-structure",
    "href": "04-intro-to-ergms/index.html#lets-add-a-little-bit-of-structure",
    "title": "Introduction to ERGMs",
    "section": "4.3 Let’s add a little bit of structure",
    "text": "4.3 Let’s add a little bit of structure\nNow that we only have a model featuring endogenous terms, we can add vertex/edge-covariates. Starting with \"fav_music\", there are a couple of different ways to use this node feature:\n\nDirectly through homophily (assortative mixing): Using the nodematch term, we can control for the propensity of individuals to connect based on shared music taste.\nHomophily (v2): We could activate the option diff = TRUE using the same term. By doing this, the homophily term is operationalized differently, adding as many terms as options in the vertex attribute.\nMixing: We can use the term nodemix for individuals’ tendency to mix between musical tastes.\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn this context, what would be the different hypotheses behind each decision?\n\n\n\nmodel_4 &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\"))\nmodel_5 &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\", diff = TRUE))\nmodel_6 &lt;- ergm(Y ~ edges + mutual + nodemix(\"fav_music\"))\n\nNow, let’s inspect what we have so far:\nknitreg(list(`2` = model_2, `4` = model_4, `5` = model_5, `6` = model_6))\n\n\n\nStatistical models\n\n\n\n\n \n\n\n2\n\n\n4\n\n\n5\n\n\n6\n\n\n\n\n\n\nedges\n\n\n-3.93***\n\n\n-4.29***\n\n\n-4.29***\n\n\n-3.56***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.11)\n\n\n(0.11)\n\n\n(0.24)\n\n\n\n\nmutual\n\n\n2.15***\n\n\n1.99***\n\n\n2.00***\n\n\n2.02***\n\n\n\n\n \n\n\n(0.32)\n\n\n(0.30)\n\n\n(0.30)\n\n\n(0.30)\n\n\n\n\nnodematch.fav_music\n\n\n \n\n\n0.85***\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.14)\n\n\n \n\n\n \n\n\n\n\nnodematch.fav_music.jazz\n\n\n \n\n\n \n\n\n0.74**\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.25)\n\n\n \n\n\n\n\nnodematch.fav_music.pop\n\n\n \n\n\n \n\n\n0.82***\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.18)\n\n\n \n\n\n\n\nnodematch.fav_music.rock\n\n\n \n\n\n \n\n\n0.87***\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.17)\n\n\n \n\n\n\n\nmix.fav_music.pop.jazz\n\n\n \n\n\n \n\n\n \n\n\n-0.54\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.34)\n\n\n\n\nmix.fav_music.rock.jazz\n\n\n \n\n\n \n\n\n \n\n\n-0.75*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.38)\n\n\n\n\nmix.fav_music.jazz.pop\n\n\n \n\n\n \n\n\n \n\n\n-0.60\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.35)\n\n\n\n\nmix.fav_music.pop.pop\n\n\n \n\n\n \n\n\n \n\n\n0.10\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.27)\n\n\n\n\nmix.fav_music.rock.pop\n\n\n \n\n\n \n\n\n \n\n\n-0.50\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.31)\n\n\n\n\nmix.fav_music.jazz.rock\n\n\n \n\n\n \n\n\n \n\n\n-1.40**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.44)\n\n\n\n\nmix.fav_music.pop.rock\n\n\n \n\n\n \n\n\n \n\n\n-0.86*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.33)\n\n\n\n\nmix.fav_music.rock.rock\n\n\n \n\n\n \n\n\n \n\n\n0.15\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.27)\n\n\n\n\nAIC\n\n\n2068.22\n\n\n2030.85\n\n\n2033.57\n\n\n2036.37\n\n\n\n\nBIC\n\n\n2082.62\n\n\n2052.45\n\n\n2069.57\n\n\n2108.38\n\n\n\n\nLog Likelihood\n\n\n-1032.11\n\n\n-1012.42\n\n\n-1011.79\n\n\n-1008.19\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nAlthough model 5 has a higher loglikelihood, using AIC or BIC suggests model 4 is a better candidate. For the sake of time, we will jump ahead and add nodematch(\"female\") as the last term of our model. The next step is to assess (a) convergence and (b) goodness-of-fit.\nmodel_final &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\") + nodematch(\"female\"))\n\n# Printing the pretty table\nknitreg(list(`2` = model_2, `4` = model_4, `Final` = model_final))\n\n\n\nStatistical models\n\n\n\n\n \n\n\n2\n\n\n4\n\n\nFinal\n\n\n\n\n\n\nedges\n\n\n-3.93***\n\n\n-4.29***\n\n\n-3.95***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.11)\n\n\n(0.12)\n\n\n\n\nmutual\n\n\n2.15***\n\n\n1.99***\n\n\n1.86***\n\n\n\n\n \n\n\n(0.32)\n\n\n(0.30)\n\n\n(0.33)\n\n\n\n\nnodematch.fav_music\n\n\n \n\n\n0.85***\n\n\n0.81***\n\n\n\n\n \n\n\n \n\n\n(0.14)\n\n\n(0.14)\n\n\n\n\nnodematch.female\n\n\n \n\n\n \n\n\n-0.74***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.15)\n\n\n\n\nAIC\n\n\n2068.22\n\n\n2030.85\n\n\n2002.18\n\n\n\n\nBIC\n\n\n2082.62\n\n\n2052.45\n\n\n2030.98\n\n\n\n\nLog Likelihood\n\n\n-1032.11\n\n\n-1012.42\n\n\n-997.09\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "06-new-topics/index.html",
    "href": "06-new-topics/index.html",
    "title": "New topics in network modeling",
    "section": "",
    "text": "With more data and computing resources, the things that we can ask and do with networks are becoming increasingly (even more) exciting and complex.\nIn this section, I will introduce some of the latest advancements and forthcoming topics in network modeling."
  },
  {
    "objectID": "06-new-topics/index.html#small-data",
    "href": "06-new-topics/index.html#small-data",
    "title": "New topics in network modeling",
    "section": "1 Small data",
    "text": "1 Small data\nPeople tend to think that big data is the next frontier, yet, in the last couple of years, small data, and particularly, small network data, has been amassing attention from the network science community.\nSamples of small network datasets are becoming increasingly common, which has two important implications: (a) the concept of statistical power is starting to enter in the ERGM community [] and (b)"
  },
  {
    "objectID": "06-new-topics/index.html#multiplexity",
    "href": "06-new-topics/index.html#multiplexity",
    "title": "New topics in network modeling",
    "section": "2 Multiplexity",
    "text": "2 Multiplexity\nMore and more data means more and more complexity. Management sciences and other social sciences have been focusing an important amount of resources to multiplexity. With multiplexity we can widen our view point and study human behavior in a more holistic way."
  },
  {
    "objectID": "06-new-topics/index.html#big-networks",
    "href": "06-new-topics/index.html#big-networks",
    "title": "New topics in network modeling",
    "section": "3 Big networks",
    "text": "3 Big networks\nYes, big data is still a thing in social network analysis. Which is why recent advancements of the ERGM framework have been focusing on the scalability of the model. A recent paper by [] implements a new algorithm for big ERGMs and illustrate its use with a network with over a million nodes; something currently unthinkable with the current implementations of the model."
  },
  {
    "objectID": "06-new-topics/index.html#relational-events",
    "href": "06-new-topics/index.html#relational-events",
    "title": "New topics in network modeling",
    "section": "4 Relational events",
    "text": "4 Relational events\nRicher time-stamped data will allow us diving deeper into the dynamics between."
  },
  {
    "objectID": "04-intro-to-ergms/index.html#footnotes",
    "href": "04-intro-to-ergms/index.html#footnotes",
    "title": "Introduction to ERGMs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile ERG Models can be used to predict individual ties (which is another way of describing them), the focus is on the processes that give origin to the network.↩︎"
  },
  {
    "objectID": "04-intro-to-ergms/index.html#inspect-the-data",
    "href": "04-intro-to-ergms/index.html#inspect-the-data",
    "title": "Introduction to ERGMs",
    "section": "4.1 Inspect the data",
    "text": "4.1 Inspect the data\nFor the sake of time, we will not take the time to investigate our network properly. However, you should always do so. Make sure you do descriptive statistics (density, centrality, modularity, etc.), check missing values, isolates (disconnected nodes), and inspect your data visually through “notepad” and visualizations before jumping into your ERG model."
  },
  {
    "objectID": "04-intro-to-ergms/index.html#start-with-endogenous-effects-first",
    "href": "04-intro-to-ergms/index.html#start-with-endogenous-effects-first",
    "title": "Introduction to ERGMs",
    "section": "4.2 Start with endogenous effects first",
    "text": "4.2 Start with endogenous effects first\nThe step is to check whether we can fit an ERGM or not. We can do so with the Bernoulli graph:\n\nmodel_1 &lt;- ergm(Y ~ edges)\nsummary(model_1)\n\nCall:\nergm(formula = Y ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -3.78885    0.06833      0  -55.45   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 13724  on 9900  degrees of freedom\n Residual Deviance:  2102  on 9899  degrees of freedom\n \nAIC: 2104  BIC: 2112  (Smaller is better. MC Std. Err. = 0)\n\n\nIt is rare to see a model in which the edgecount is not significant. The next term we will add is reciprocity (mutual in the ergm package)\n\nmodel_2 &lt;- ergm(Y ~ edges + mutual) \nsummary(model_2)\n## Call:\n## ergm(formula = Y ~ edges + mutual)\n## \n## Monte Carlo Maximum Likelihood Results:\n## \n##        Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \n## edges  -3.93277    0.07833      0 -50.205   &lt;1e-04 ***\n## mutual  2.15152    0.31514      0   6.827   &lt;1e-04 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 13724  on 9900  degrees of freedom\n##  Residual Deviance:  2064  on 9898  degrees of freedom\n##  \n## AIC: 2068  BIC: 2083  (Smaller is better. MC Std. Err. = 0.8083)\n\nAs expected, reciprocity is significant (we made it like this!.) Notwithstanding, there is a difference between this model and the previous one. This model was not fitted using MLE. Instead, since the reciprocity term involves more than one tie, the model cannot be reduced to a Logistic regression, so it needs to be estimated using one of the other available estimation methods in the ergm package.\nThe model starts gaining complexity as we add higher-order terms involving more ties. An infamous example is the number of triangles. Although highly important for social sciences, including triangle terms in your ERGMs results in a degenerate model (when the MCMC chain jumps between empty and fully connected graphs). One exception is if you deal with small networks. To address this, Snijders et al. (2006) and Hunter (2007) introduced various new terms that significantly reduce the risk of degeneracy. Here, we will illustrate the use of the term “geometrically weighted dyadic shared partner” (gwdsp,) which Prof. David Hunter proposed. The gwdsp term is akin to triadic closure but reduces the chances of degeneracy.\n\n# Fitting two more models (output message suppressed)\nmodel_3 &lt;- ergm(Y ~ edges + mutual + gwdsp(.5, fixed = TRUE))\n# model_4 &lt;- ergm(Y ~ edges + triangles) # bad idea\n\nRight after fitting a model, we want to inspect the results. An excellent tool for this is the R package texreg (Leifeld 2013):\nlibrary(texreg)\nknitreg(list(model_1, model_2, model_3))\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\n\n\n\nedges\n\n\n-3.79***\n\n\n-3.93***\n\n\n-3.84***\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.08)\n\n\n(0.21)\n\n\n\n\nmutual\n\n\n \n\n\n2.15***\n\n\n2.14***\n\n\n\n\n \n\n\n \n\n\n(0.32)\n\n\n(0.29)\n\n\n\n\ngwdsp.OTP.fixed.0.5\n\n\n \n\n\n \n\n\n-0.02\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.05)\n\n\n\n\nAIC\n\n\n2104.43\n\n\n2068.22\n\n\n2071.94\n\n\n\n\nBIC\n\n\n2111.63\n\n\n2082.62\n\n\n2093.54\n\n\n\n\nLog Likelihood\n\n\n-1051.22\n\n\n-1032.11\n\n\n-1032.97\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nSo far, model_2 is winning. We will continue with this model."
  },
  {
    "objectID": "04-intro-to-ergms/index.html#evaluate-your-results",
    "href": "04-intro-to-ergms/index.html#evaluate-your-results",
    "title": "Introduction to ERGMs",
    "section": "4.4 Evaluate your results",
    "text": "4.4 Evaluate your results\n\n4.4.1 Convergence\nAs our model was fitted using MCMC, we must ensure the chains converged. We can use the mcmc.diagnostics function from the ergm package to check model convergence. This function looks at the last set of simulations of the MCMC model and generates various diagnostics for the user.\nUnder the hood, the fitting algorithm generates a stream of networks based on those parameters for each new proposed set of model parameters. The last stream of networks is thus simulated using the final state of the model. The mcmc.diagnostics function takes that stream of networks and plots the sequence of the sufficient statistics included in the model. A converged model should show a stationary statistics sequence, moving around a fixed value without (a) becoming stuck at any point and (b) chaining the tendency. This model shows both:\n\nmcmc.diagnostics(model_final, which   = c(\"plots\"))\n\n\n\n\n\n\n\n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nNow that we know our model was good enough to represent the observed statistics (sample them, actually,) let’s see how good it is at capturing other features of the network that were not included in the model.\n\n\n4.4.2 Goodness-of-fit\nThis would be the last step in the sequence of steps to fit an ERGM. As we mentioned before, the idea of Goodness-of-fit [GOF] in ERG models is to see how well our model captures other properties of the graph that were not included in the model. By default, the gof function from the ergm package computes GOF for:\n\nThe model statistics.\nThe outdegree distribution.\nThe indegree distribution.\nThe distribution of edge-wise shared partners.\nThe distribution of the geodesic distances (shortest path).\n\nThe process of evaluating GOF is relatively straightforward. Using samples from the posterior distribution, we check whether the observed statistics from above are covered (fall within the CI) of our model. If they do, we say that the model has a good fit. Otherwise, if we observe significant anomalies, we return to the bench and try to improve our model.\nAs with all simulated data, our gof() call shows that our selected model was an excellent choice for the observed graph:\n\ngof_final &lt;- gof(model_final)\nprint(gof_final)\n## \n## Goodness-of-fit for in-degree \n## \n##           obs min  mean max MC p-value\n## idegree0   11   4 11.00  25       1.00\n## idegree1   22  16 23.86  33       0.74\n## idegree2   23  19 26.43  41       0.56\n## idegree3   30  11 20.47  33       0.08\n## idegree4   10   5 10.81  19       0.88\n## idegree5    3   1  4.66  12       0.70\n## idegree6    1   0  1.91   7       0.86\n## idegree7    0   0  0.60   4       1.00\n## idegree8    0   0  0.18   3       1.00\n## idegree9    0   0  0.06   1       1.00\n## idegree10   0   0  0.01   1       1.00\n## idegree12   0   0  0.01   1       1.00\n## \n## Goodness-of-fit for out-degree \n## \n##           obs min  mean max MC p-value\n## odegree0   10   4 10.75  19       0.90\n## odegree1   30  12 24.16  33       0.22\n## odegree2   23  18 26.76  40       0.52\n## odegree3   17  12 19.71  29       0.64\n## odegree4   10   5 11.05  19       0.80\n## odegree5    8   0  4.89  10       0.22\n## odegree6    2   0  1.96   6       1.00\n## odegree7    0   0  0.52   2       1.00\n## odegree8    0   0  0.15   2       1.00\n## odegree9    0   0  0.04   1       1.00\n## odegree11   0   0  0.01   1       1.00\n## \n## Goodness-of-fit for edgewise shared partner \n## \n##          obs min   mean max MC p-value\n## esp.OTP0 212 180 210.99 240       0.96\n## esp.OTP1   7   3  11.27  24       0.40\n## esp.OTP2   0   0   0.27   2       1.00\n## \n## Goodness-of-fit for minimum geodesic distance \n## \n##      obs  min    mean  max MC p-value\n## 1    219  184  222.53  264       0.96\n## 2    449  320  469.83  626       0.76\n## 3    790  507  863.71 1284       0.68\n## 4   1151  688 1277.86 2016       0.76\n## 5   1245  767 1409.86 2122       0.56\n## 6   1043  698 1163.97 1546       0.46\n## 7    745  347  771.20 1119       0.86\n## 8    434   87  442.32  810       1.00\n## 9    198   19  231.86  631       0.88\n## 10    80    2  115.88  435       0.92\n## 11    10    0   58.27  328       0.50\n## 12     1    0   29.23  204       0.54\n## 13     0    0   14.39  136       0.80\n## 14     0    0    6.85  104       1.00\n## 15     0    0    3.26   76       1.00\n## 16     0    0    1.51   48       1.00\n## 17     0    0    0.72   29       1.00\n## 18     0    0    0.34   18       1.00\n## 19     0    0    0.16   11       1.00\n## 20     0    0    0.08    8       1.00\n## 21     0    0    0.05    5       1.00\n## 22     0    0    0.02    2       1.00\n## 23     0    0    0.01    1       1.00\n## Inf 3535 1150 2816.09 5550       0.40\n## \n## Goodness-of-fit for model statistics \n## \n##                     obs min   mean max MC p-value\n## edges               219 184 222.53 264       0.96\n## mutual               16  10  16.88  26       0.88\n## nodematch.fav_music 123  90 124.93 165       0.90\n## nodematch.female     72  54  71.71  97       1.00\n\nIt is easier to see the results using the plot function:\n\n# Plotting the result (5 figures)\nop &lt;- par(mfrow = c(3,2))\nplot(gof_final)\npar(op)"
  },
  {
    "objectID": "05-advanced-ergms/index.html#example-1-interlocking-egos-and-disconnected-alters",
    "href": "05-advanced-ergms/index.html#example-1-interlocking-egos-and-disconnected-alters",
    "title": "Advanced ERGMs: Constraints",
    "section": "2.1 Example 1: Interlocking egos and disconnected alters",
    "text": "2.1 Example 1: Interlocking egos and disconnected alters\nImagine that we have two sets of vertices. The first, group E, are egos part of an egocentric study. The second group, called A, is composed of people mentioned by egos in E but were not surveyed. Assume that individuals in A can only connect to individuals in E; moreover, individuals in E have no restrictions on connecting. In other words, only two types of ties exist: E-E and A-E. The question is now, how can we enforce such a constraint in an ERGM?\nUsing offsets, and in particular, setting coefficients to -Inf provides an easy way to restrict the support set of ERGMs. For example, if we wanted to constrain the support to include networks with no triangles, we would add the term offset(triangle) and use the option offset.coef = -Inf to indicate that realizations including triangles are not possible. Using R:\nergm(net ~ edges + offset(triangle), offset.coef = -Inf)\nIn this model, a Bernoulli graph, we reduce the sample space to networks with no triangles. In our example, such a statistic should only take non-zero values whenever ties within the A class happen. We can use the nodematch() term to do that. Formally\n\n\\text{NodeMatch}(x) = \\sum_{i,j} y_{ij} \\mathbf{1}({x_{i} = x_{j}})\n\nThis statistic will sum over all ties in which source (i) and target (j)’s X attribute is equal. One way to make this happen is by creating an auxiliary variable that equals, e.g., 0 for all vertices in A, and a unique value different from zero otherwise. For example, if we had 2 As and three Es, the data would look something like this: \\{0,0,1,2,3\\}. The following code block creates an empty graph with 50 nodes, 10 of which are in group E (ego).\n\nlibrary(ergm, quietly =  TRUE)\nlibrary(sna, quietly =  TRUE)\n\nn &lt;- 50\nn_egos &lt;- 10\nnet &lt;- as.network(matrix(0, ncol = n, nrow = n), directed = TRUE)\n\n# Let's assing the groups\nnet %v% \"is.ego\" &lt;- c(rep(TRUE, n_egos), rep(FALSE, n - n_egos))\nnet %v% \"is.ego\"\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE\n\ngplot(net, vertex.col = net %v% \"is.ego\")\n\n\n\n\nTo create the auxiliary variable, we will use the following function:\n\n# Function that creates an aux variable for the ergm model\nmake_aux_var &lt;- function(my_net, is_ego_dummy) {\n  \n  n_vertex &lt;- length(my_net %v% is_ego_dummy)\n  n_ego_   &lt;- sum(my_net %v% is_ego_dummy)\n  \n  # Creating an auxiliary variable to identify the non-informant non-informant ties\n  my_net %v% \"aux_var\" &lt;- ifelse(\n    !my_net %v% is_ego_dummy, 0, 1:(n_vertex - n_ego_)\n    )\n\n  my_net\n}\n\nCalling the function in our data results in the following:\n\nnet &lt;- make_aux_var(net, \"is.ego\")\n\n# Taking a look over the first 15 rows of data\ncbind(\n  Is_Ego = net %v% \"is.ego\",\n  Aux    = net %v% \"aux_var\"  \n) |&gt; head(n = 15)\n\n      Is_Ego Aux\n [1,]      1   1\n [2,]      1   2\n [3,]      1   3\n [4,]      1   4\n [5,]      1   5\n [6,]      1   6\n [7,]      1   7\n [8,]      1   8\n [9,]      1   9\n[10,]      1  10\n[11,]      0   0\n[12,]      0   0\n[13,]      0   0\n[14,]      0   0\n[15,]      0   0\n\n\nWe can now use this data to simulate a network in which ties between A-class vertices are not possible:\n\nset.seed(2828)\nnet_sim &lt;- simulate(net ~ edges + nodematch(\"aux_var\"), coef = c(-3.0, -Inf))\ngplot(net_sim, vertex.col = net_sim %v% \"is.ego\")\n\n\n\n\nAs you can see, this network has only ties of the type E-E and A-E. We can double-check by (i) looking at the counts and (ii) visualizing each induced-subgraph separately:\n\nsummary(net_sim ~ edges + nodematch(\"aux_var\"))\n\n            edges nodematch.aux_var \n               49                 0 \n\nnet_of_alters &lt;- get.inducedSubgraph(\n  net_sim, which((net_sim %v% \"aux_var\") == 0)\n  )\n\nnet_of_egos &lt;- get.inducedSubgraph(\n  net_sim, which((net_sim %v% \"aux_var\") != 0)\n  )\n\n# Counts\nsummary(net_of_alters ~ edges + nodematch(\"aux_var\"))\n\n            edges nodematch.aux_var \n                0                 0 \n\nsummary(net_of_egos ~ edges + nodematch(\"aux_var\"))\n\n            edges nodematch.aux_var \n                1                 0 \n\n# Figures\nop &lt;- par(mfcol = c(1, 2))\ngplot(net_of_alters, vertex.col = net_of_alters %v% \"is.ego\", main = \"A\")\ngplot(net_of_egos, vertex.col = net_of_egos %v% \"is.ego\", main = \"E\")\n\n\n\npar(op)\n\nNow, to fit an ERGM with this constraint, we simply need to make use of the offset terms. Here is an example:\n\nans &lt;- ergm(\n  net_sim ~ edges + offset(nodematch(\"aux_var\")), # The model (notice the offset)\n  offset.coef = -Inf                              # The offset coefficient\n  )\n## Starting maximum pseudolikelihood estimation (MPLE):\n## Obtaining the responsible dyads.\n## Evaluating the predictor and response matrix.\n## Maximizing the pseudolikelihood.\n## Finished MPLE.\n## Evaluating log-likelihood at the estimate.\nsummary(ans)\n## Call:\n## ergm(formula = net_sim ~ edges + offset(nodematch(\"aux_var\")), \n##     offset.coef = -Inf)\n## \n## Maximum Likelihood Results:\n## \n##                           Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \n## edges                       -2.843      0.147      0  -19.34   &lt;1e-04 ***\n## offset(nodematch.aux_var)     -Inf      0.000      0    -Inf   &lt;1e-04 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 1233.8  on 890  degrees of freedom\n##  Residual Deviance:  379.4  on 888  degrees of freedom\n##  \n## AIC: 381.4  BIC: 386.2  (Smaller is better. MC Std. Err. = 0)\n## \n##  The following terms are fixed by offset and are not estimated:\n##   offset(nodematch.aux_var)\n\nThis ERGM model–which by the way only featured dyadic-independent terms, and thus can be reduced to a logistic regression–restricts the support by excluding all networks in which ties within the class A exists. To finalize, let’s look at a few simulations based on this model:\n\nset.seed(1323)\nop &lt;- par(mfcol = c(2,2), mar = rep(1, 4))\nfor (i in 1:4) {\n  gplot(simulate(ans), vertex.col = net %v% \"is.ego\", vertex.cex = 2)\n  box()\n}\n\n\n\npar(op)\n\nAll networks with no ties between A nodes."
  },
  {
    "objectID": "05-advanced-ergms/index.html#example-2-bi-partite-networks",
    "href": "05-advanced-ergms/index.html#example-2-bi-partite-networks",
    "title": "Advanced ERGMs: Constraints",
    "section": "2.2 Example 2: Bi-partite networks",
    "text": "2.2 Example 2: Bi-partite networks\nIn the case of bipartite networks (sometimes called affiliation networks,) we can use ergm’s terms for bipartite graphs to corroborate what we discussed here. For example, the two-star term. Let’s start simulating a bipartite network using the edges and two-star parameters. Since the k-star term is usually complex to fit (tends to generate degenerate models,) we will take advantage of the Log() transformation function in the ergm package to smooth the term.1\nThe bipartite network that we will be simulating will have 100 actors and 50 entities. Actors, which we will map to the first level of the ergm terms, this is, b1star b1nodematch, etc. will send ties to the entities, the second level of the bipartite ERGM. To create a bipartite network, we will create an empty matrix of size nactors x nentitites; thus, actors are represented by rows and entities by columns.\n\n# Parameters for the simulation\nnactors   &lt;- 100\nnentities &lt;- floor(nactors/2)\nn         &lt;- nactors + nentities\n\n# Creating an empty bipartite network (baseline)\nnet_b &lt;- network(\n  matrix(0, nrow = nactors, ncol = nentities), bipartite = TRUE\n)\n\n# Simulating the bipartite ERGM,\nnet_b &lt;- simulate(net_b ~ edges + Log(~b1star(2)), coef = c(-3, 1.5), seed = 55)\n\nLet’s see what we got here:\n\nsummary(net_b ~ edges + Log(~b1star(2)))\n\n      edges Log~b1star2 \n 245.000000    5.746203 \n\nnetplot::nplot(net_b, vertex.col = (1:n &lt;= nactors) + 1)\n\n\n\n\nNotice that the first nactors vertices in the network are the actors, and the remaining are the entities. Now, although the ergm package features bipartite network terms, we can still fit a bipartite ERGM without explicitly declaring the graph as such. In such case, the b1star(2) term of a bipartite network is equivalent to an ostar(2) in a directed graph. Likewise, b2star(2) in a bipartite graph matches the istar(2) term in a directed graph. This information will be relevant when fitting the ERGM. Let’s transform the bipartite network into a directed graph. The following code block does so:\n\n# Identifying the edges\nnet_not_b &lt;- which(as.matrix(net_b) != 0, arr.ind = TRUE)\n\n# We need to offset the endpoint of the ties by nactors\n# so that the ids go from 1 through (nactors + nentitites)\nnet_not_b[,2] &lt;- net_not_b[,2] + nactors\n\n# The resulting graph is a directed network\nnet_not_b &lt;- network(net_not_b, directed = TRUE)\n\nNow we are almost done. As before, we need to use node-level covariates to put the constraints in our model. For this ERGM to reflect an ERGM on a bipartite network, we need two constraints:\n\nOnly ties from actors to entities are allowed, and\nentities can only receive ties.\n\nThe corresponding offset terms for this model are: nodematch(\"is.actor\") ~ -Inf, and nodeocov(\"isnot.actor\") ~ -Inf. Mathematically:\n\\begin{align*}\n\\text{NodeMatch(x = \"is.actor\")} &= \\sum_{i&lt;j} y_{ij}\\mathbb{1}\\left(x_i = x_j\\right) \\\\\n\\text{NodeOCov(x = \"isnot.actor\")} &= \\sum_{i} x_i \\times \\sum_{j&lt;i} y_{ij}\n\\end{align*}\nIn other words, we are setting that ties between nodes of the same class are forbidden, and outgoing ties are forbidden for entities. Let’s create the vertex attributes needed to use the aforementioned terms:\n\nnet_not_b %v% \"is.actor\" &lt;- as.integer(1:n &lt;= nactors)\nnet_not_b %v% \"isnot.actor\" &lt;- as.integer(1:n &gt; nactors)\n\nFinally, to make sure we have done all well, let’s look how both networks–bipartite and unimodal–look side by side:\n\n# First, let's get the layout\nfig &lt;- netplot::nplot(net_b, vertex.col = (1:n &lt;= nactors) + 1)\ngridExtra::grid.arrange(\n  fig,\n  netplot::nplot(\n    net_not_b, vertex.col = (1:n &lt;= nactors) + 1,\n    layout = fig$.layout\n     ),\n  ncol = 2, nrow = 1\n)\n\n\n\n# Looking at the counts\nsummary(net_b ~ edges + b1star(2) + b2star(2))\n\n  edges b1star2 b2star2 \n    245     313     645 \n\nsummary(net_not_b ~ edges + ostar(2) + istar(2))\n\n edges ostar2 istar2 \n   245    313    645 \n\n\nWith the two networks matching, we can now fit the ERGMs with and without offset terms and compare the results between the two models:\n\n# ERGM with a bipartite graph\nres_b     &lt;- ergm(\n  # Main formula\n  net_b ~ edges + Log(~b1star(2)),\n\n  # Control parameters\n  control = control.ergm(seed = 1)\n  )\n\n# ERGM with a digraph with constraints\nres_not_b &lt;- ergm(\n  # Main formula\n  net_not_b ~ edges + Log(~ostar(2)) +\n\n  # Offset terms \n  offset(nodematch(\"is.actor\")) + offset(nodeocov(\"isnot.actor\")),\n  offset.coef = c(-Inf, -Inf),\n\n  # Control parameters\n  control = control.ergm(seed = 1)\n  )\n\nHere are the estimates (using the texreg R package for a prettier output):\n\ntexreg::screenreg(list(Bipartite = res_b, Directed = res_not_b))\n\n\n======================================================================\n                              Bipartite    Directed                   \n----------------------------------------------------------------------\nedges                           -3.14 ***                    -3.11 ***\n                                (0.15)                       (0.14)   \nLog~b1star2                     21.89                                 \n                               (17.13)                                \nLog~ostar2                                                   19.66    \n                                                            (16.75)   \noffset(nodematch.is.actor)                                    -Inf    \n                                                                      \noffset(nodeocov.isnot.actor)                                  -Inf    \n                                                                      \n----------------------------------------------------------------------\nAIC                           1958.00      -2134192392498170112.00    \nBIC                           1971.03      -2134192392498170112.00    \nLog Likelihood                -977.00       1067096196249085056.00    \n======================================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nAs expected, both models yield the “same” estimate. The minor differences observed between the models are how the ergm package performs the sampling. In particular, in the bipartite case, ergm has special routines for making the sampling more efficient, having a higher acceptance rate than that of the model in which the bipartite graph was not explicitly declared. We can tell this by inspecting rejection rates:\n\ndata.frame(\n  Bipartite = coda::rejectionRate(res_b$sample[[1]]) * 100,\n  Directed  = coda::rejectionRate(res_not_b$sample[[1]][, -c(3,4)]) * 100\n) |&gt; knitr::kable(digits = 2, caption = \"Rejection rate (percent)\")\n\n\nRejection rate (percent)\n\n\n\nBipartite\nDirected\n\n\n\n\nedges\n2.48\n3.67\n\n\nLog~b1star2\n1.24\n2.04\n\n\n\n\n\nThe ERGM fitted with the offset terms has a much higher rejection rate than that of the ERGM fitted with the bipartite ERGM.\nFinally, the fact that we can fit ERGMs using offset does not mean that we need to use it ALL the time. Unless there is a very good reason to go around ergm’s capabilities, I wouldn’t recommend fitting bipartite ERGMs as we just did, as the authors of the package have included (MANY) features to make our job easier."
  },
  {
    "objectID": "05-advanced-ergms/index.html#footnotes",
    "href": "05-advanced-ergms/index.html#footnotes",
    "title": "Advanced ERGMs: Constraints",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to Laura Koehly, who devised this complicated model.↩︎\nAfter writing this example, it became apparent the use of the Log() transformation function may not be ideal. Since many terms used in ERGMs can be zero, e.g., triangles, the term Log(~ ostar(2)) is undefined when ostar(2) = 0. In practice, the ERGM package sets a lower limit for the log of 0, so, instead of having Log(0) ~ -Inf, they set it to be a really large negative number. This causes all sorts of issues to the estimates; in our example, an overestimation of the population parameter and a positive log-likelihood. Therefore, I wouldn’t recommend using this transformation too often.↩︎"
  },
  {
    "objectID": "05-advanced-ergms/index.html",
    "href": "05-advanced-ergms/index.html",
    "title": "Advanced ERGMs: Constraints",
    "section": "",
    "text": "For this section, we will dive in into ERGM constranints. Using constraints, you will be able to modify the sampling space of the model to things such as:\n\nPool (multilevel) models.\nAccounting for data generating process.\nMake your model behave (with caution.)\nEven fit Discrete-Exponential Family Models (DEFM.)\n\nWe will start with formally understanding what constraining the space means and then continue with some examples."
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-siena",
    "href": "03-behavior-and-coevolution/index.html#code-example-siena",
    "title": "Behavior and coevolution",
    "section": "5.1 Code example: Siena",
    "text": "5.1 Code example: Siena\nThis example was adapted from the RSiena R package (see ?sienaGOF-auxiliary page).We start by loading the package and taking a look at the data we will use:\n\nlibrary(RSiena)\n\n# Visualizing the adjacency matrix & behavior\nop &lt;- par(mfrow=c(2, 2))\nimage(s501, title = \"Net: s501\")\n\nWarning in plot.window(...): \"title\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"title\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"title\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"title\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"title\" is not a graphical parameter\n\n\nWarning in title(...): \"title\" is not a graphical parameter\n\nimage(s502, title = \"Net: s502\")\n\nWarning in plot.window(...): \"title\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"title\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"title\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"title\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"title\" is not a graphical parameter\n\n\nWarning in title(...): \"title\" is not a graphical parameter\n\nhist(s50a[,1])\nhist(s50a[,2])\n\n\n\npar(op)\n\nThe next step is the data preparation process. RSiena does not receive raw data as is. We need to explicitly declare the networks and outcome variable. Siena models can also model network changes\n\n# Initializing the dependent variable (network)\nmynet1 &lt;- sienaDependent(array(c(s501, s502), dim=c(50, 50, 2)))\nmynet1\n\nType         oneMode             \nObservations 2                   \nNodeset      Actors (50 elements)\n\nmybeh  &lt;- sienaDependent(s50a[,1:2], type=\"behavior\")\nmybeh\n\nType         behavior            \nObservations 2                   \nNodeset      Actors (50 elements)\n\n# Node-level covariates (artificial)\nmycov  &lt;- c(rep(1:3,16),1,2)\n\n# Edge-level covariates (also artificial)\nmydycov &lt;- matrix(rep(1:5, 500), 50, 50) \n\n\nmydata &lt;- sienaDataCreate(mynet1, mybeh)\nmyeff &lt;- getEffects(mydata)\nmyeff &lt;- includeEffects(myeff, transTies, cycle3)\n\n  effectName      include fix   test  initialValue parm\n1 3-cycles        TRUE    FALSE FALSE          0   0   \n2 transitive ties TRUE    FALSE FALSE          0   0   \n\n\n\n# Shorter phases 2 and 3, just for example:\nmyalgorithm &lt;- sienaAlgorithmCreate(nsub=1, n3=50, seed=122, projname=NULL)\n\nIf you use this algorithm object, siena07 will create/use an output file Siena.txt .\n\n(ans &lt;- siena07(myalgorithm, data=mydata, effects=myeff, returnDeps=TRUE,\n   batch=TRUE))\n\n\nStart phase 0 \ntheta:  4.696 -1.489  0.000  0.000  0.000  0.706  0.322  0.000 \n\nStart phase 1 \nPhase 1 Iteration 1 Progress: 0%\nPhase 1 Iteration 2 Progress: 0%\nPhase 1 Iteration 3 Progress: 0%\nPhase 1 Iteration 4 Progress: 1%\nPhase 1 Iteration 5 Progress: 1%\nPhase 1 Iteration 10 Progress: 1%\nPhase 1 Iteration 15 Progress: 2%\nPhase 1 Iteration 20 Progress: 3%\nPhase 1 Iteration 25 Progress: 3%\nPhase 1 Iteration 30 Progress: 4%\nPhase 1 Iteration 35 Progress: 5%\nPhase 1 Iteration 40 Progress: 5%\nPhase 1 Iteration 45 Progress: 6%\nPhase 1 Iteration 50 Progress: 7%\ntheta:  5.626 -1.765  0.513  0.244  0.295  0.401  0.277  0.102 \n\nStart phase 2.1\nPhase 2 Subphase 1 Iteration 1 Progress: 61%\nPhase 2 Subphase 1 Iteration 2 Progress: 61%\ntheta  6.222 -1.910  0.847  0.400  0.589  0.383  0.139  0.150 \nac  0.527  0.050  2.786  6.110  5.883  1.014  0.922 -2.652 \nPhase 2 Subphase 1 Iteration 3 Progress: 61%\nPhase 2 Subphase 1 Iteration 4 Progress: 62%\ntheta  6.2239 -2.1219  1.2821  0.0671  0.6807  0.8771  0.2755  0.1887 \nac  0.992  1.239  1.230 -3.756  2.550  1.108  1.351  0.193 \nPhase 2 Subphase 1 Iteration 5 Progress: 62%\nPhase 2 Subphase 1 Iteration 6 Progress: 62%\ntheta  6.422 -2.392  1.720  0.214  0.799  1.117  0.269  0.136 \nac  0.910  1.271  1.301 -3.605  1.196  1.118  1.252  0.303 \nPhase 2 Subphase 1 Iteration 7 Progress: 62%\nPhase 2 Subphase 1 Iteration 8 Progress: 62%\ntheta  6.6477 -2.5994  1.9545  0.2903  0.9594  1.1115  0.3731  0.0797 \nac  0.743  1.274  1.310 -3.464  0.987  0.953  1.292  0.470 \nPhase 2 Subphase 1 Iteration 9 Progress: 62%\nPhase 2 Subphase 1 Iteration 10 Progress: 62%\ntheta  6.5262 -2.7189  2.0723  0.4106  1.0502  1.1184  0.3314 -0.0143 \nac  0.7050  1.2030  0.5857 -2.1894  0.0142  0.9217  1.1864  0.2257 \ntheta  6.6463 -2.6155  1.9720  0.2869  0.8550  1.2018  0.3186 -0.0498 \nac  0.17818 -0.61049 -0.69343 -0.61515 -0.73885  0.00491 -0.31594  0.01206 \ntheta:  6.6463 -2.6155  1.9720  0.2869  0.8550  1.2018  0.3186 -0.0498 \n\nStart phase 3 \n\n\nEstimates, standard errors and convergence t-ratios\n\n                                      Estimate   Standard   Convergence \n                                                   Error      t-ratio   \nNetwork Dynamics \n  1. rate basic rate parameter mynet1  6.6463  ( 3.6467   )   -0.0572   \n  2. eval outdegree (density)         -2.6155  ( 0.2693   )   -0.4537   \n  3. eval reciprocity                  1.9720  ( 0.2606   )   -0.4184   \n  4. eval 3-cycles                     0.2869  ( 0.3186   )   -0.5529   \n  5. eval transitive ties              0.8550  ( 0.2896   )   -0.3299   \n\nBehavior Dynamics\n  6. rate rate mybeh period 1          1.2018  ( 0.3102   )    0.0240   \n  7. eval mybeh linear shape           0.3186  ( 0.1805   )   -0.2794   \n  8. eval mybeh quadratic shape       -0.0498  ( 0.5535   )   -0.2177   \n\nOverall maximum convergence ratio:    0.8551 \n\n\nTotal of 337 iteration steps."
  },
  {
    "objectID": "06-new-topics/index.html#overview",
    "href": "06-new-topics/index.html#overview",
    "title": "New topics in network modeling",
    "section": "",
    "text": "With more data and computing resources, the things that we can ask and do with networks are becoming increasingly (even more) exciting and complex.\nIn this section, I will introduce some of the latest advancements and forthcoming topics in network modeling."
  },
  {
    "objectID": "06-new-topics/index.html#mutli-ergms",
    "href": "06-new-topics/index.html#mutli-ergms",
    "title": "New topics in network modeling",
    "section": "1.1 Mutli-ERGMs",
    "text": "1.1 Mutli-ERGMs\n\n\n\nIn Krivitsky, Coletti, and Hens (2023a), the authors present a start-to-finish pooled ERGM example featuring heterogeneous data sources.\nThey increase power and allow exploring heterogeneous effects across types/classes of networks."
  },
  {
    "objectID": "06-new-topics/index.html#statistical-power-of-soam",
    "href": "06-new-topics/index.html#statistical-power-of-soam",
    "title": "New topics in network modeling",
    "section": "1.2 Statistical power of SOAM",
    "text": "1.2 Statistical power of SOAM\n\n\nSOAM Stadtfeld et al. (2020) proposes ways to perform power analysis for Siena models. At the center of their six-step approach is simulation."
  },
  {
    "objectID": "06-new-topics/index.html#bayesian-alaam",
    "href": "06-new-topics/index.html#bayesian-alaam",
    "title": "New topics in network modeling",
    "section": "1.3 Bayesian ALAAM",
    "text": "1.3 Bayesian ALAAM\nEver wondered how to model influence exclusively?\n\n\n\nThe Auto-Logistic Actor Attribute Model [ALAAM] is a model that allows us to do just that.\nKoskinen and Daraganova (2022) extends the ALAAM model to a Bayesian framework.\n\n\n\nIt provides greater flexibility to accommodate more complicated models and add extensions such as hierarchical models."
  },
  {
    "objectID": "06-new-topics/index.html#relational-event-models",
    "href": "06-new-topics/index.html#relational-event-models",
    "title": "New topics in network modeling",
    "section": "1.4 Relational Event Models",
    "text": "1.4 Relational Event Models\n\nREMs are great for modeling sequences of ties (instead of panel or cross-sectional.)\nButts et al. (2023) provides a general overview of Relational Event Models [REMs,] new methods, and future steps.\n\n\n\n\nFigure 3 reproduced from Brandenberger (2020)"
  },
  {
    "objectID": "06-new-topics/index.html#big-ergms",
    "href": "06-new-topics/index.html#big-ergms",
    "title": "New topics in network modeling",
    "section": "1.5 Big ERGMs",
    "text": "1.5 Big ERGMs\n\n\n\nERGMs In A. Stivala, Robins, and Lomi (2020), a new method is proposed to estimate large ERGMs (featuring millions of nodes).\n\n\n\n\n\nPartial map of the Internet based on the January 15, 2005 data found on opte.org. – Wiki"
  },
  {
    "objectID": "06-new-topics/index.html#exponential-random-network-models",
    "href": "06-new-topics/index.html#exponential-random-network-models",
    "title": "New topics in network modeling",
    "section": "1.6 Exponential Random Network Models",
    "text": "1.6 Exponential Random Network Models\n\nWang, Fellows, and Handcock recently published a re-introduction of the ERNM framework (Wang, Fellows, and Handcock 2023).\nERNMs generalize ERGMs to incorporate behavior and are the cross-sectional causing of SIENA models.\n\n\\begin{align*}\n\\text{ER\\textbf{G}M}: & P_{\\mathcal{Y}, \\bm{\\theta}}(\\bm{Y}=\\bm{y} | \\bm{X}=\\bm{x}) \\\\\n\\text{ER\\textbf{N}M}: & P_{\\mathcal{Y}, \\bm{\\theta}}(\\bm{Y}=\\bm{y}, \\bm{X}=\\bm{x})\n\\end{align*}"
  },
  {
    "objectID": "06-new-topics/index.html#ergmitos-small-ergms",
    "href": "06-new-topics/index.html#ergmitos-small-ergms",
    "title": "New topics in network modeling",
    "section": "2.1 ERGMitos: Small ERGMs",
    "text": "2.1 ERGMitos: Small ERGMs\n\nERGMitos1 (Vega Yon, Slaughter, and Haye 2021) leverage small network sizes to use exact statistics.\n\n\n\n\nFive small networks from the ergmito R package"
  },
  {
    "objectID": "06-new-topics/index.html#discrete-exponential-family-models",
    "href": "06-new-topics/index.html#discrete-exponential-family-models",
    "title": "New topics in network modeling",
    "section": "2.2 Discrete Exponential-family Models",
    "text": "2.2 Discrete Exponential-family Models\n\n\n\nERGMs are a particular case of Random Markov fields.\nWe can use the ERGM framework for modeling vectors of binary outcomes, e.g., the consumption of \\{tobacco, MJ, alcohol\\}"
  },
  {
    "objectID": "06-new-topics/index.html#power-analysis-in-ergms",
    "href": "06-new-topics/index.html#power-analysis-in-ergms",
    "title": "New topics in network modeling",
    "section": "2.3 Power analysis in ERGMs",
    "text": "2.3 Power analysis in ERGMs\n\nUsing conditional ERGMs (closely related to constrained), we can do power analysis for network samples (Vega Yon 2023).\n\n\n\n\nReproduced from Krivitsky, Coletti, and Hens (2023b)"
  },
  {
    "objectID": "06-new-topics/index.html#two-step-estimation-ergms",
    "href": "06-new-topics/index.html#two-step-estimation-ergms",
    "title": "New topics in network modeling",
    "section": "2.4 Two-step estimation ERGMs",
    "text": "2.4 Two-step estimation ERGMs\n\n\nConditioning the ERGM on an observed statistic “drops” the associated coefficient.\nHypothesis: As n increases, conditional ERGM estimates are consistent with the full model:\n\n\n\n\n\nSimulation study trying to demonstrate the concept (Work in progress)"
  },
  {
    "objectID": "06-new-topics/index.html#thanks",
    "href": "06-new-topics/index.html#thanks",
    "title": "New topics in network modeling",
    "section": "2.5 Thanks!",
    "text": "2.5 Thanks!"
  },
  {
    "objectID": "06-new-topics/index.html#bonus-track-why-network-scientists-dont-use-ergms",
    "href": "06-new-topics/index.html#bonus-track-why-network-scientists-dont-use-ergms",
    "title": "New topics in network modeling",
    "section": "2.6 Bonus track: Why network scientists don’t use ERGMs?",
    "text": "2.6 Bonus track: Why network scientists don’t use ERGMs?\n\nAttempts to overcome these problems by extending the blockmodel have focused particularly on the use of (more complicated) p∗ or exponential random graph models, but while these are conceptually appealing, they quickly lose the analytic tractability of the original blockmodel as their complexity increases.\n– Karrer and Newman (2011)"
  },
  {
    "objectID": "06-new-topics/index.html#footnotes",
    "href": "06-new-topics/index.html#footnotes",
    "title": "New topics in network modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom the Spanish suffix meaning small.↩︎"
  },
  {
    "objectID": "03-behavior-and-coevolution/index.html#code-example-ernm",
    "href": "03-behavior-and-coevolution/index.html#code-example-ernm",
    "title": "Behavior and coevolution",
    "section": "5.2 Code example: ERNM",
    "text": "5.2 Code example: ERNM"
  }
]