---
title: Introduction to ERGMs
references: ../references.bib
---

\newcommand{\tpose}[1]{#1^{\mathbf{t}}}
\newcommand{\Prergm}[0]{P_{\mathcal{Y}, \bm{\theta}}}
\newcommand{\Y}[0]{\bm{Y}}
\newcommand{\y}[0]{\bm{y}}
\newcommand{\thetas}[0]{\bm{\theta}}
\newcommand{\isone}[1]{\mathbb{1}\left(#1\right)}


Exponential-Family Random Graph Models, known as ERGMs or ERG Models, are a large family of statistical distributions for random graphs. In ERGMs, the focus is on the processes that give origin to the network rather than the individual ties. 

The most common representation of ERGMs is the following:

$$
\Prergm(\bm{Y}=\bm{y}) =  \exp\left(\tpose{\bm{\theta}} s(y)\right)\kappa(\bm{\theta})^{-1}
$$

where $\bm{Y}$ is a random graph, $\bm{y}\in \mathcal{Y}$ is a particular realization of $Y$, $\bm{\theta}$ is a vector of parameters, $s(\bm{y})$ is a vector of statistics, and $\kappa(\bm{\theta})$ is a normalizing constant. The normalizing constant is defined as:

$$
\kappa(\bm{\theta}) = \sum_{\bm{y} \in \mathcal{Y}} \exp\left(\tpose{\bm{\theta}} s(\bm{y})\right)
$$

From the statistical point of view, the normalizing constant makes this model attractive; only cases where $\mathcal{Y}$ is small enough to be enumerated are feasible [@vegayonExponentialRandomGraph2021]. Because of that reason, estimating ERGMs is a challenging task.

# Bernoulli ERGMs

The simplest ERG model we can fit is a Bernoulli (Erdos-Renyi) model. Here, the only statistic is the edgecount. Now, if we can write the function computing sufficient statistics as a sum over all edges, *i.e.*, $s(\y) = \sum_{ij}s'(y_{ij})$, the model reduces to a Logistic regression. 

```{r}
#| label: bernoulli-ergm
#| message: false
#| warning: false
#| collapse: true
library(ergm)

# Simulating a Bernoulli network
Y <- matrix(rbinom(100^2, 1, 0.1), 100, 100)
diag(Y) <- NA

cbind(
  logit = glm(as.vector(Y) ~ 1, family = binomial) |> coef(),
  ergm  = ergm(Y ~ edges) |> coef(),
  avg   = mean(Y, na.rm = TRUE) |> qlogis()
)
```

# No Markov terms

Imagine that we are interested in assessing whether gender homophily (the tendency between individuals of the same gender to connect) is present in a network. ERG models are the right tool for this task. Moreover, if we assume that gender homophily is the only mechanism that governs the network, the problem reduces to a Logistic regression:

$$
\Prergm(y_{ij} = 1) = \text{Logit}^{-1}\left(\theta_{edges} + \theta_{homophily}\isone{X_i = X_j}\right)
$$

where $\isone{\cdot}$ is the indicator function, and $X_i$ equals one if the $ith$ individual is female, and zero otherwise. To see this, let's simulate some data

```{r}
#| label: simple-example
#| message: false
#| warning: false
# Simulating data
set.seed(7731)
X <- rbinom(100, 1, 0.5)

# Simulating the network with homophily
library(ergm)
Y <- network(100)
Y %v% "X" <- X
Y <- ergm::simulate_formula(Y ~ edges + nodematch("X"), coef = c(-3, 2))
```

```{r}
#| label: simple-example-plot
#| message: false
#| cache: true
# Plotting the network
library(netplot)
np <- nplot(Y, vertex.color = ~X)
```

Using the ERGM package, we can fit the model using code very similar to the one used to simulate the network:

```{r}
#| label: simple-example-fit
#| message: false
#| warning: false
fit_homophily <- ergm(Y ~ edges + nodematch("X"))
```

We will check the results later and compare them against the following: the Logit model.

```{r}
#| label: simple-example-fit-glm
#| message: false
n <- 100
sstats <- summary_formula(Y ~ edges + nodematch("X"))
Y_mat <- as.matrix(Y)
diag(Y_mat) <- NA

# To speedup computations, we pre-compute this value
X_vec <- outer(X, X, "==") |> as.numeric()

# Function to compute the negative loglikelihood
obj <- \(theta) {

  # Compute the probability according to the value of Y
  p <- ifelse(
    Y_mat == 1,
    plogis(theta[1] + X_vec * theta[2]),
    1 - plogis(theta[1] + X_vec * theta[2])
  )

  # The -loglikelihood
  -sum(log(p[!is.na(p)]))
  

}

# Fitting the model
fit_homophily_logit <- optim(c(0,0), obj, hessian = TRUE)
```

Now that we have the values let's compare them:

```{r}
#| label: simple-example-fit-glm-coef
#| message: false
# The coefficients
rbind(
  ergm = coef(fit_homophily),
  logit = fit_homophily_logit$par
)

# The standard errors
rbind(
  ergm = vcov(fit_homophily) |> diag() |> sqrt(),
  logit = sqrt(diag(solve(fit_homophily_logit$hessian)))
)

```

# The most important results

If we were able to say what two of the most important results in ERGMs are, I would say the following: (a) conditioning on the rest of the graph, the probability of a tie distributes Logistic (Bernoulli), and (b) the ratio between two loglikelihoods can be approximated through simulation.

## The logistic distribution

Let's start by stating the result: Conditioning on all graphs that are not $y_{ij}$, the probability of a tie $Y_{ij}$ is distributed Logistic; formally:

\newcommand{\chng}[0]{\delta_{ij}(\bm{y})}

$$
\Prergm(Y_{ij}=1 | \bm{y}_{-ij}) = \frac{1}{1 + \exp \left(\tpose{\bm{\theta}}\chng{}\right)},
$$

where $\chng{}\equiv s_{ij}^+(\bm{y}) - s_{ij}^-(\bm{y})$ is the change statistic, and $s_{ij}^+(\bm{y})$ and $s_{ij}^-(\bm{y})$ are the statistics of the graph with and without the tie $Y_{ij}$, respectively.

The importance of this result is two-fold: (a) we can use this equation to interpret fitted models in the context of a single graph (like using odds,) and (b) we can use this equation to simulate from the model, **without touching the normalizing constant**. 

## The ratio of loglikelihoods

The second significant result is that the ratio of loglikelihoods can be approximated through simulation. It is based on the following observation by @geyerConstrainedMonteCarlo1992:

$$
\frac{\kappa(\bm{\theta})}{\kappa(\bm{\theta}_0)} = \mathbb{E}_{\mathcal{Y}, \bm{\theta}_0}\left(\tpose{(\bm{\theta} - \bm{\theta}_0)s(\bm{y})}\right),
$$

Using the latter, we can approximate the following loglikelihood ratio:

\begin{align*}
l(\bm{\theta}) - l(\bm{\theta}_0) = & \tpose{(\bm{\theta} - \bm{\theta}_0)}s(\bm{y}) - \log\left[\frac{\kappa(\bm{\theta})}{\kappa(\bm{\theta}_0)}\right]\\
\approx & \tpose{(\bm{\theta} - \bm{\theta}_0)}s(\bm{y}) - \log\left[M^{-1}\sum_{\bm{y}^{(m)}} \tpose{(\bm{\theta} - \bm{\theta}_0)}s(\bm{y}^{(m)})\right]
\end{align*}


Where $\bm{\theta}_0$ is an arbitrary vector of parameters, and $\bm{y}^{(m)}$ are sampled from the distribution $P_{\mathcal{Y}, \bm{\theta}_0}$. In the words of @geyerConstrainedMonteCarlo1992, "[...] it is possible to approximate $\bm{\theta}$ by using simulations from one distribution $P_{\mathcal{Y}, \bm{\theta}_0}$ no matter which $\bm{\theta}_0$ in the parameter space is."

# Start to finish example

There is no silver bullet to fit ERGMs. However, the following steps are a good starting point:

1. **Inspect the data**: Ensure the network you work with is processed correctly. Some common mistakes include isolates being excluded, vertex attributes not being properly aligned (mismatch), etc.

2. **Start with endogenous effects first**: Before jumping into vertex/edge-covariates, try fitting models that only include structural terms such as edgecount, triangles (or their equivalent,) stars, reciprocity, etc.

3. **After structure is controlled**: You can add vertex/edge-covariates. The most common ones are homophily, nodal-activity, etc.

Although we could go ahead and use an existing dataset to play with, instead, we will simulate a directed graph with the following properties:

- 100 nodes.
- Homophily on Music taste.
- Gender heterophily.
- Reciprocity

```{r}
#| label: generating-data
#| message: false
# Simulating the covariates (vertex attribute)
set.seed(1235)

# Simulating the data
Y <- network(100, directed = TRUE)
Y %v% "fav_music" <- sample(c("rock", "jazz", "pop"), 100, replace = TRUE)
Y %v% "female"    <- rbinom(100, 1, 0.5) 

Y <- ergm::simulate_formula(
  Y ~
    edges +
    nodematch("fav_music") + 
    nodematch("female") +
    mutual,
  coef = c(-4, 1, -1, 2)
  )
```

```{r}
library(sna)
gplot(
  Y,
  vertex.col = c("green", "red")[Y %v% "female" + 1] 
  )
```

## Data

## Simplest model: Bernoulli graph

The step is to check whether we can fit an ERGM or not. We can do so with the Bernoulli graph:

```{r}
model_1 <- ergm(Y ~ edges)
summary(model_1)
```



```{r}
#| label: first-models
#| collapse: true
#| message: false
#| cache: true
# Fitting two more models (output message suppressed)
model_2 <- ergm(Y ~ edges + mutual)
model_3 <- ergm(Y ~ edges + mutual + gwdsp(.5, fixed = TRUE))
# model_4 <- ergm(Y ~ edges + triangles) # bad idea
```

Right after fitting a model, we want to inspect the results. An excellent tool for this is the R package `texreg` [@leifeldTexregConversionStatistical2013]:

```{r}
#| label: simples-table
#| message: false
#| warning: false
#| results: asis
library(texreg)
knitreg(list(model_1, model_2, model_3))
```

So far, `model_2` is winning. We will continue with this model.

## Let's add a little bit of structure

```{r}
#| label: working-homophily
#| message: false
model_4 <- ergm(Y ~ edges + mutual + nodematch("fav_music"))
model_5 <- ergm(Y ~ edges + mutual + nodematch("fav_music", diff = TRUE))
```

Now, let's inspect what do we have so far:

```{r}
#| label: working-homophily-table
#| message: false
#| warning: false
#| results: asis
knitreg(list(model_2, model_4, model_5))
```

# References

